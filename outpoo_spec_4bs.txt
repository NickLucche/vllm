============================= test session starts ==============================
platform linux -- Python 3.10.12, pytest-8.3.3, pluggy-1.5.0
rootdir: /home/ubuntu/vllm
configfile: pyproject.toml
plugins: anyio-4.4.0
collected 1 item

tests/spec_decode/e2e/test_multistep_correctness.py Fork a new process to run a test 0
PROMPTS ['The president of the United States is a man, whats his name?', 'The future of AI is, in your own words: ', 'The president of the United States is a man, whats his name?', 'The future of AI is, in your own words: ']
INFO 10-04 10:27:14 config.py:1660] Downcasting torch.float32 to torch.float16.
Enableeeed!!
INFO 10-04 10:27:15 config.py:1660] Downcasting torch.float32 to torch.float16.
INFO 10-04 10:27:15 config.py:1017] Chunked prefill is enabled with max_num_batched_tokens=4.
WARNING 10-04 10:27:15 config.py:389] To see benefits of async output processing, enable CUDA graph. Since, enforce-eager is enabled, async output processor cannot be used
INFO 10-04 10:27:15 llm_engine.py:237] Initializing an LLM engine (v0.6.3.dev60+g35bd2151.d20241001) with config: model='JackFram/llama-68m', speculative_config=SpeculativeConfig(draft_model='JackFram/llama-68m', num_spec_tokens=5), tokenizer='JackFram/llama-68m', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, rope_scaling=None, rope_theta=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.float16, max_seq_len=2048, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=True, kv_cache_dtype=auto, quantization_param_path=None, device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='outlines'), observability_config=ObservabilityConfig(otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=0, served_model_name=JackFram/llama-68m, use_v2_block_manager=True, num_scheduler_steps=1, chunked_prefill_enabled=True multi_step_stream_outputs=False, enable_prefix_caching=False, use_async_output_proc=False, use_cached_outputs=False, mm_processor_kwargs=None)
INFO 10-04 10:27:15 spec_decode_worker.py:164] Configuring SpecDecodeWorker with proposer=<class 'vllm.spec_decode.multi_step_worker.MultiStepWorker'>
INFO 10-04 10:27:15 rejection_sampler.py:57] Use pytorch for rejection sampling.
INFO 10-04 10:27:15 spec_decode_worker.py:176] Configuring SpecDecodeWorker with sampler=<class 'vllm.model_executor.layers.rejection_sampler.RejectionSampler'>
INFO 10-04 10:27:16 model_runner.py:1022] Starting to load model JackFram/llama-68m...
INFO 10-04 10:27:16 weight_utils.py:242] Using model weights format ['*.bin']
INFO 10-04 10:27:16 model_runner.py:1033] Loading model weights took 0.1270 GB
INFO 10-04 10:27:16 model_runner.py:1022] Starting to load model JackFram/llama-68m...
INFO 10-04 10:27:16 weight_utils.py:242] Using model weights format ['*.bin']
INFO 10-04 10:27:17 model_runner.py:1033] Loading model weights took 0.1267 GB
INFO 10-04 10:27:17 gpu_executor.py:122] # GPU blocks: 106008, # CPU blocks: 43690
PROMPTS (is prompt, K, token_chunk, do_sample):  [(True, 0, 4, False)]
NO SPEC
DO sample [False]
NO SPEC TARGET OUT AND DO_SAMPLE [(SamplerOutput(outputs=[], sampled_token_probs=torch.Size([0, 32000]), sampled_token_ids=torch.Size([0, 1]), spec_decode_worker_metrics=None), False)]
================================================================================
NO_SPEC SAMPLER OUTPUT SamplerOutput(outputs=[CompletionSequenceGroupOutput(samples=[], prompt_logprobs=None)], sampled_token_probs=None, sampled_token_ids=None, spec_decode_worker_metrics=None)
LLM STep outputs [SamplerOutput(outputs=[CompletionSequenceGroupOutput(samples=[], prompt_logprobs=None)], sampled_token_probs=None, sampled_token_ids=None, spec_decode_worker_metrics=None)]
LLM process output indices range(0, 1)
OUTPUTS PER SEQ GROUP (ONE IS PROMPT THE OTHER NOT) [SamplerOutput(outputs=[CompletionSequenceGroupOutput(samples=[], prompt_logprobs=None)], sampled_token_probs=None, sampled_token_ids=None, spec_decode_worker_metrics=None)]
output has multiples False 1
Updating computed tokens manually
META DO_SAMPLE False
OUT [CompletionSequenceGroupOutput(samples=[], prompt_logprobs=None)] 



SEQ GROUP DIO CANE SHOULD SAMPLE HERE False
SequenceGroup(request_id=0, sampling_params=SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=True, max_tokens=32, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), guided_decoding=None, num_seqs=1)
PROMPTS (is prompt, K, token_chunk, do_sample):  [(True, 0, 4, False)]
NO SPEC
DO sample [False]
NO SPEC TARGET OUT AND DO_SAMPLE [(SamplerOutput(outputs=[], sampled_token_probs=torch.Size([0, 32000]), sampled_token_ids=torch.Size([0, 1]), spec_decode_worker_metrics=None), False)]
================================================================================
NO_SPEC SAMPLER OUTPUT SamplerOutput(outputs=[CompletionSequenceGroupOutput(samples=[], prompt_logprobs=None)], sampled_token_probs=None, sampled_token_ids=None, spec_decode_worker_metrics=None)
LLM STep outputs [SamplerOutput(outputs=[CompletionSequenceGroupOutput(samples=[], prompt_logprobs=None)], sampled_token_probs=None, sampled_token_ids=None, spec_decode_worker_metrics=None)]
LLM process output indices range(0, 1)
OUTPUTS PER SEQ GROUP (ONE IS PROMPT THE OTHER NOT) [SamplerOutput(outputs=[CompletionSequenceGroupOutput(samples=[], prompt_logprobs=None)], sampled_token_probs=None, sampled_token_ids=None, spec_decode_worker_metrics=None)]
output has multiples False 1
Updating computed tokens manually
META DO_SAMPLE False
OUT [CompletionSequenceGroupOutput(samples=[], prompt_logprobs=None)] 



SEQ GROUP DIO CANE SHOULD SAMPLE HERE False
SequenceGroup(request_id=0, sampling_params=SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=True, max_tokens=32, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), guided_decoding=None, num_seqs=1)
PROMPTS (is prompt, K, token_chunk, do_sample):  [(True, 0, 4, False)]
NO SPEC
DO sample [False]
NO SPEC TARGET OUT AND DO_SAMPLE [(SamplerOutput(outputs=[], sampled_token_probs=torch.Size([0, 32000]), sampled_token_ids=torch.Size([0, 1]), spec_decode_worker_metrics=None), False)]
================================================================================
NO_SPEC SAMPLER OUTPUT SamplerOutput(outputs=[CompletionSequenceGroupOutput(samples=[], prompt_logprobs=None)], sampled_token_probs=None, sampled_token_ids=None, spec_decode_worker_metrics=None)
LLM STep outputs [SamplerOutput(outputs=[CompletionSequenceGroupOutput(samples=[], prompt_logprobs=None)], sampled_token_probs=None, sampled_token_ids=None, spec_decode_worker_metrics=None)]
LLM process output indices range(0, 1)
OUTPUTS PER SEQ GROUP (ONE IS PROMPT THE OTHER NOT) [SamplerOutput(outputs=[CompletionSequenceGroupOutput(samples=[], prompt_logprobs=None)], sampled_token_probs=None, sampled_token_ids=None, spec_decode_worker_metrics=None)]
output has multiples False 1
Updating computed tokens manually
META DO_SAMPLE False
OUT [CompletionSequenceGroupOutput(samples=[], prompt_logprobs=None)] 



SEQ GROUP DIO CANE SHOULD SAMPLE HERE False
SequenceGroup(request_id=0, sampling_params=SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=True, max_tokens=32, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), guided_decoding=None, num_seqs=1)
PROMPTS (is prompt, K, token_chunk, do_sample):  [(True, 0, 4, True)]
NO SPEC
DO sample [True]
NO SPEC TARGET OUT AND DO_SAMPLE [(SamplerOutput(outputs=[], sampled_token_probs=torch.Size([1, 32000]), sampled_token_ids=torch.Size([1, 1]), spec_decode_worker_metrics=None), True)]
================================================================================
NO_SPEC SAMPLER OUTPUT SamplerOutput(outputs=[CompletionSequenceGroupOutput(samples=[SequenceOutput(parent_seq_id=0, output_token=13, logprobs={13: Logprob(logprob=0.0, rank=-1, decoded_token=None)})], prompt_logprobs=None)], sampled_token_probs=None, sampled_token_ids=None, spec_decode_worker_metrics=None)
LLM STep outputs [SamplerOutput(outputs=[CompletionSequenceGroupOutput(samples=[SequenceOutput(parent_seq_id=0, output_token=13, logprobs={13: Logprob(logprob=0.0, rank=-1, decoded_token=None)})], prompt_logprobs=None)], sampled_token_probs=None, sampled_token_ids=None, spec_decode_worker_metrics=None)]
LLM process output indices range(0, 1)
OUTPUTS PER SEQ GROUP (ONE IS PROMPT THE OTHER NOT) [SamplerOutput(outputs=[CompletionSequenceGroupOutput(samples=[SequenceOutput(parent_seq_id=0, output_token=13, logprobs={13: Logprob(logprob=0.0, rank=-1, decoded_token=None)})], prompt_logprobs=None)], sampled_token_probs=None, sampled_token_ids=None, spec_decode_worker_metrics=None)]
output has multiples False 1
Updating computed tokens manually
META DO_SAMPLE True
OUT [CompletionSequenceGroupOutput(samples=[SequenceOutput(parent_seq_id=0, output_token=13, logprobs={13: Logprob(logprob=0.0, rank=-1, decoded_token=None)})], prompt_logprobs=None)] 



SEQ GROUP DIO CANE SHOULD SAMPLE HERE True
SequenceGroup(request_id=0, sampling_params=SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=True, max_tokens=32, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), guided_decoding=None, num_seqs=1)
PROMPTS (is prompt, K, token_chunk, do_sample):  [(True, 0, 3, False), (False, None, 1, True)]
SPECULATIING WITH REQ METADATA [SequenceGroupMetadata(request_id='1', is_prompt=True, seq_data={1: SequenceData(prompt_token_ids=array('l', [1, 450, 5434, 310, 319, 29902, 338, 29892, 297, 596, 1914, 3838, 29901, 29871]), output_token_ids=(), cumulative_logprob=0.0, get_num_computed_tokens=0}, sampling_params=SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=True, max_tokens=32, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), guided_decoding=None, block_tables={1: [2]}, do_sample=False, pooling_params=None, lora_request=None, computed_block_nums=[], state=SequenceGroupState(num_steps=1, current_step=0), multi_modal_data={}, encoder_seq_data=None, cross_block_table=None, prompt_adapter_request=None, token_chunk_size=3, num_speculative_tokens=0), SequenceGroupMetadata(request_id='0', is_prompt=False, seq_data={0: SequenceData(prompt_token_ids=array('l', [1, 450, 6673, 310, 278, 3303, 3900, 338, 263, 767, 29892, 825, 29879, 670, 1024, 29973]), output_token_ids=(13,), cumulative_logprob=0.0, get_num_computed_tokens=17}, sampling_params=SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=True, max_tokens=32, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), guided_decoding=None, block_tables={0: [0, 1]}, do_sample=True, pooling_params=None, lora_request=None, computed_block_nums=[], state=SequenceGroupState(num_steps=1, current_step=0), multi_modal_data={}, encoder_seq_data=None, cross_block_table=None, prompt_adapter_request=None, token_chunk_size=1, num_speculative_tokens=None)]
SCORING ON THE UNION OF REQUESTS (includes expanded batch), is prompt? [False, False, False, False, False, False, True]
SCORING ON THE UNION OF REQUESTS (includes expanded batch), do_sample? [True, True, True, True, True, True, False]
MIXEEED BATCH BOYS!
Proposal len X do_sample X is_prompt [(0, (False, True)), (5, (True, False))]
Target probs distr tensor(0., device='cuda:0') tensor(3.1250e-05, device='cuda:0') tensor(1., device='cuda:0') torch.Size([1, 6, 32000])
Draft probs distr tensor(0., device='cuda:0') tensor(3.1250e-05, device='cuda:0') tensor(1., device='cuda:0') torch.Size([1, 5, 32000])
VERIFY STEP ENDED tensor([[  -1,   -1,   -1,   -1,   -1,   -1],
        [1576,  261,   -1,   -1,   -1,   -1]], device='cuda:0')
SAMPLER OUTPUT LIST 2x2
LLM STep outputs [SamplerOutput(outputs=[CompletionSequenceGroupOutput(samples=[SequenceOutput(parent_seq_id=1, output_token=-1, logprobs={-1: Logprob(logprob=0.0, rank=-1, decoded_token=None)})], prompt_logprobs=None), CompletionSequenceGroupOutput(samples=[SequenceOutput(parent_seq_id=0, output_token=1576, logprobs={1576: Logprob(logprob=0.0, rank=-1, decoded_token=None)})], prompt_logprobs=None)], sampled_token_probs=None, sampled_token_ids=None, spec_decode_worker_metrics=None), SamplerOutput(outputs=[CompletionSequenceGroupOutput(samples=[SequenceOutput(parent_seq_id=1, output_token=-1, logprobs={-1: Logprob(logprob=0.0, rank=-1, decoded_token=None)})], prompt_logprobs=None), CompletionSequenceGroupOutput(samples=[SequenceOutput(parent_seq_id=0, output_token=261, logprobs={261: Logprob(logprob=0.0, rank=-1, decoded_token=None)})], prompt_logprobs=None)], sampled_token_probs=None, sampled_token_ids=None, spec_decode_worker_metrics=None)]
LLM process output indices range(0, 2)
OUTPUTS PER SEQ GROUP (ONE IS PROMPT THE OTHER NOT) [[CompletionSequenceGroupOutput(samples=[SequenceOutput(parent_seq_id=1, output_token=-1, logprobs={-1: Logprob(logprob=0.0, rank=-1, decoded_token=None)})], prompt_logprobs=None), CompletionSequenceGroupOutput(samples=[SequenceOutput(parent_seq_id=1, output_token=-1, logprobs={-1: Logprob(logprob=0.0, rank=-1, decoded_token=None)})], prompt_logprobs=None)], [CompletionSequenceGroupOutput(samples=[SequenceOutput(parent_seq_id=0, output_token=1576, logprobs={1576: Logprob(logprob=0.0, rank=-1, decoded_token=None)})], prompt_logprobs=None), CompletionSequenceGroupOutput(samples=[SequenceOutput(parent_seq_id=0, output_token=261, logprobs={261: Logprob(logprob=0.0, rank=-1, decoded_token=None)})], prompt_logprobs=None)]]
output has multiples True 2
Updating computed tokens manually
META DO_SAMPLE False
OUT [CompletionSequenceGroupOutput(samples=[SequenceOutput(parent_seq_id=1, output_token=-1, logprobs={-1: Logprob(logprob=0.0, rank=-1, decoded_token=None)})], prompt_logprobs=None), CompletionSequenceGroupOutput(samples=[SequenceOutput(parent_seq_id=1, output_token=-1, logprobs={-1: Logprob(logprob=0.0, rank=-1, decoded_token=None)})], prompt_logprobs=None)] 



SEQ GROUP DIO CANE SHOULD SAMPLE HERE False
SequenceGroup(request_id=1, sampling_params=SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=True, max_tokens=32, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), guided_decoding=None, num_seqs=1)
output has multiples True 2
SEQ GROUP DIO CANE SHOULD SAMPLE HERE True
SequenceGroup(request_id=0, sampling_params=SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=True, max_tokens=32, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), guided_decoding=None, num_seqs=1)
PROMPTS (is prompt, K, token_chunk, do_sample):  [(True, 0, 3, False), (False, None, 1, True)]
SPECULATIING WITH REQ METADATA [SequenceGroupMetadata(request_id='1', is_prompt=True, seq_data={1: SequenceData(prompt_token_ids=array('l', [1, 450, 5434, 310, 319, 29902, 338, 29892, 297, 596, 1914, 3838, 29901, 29871]), output_token_ids=(), cumulative_logprob=0.0, get_num_computed_tokens=3}, sampling_params=SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=True, max_tokens=32, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), guided_decoding=None, block_tables={1: [2]}, do_sample=False, pooling_params=None, lora_request=None, computed_block_nums=[], state=SequenceGroupState(num_steps=1, current_step=0), multi_modal_data={}, encoder_seq_data=None, cross_block_table=None, prompt_adapter_request=None, token_chunk_size=3, num_speculative_tokens=0), SequenceGroupMetadata(request_id='0', is_prompt=False, seq_data={0: SequenceData(prompt_token_ids=array('l', [1, 450, 6673, 310, 278, 3303, 3900, 338, 263, 767, 29892, 825, 29879, 670, 1024, 29973]), output_token_ids=(13, 1576, 261), cumulative_logprob=0.0, get_num_computed_tokens=19}, sampling_params=SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=True, max_tokens=32, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), guided_decoding=None, block_tables={0: [0, 1]}, do_sample=True, pooling_params=None, lora_request=None, computed_block_nums=[], state=SequenceGroupState(num_steps=1, current_step=0), multi_modal_data={}, encoder_seq_data=None, cross_block_table=None, prompt_adapter_request=None, token_chunk_size=1, num_speculative_tokens=None)]
SCORING ON THE UNION OF REQUESTS (includes expanded batch), is prompt? [False, False, False, False, False, False, True]
SCORING ON THE UNION OF REQUESTS (includes expanded batch), do_sample? [True, True, True, True, True, True, False]
MIXEEED BATCH BOYS!
Proposal len X do_sample X is_prompt [(0, (False, True)), (5, (True, False))]
Target probs distr tensor(0., device='cuda:0') tensor(3.1250e-05, device='cuda:0') tensor(1., device='cuda:0') torch.Size([1, 6, 32000])
Draft probs distr tensor(0., device='cuda:0') tensor(3.1250e-05, device='cuda:0') tensor(1., device='cuda:0') torch.Size([1, 5, 32000])
VERIFY STEP ENDED tensor([[   -1,    -1,    -1,    -1,    -1,    -1],
        [29901,    -1,    -1,    -1,    -1,    -1]], device='cuda:0')
SAMPLER OUTPUT LIST 1x2
LLM STep outputs [SamplerOutput(outputs=[CompletionSequenceGroupOutput(samples=[SequenceOutput(parent_seq_id=1, output_token=-1, logprobs={-1: Logprob(logprob=0.0, rank=-1, decoded_token=None)})], prompt_logprobs=None), CompletionSequenceGroupOutput(samples=[SequenceOutput(parent_seq_id=0, output_token=29901, logprobs={29901: Logprob(logprob=0.0, rank=-1, decoded_token=None)})], prompt_logprobs=None)], sampled_token_probs=None, sampled_token_ids=None, spec_decode_worker_metrics=SpecDecodeWorkerMetrics(draft_acceptance_rate=0.4, system_efficiency=0.3333333333333333, draft_tokens=5, emitted_tokens=2, accepted_tokens=2, num_spec_tokens=5))]
LLM process output indices range(0, 2)
OUTPUTS PER SEQ GROUP (ONE IS PROMPT THE OTHER NOT) [SamplerOutput(outputs=[CompletionSequenceGroupOutput(samples=[SequenceOutput(parent_seq_id=1, output_token=-1, logprobs={-1: Logprob(logprob=0.0, rank=-1, decoded_token=None)})], prompt_logprobs=None), CompletionSequenceGroupOutput(samples=[SequenceOutput(parent_seq_id=0, output_token=29901, logprobs={29901: Logprob(logprob=0.0, rank=-1, decoded_token=None)})], prompt_logprobs=None)], sampled_token_probs=None, sampled_token_ids=None, spec_decode_worker_metrics=SpecDecodeWorkerMetrics(draft_acceptance_rate=0.4, system_efficiency=0.3333333333333333, draft_tokens=5, emitted_tokens=2, accepted_tokens=2, num_spec_tokens=5))]
output has multiples False 1
Updating computed tokens manually
META DO_SAMPLE False
OUT [CompletionSequenceGroupOutput(samples=[SequenceOutput(parent_seq_id=1, output_token=-1, logprobs={-1: Logprob(logprob=0.0, rank=-1, decoded_token=None)})], prompt_logprobs=None)] 



SEQ GROUP DIO CANE SHOULD SAMPLE HERE False
SequenceGroup(request_id=1, sampling_params=SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=True, max_tokens=32, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), guided_decoding=None, num_seqs=1)
output has multiples False 1
SEQ GROUP DIO CANE SHOULD SAMPLE HERE True
SequenceGroup(request_id=0, sampling_params=SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=True, max_tokens=32, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), guided_decoding=None, num_seqs=1)
PROMPTS (is prompt, K, token_chunk, do_sample):  [(True, 0, 3, False), (False, None, 1, True)]
SPECULATIING WITH REQ METADATA [SequenceGroupMetadata(request_id='1', is_prompt=True, seq_data={1: SequenceData(prompt_token_ids=array('l', [1, 450, 5434, 310, 319, 29902, 338, 29892, 297, 596, 1914, 3838, 29901, 29871]), output_token_ids=(), cumulative_logprob=0.0, get_num_computed_tokens=6}, sampling_params=SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=True, max_tokens=32, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), guided_decoding=None, block_tables={1: [2]}, do_sample=False, pooling_params=None, lora_request=None, computed_block_nums=[], state=SequenceGroupState(num_steps=1, current_step=0), multi_modal_data={}, encoder_seq_data=None, cross_block_table=None, prompt_adapter_request=None, token_chunk_size=3, num_speculative_tokens=0), SequenceGroupMetadata(request_id='0', is_prompt=False, seq_data={0: SequenceData(prompt_token_ids=array('l', [1, 450, 6673, 310, 278, 3303, 3900, 338, 263, 767, 29892, 825, 29879, 670, 1024, 29973]), output_token_ids=(13, 1576, 261, 29901), cumulative_logprob=0.0, get_num_computed_tokens=20}, sampling_params=SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=True, max_tokens=32, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), guided_decoding=None, block_tables={0: [0, 1]}, do_sample=True, pooling_params=None, lora_request=None, computed_block_nums=[], state=SequenceGroupState(num_steps=1, current_step=0), multi_modal_data={}, encoder_seq_data=None, cross_block_table=None, prompt_adapter_request=None, token_chunk_size=1, num_speculative_tokens=None)]
SCORING ON THE UNION OF REQUESTS (includes expanded batch), is prompt? [False, False, False, False, False, False, True]
SCORING ON THE UNION OF REQUESTS (includes expanded batch), do_sample? [True, True, True, True, True, True, False]
MIXEEED BATCH BOYS!
Proposal len X do_sample X is_prompt [(0, (False, True)), (5, (True, False))]
Target probs distr tensor(0., device='cuda:0') tensor(3.1250e-05, device='cuda:0') tensor(1., device='cuda:0') torch.Size([1, 6, 32000])
Draft probs distr tensor(0., device='cuda:0') tensor(3.1250e-05, device='cuda:0') tensor(1., device='cuda:0') torch.Size([1, 5, 32000])
VERIFY STEP ENDED tensor([[ -1,  -1,  -1,  -1,  -1,  -1],
        [450, 759,  -1,  -1,  -1,  -1]], device='cuda:0')
SAMPLER OUTPUT LIST 2x2
LLM STep outputs [SamplerOutput(outputs=[CompletionSequenceGroupOutput(samples=[SequenceOutput(parent_seq_id=1, output_token=-1, logprobs={-1: Logprob(logprob=0.0, rank=-1, decoded_token=None)})], prompt_logprobs=None), CompletionSequenceGroupOutput(samples=[SequenceOutput(parent_seq_id=0, output_token=450, logprobs={450: Logprob(logprob=0.0, rank=-1, decoded_token=None)})], prompt_logprobs=None)], sampled_token_probs=None, sampled_token_ids=None, spec_decode_worker_metrics=None), SamplerOutput(outputs=[CompletionSequenceGroupOutput(samples=[SequenceOutput(parent_seq_id=1, output_token=-1, logprobs={-1: Logprob(logprob=0.0, rank=-1, decoded_token=None)})], prompt_logprobs=None), CompletionSequenceGroupOutput(samples=[SequenceOutput(parent_seq_id=0, output_token=759, logprobs={759: Logprob(logprob=0.0, rank=-1, decoded_token=None)})], prompt_logprobs=None)], sampled_token_probs=None, sampled_token_ids=None, spec_decode_worker_metrics=None)]
LLM process output indices range(0, 2)
OUTPUTS PER SEQ GROUP (ONE IS PROMPT THE OTHER NOT) [[CompletionSequenceGroupOutput(samples=[SequenceOutput(parent_seq_id=1, output_token=-1, logprobs={-1: Logprob(logprob=0.0, rank=-1, decoded_token=None)})], prompt_logprobs=None), CompletionSequenceGroupOutput(samples=[SequenceOutput(parent_seq_id=1, output_token=-1, logprobs={-1: Logprob(logprob=0.0, rank=-1, decoded_token=None)})], prompt_logprobs=None)], [CompletionSequenceGroupOutput(samples=[SequenceOutput(parent_seq_id=0, output_token=450, logprobs={450: Logprob(logprob=0.0, rank=-1, decoded_token=None)})], prompt_logprobs=None), CompletionSequenceGroupOutput(samples=[SequenceOutput(parent_seq_id=0, output_token=759, logprobs={759: Logprob(logprob=0.0, rank=-1, decoded_token=None)})], prompt_logprobs=None)]]
output has multiples True 2
Updating computed tokens manually
META DO_SAMPLE False
OUT [CompletionSequenceGroupOutput(samples=[SequenceOutput(parent_seq_id=1, output_token=-1, logprobs={-1: Logprob(logprob=0.0, rank=-1, decoded_token=None)})], prompt_logprobs=None), CompletionSequenceGroupOutput(samples=[SequenceOutput(parent_seq_id=1, output_token=-1, logprobs={-1: Logprob(logprob=0.0, rank=-1, decoded_token=None)})], prompt_logprobs=None)] 



SEQ GROUP DIO CANE SHOULD SAMPLE HERE False
SequenceGroup(request_id=1, sampling_params=SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=True, max_tokens=32, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), guided_decoding=None, num_seqs=1)
output has multiples True 2
SEQ GROUP DIO CANE SHOULD SAMPLE HERE True
SequenceGroup(request_id=0, sampling_params=SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=True, max_tokens=32, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), guided_decoding=None, num_seqs=1)
PROMPTS (is prompt, K, token_chunk, do_sample):  [(True, 0, 3, False), (False, None, 1, True)]
SPECULATIING WITH REQ METADATA [SequenceGroupMetadata(request_id='1', is_prompt=True, seq_data={1: SequenceData(prompt_token_ids=array('l', [1, 450, 5434, 310, 319, 29902, 338, 29892, 297, 596, 1914, 3838, 29901, 29871]), output_token_ids=(), cumulative_logprob=0.0, get_num_computed_tokens=9}, sampling_params=SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=True, max_tokens=32, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), guided_decoding=None, block_tables={1: [2]}, do_sample=False, pooling_params=None, lora_request=None, computed_block_nums=[], state=SequenceGroupState(num_steps=1, current_step=0), multi_modal_data={}, encoder_seq_data=None, cross_block_table=None, prompt_adapter_request=None, token_chunk_size=3, num_speculative_tokens=0), SequenceGroupMetadata(request_id='0', is_prompt=False, seq_data={0: SequenceData(prompt_token_ids=array('l', [1, 450, 6673, 310, 278, 3303, 3900, 338, 263, 767, 29892, 825, 29879, 670, 1024, 29973]), output_token_ids=(13, 1576, 261, 29901, 450, 759), cumulative_logprob=0.0, get_num_computed_tokens=22}, sampling_params=SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=True, max_tokens=32, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), guided_decoding=None, block_tables={0: [0, 1]}, do_sample=True, pooling_params=None, lora_request=None, computed_block_nums=[], state=SequenceGroupState(num_steps=1, current_step=0), multi_modal_data={}, encoder_seq_data=None, cross_block_table=None, prompt_adapter_request=None, token_chunk_size=1, num_speculative_tokens=None)]
SCORING ON THE UNION OF REQUESTS (includes expanded batch), is prompt? [False, False, False, False, False, False, True]
SCORING ON THE UNION OF REQUESTS (includes expanded batch), do_sample? [True, True, True, True, True, True, False]
MIXEEED BATCH BOYS!
Proposal len X do_sample X is_prompt [(0, (False, True)), (5, (True, False))]
Target probs distr tensor(0., device='cuda:0') tensor(3.1250e-05, device='cuda:0') tensor(1., device='cuda:0') torch.Size([1, 6, 32000])
Draft probs distr tensor(0., device='cuda:0') tensor(3.1250e-05, device='cuda:0') tensor(1., device='cuda:0') torch.Size([1, 5, 32000])
VERIFY STEP ENDED tensor([[ -1,  -1,  -1,  -1,  -1,  -1],
        [391, 414,  -1,  -1,  -1,  -1]], device='cuda:0')
SAMPLER OUTPUT LIST 2x2
LLM STep outputs [SamplerOutput(outputs=[CompletionSequenceGroupOutput(samples=[SequenceOutput(parent_seq_id=1, output_token=-1, logprobs={-1: Logprob(logprob=0.0, rank=-1, decoded_token=None)})], prompt_logprobs=None), CompletionSequenceGroupOutput(samples=[SequenceOutput(parent_seq_id=0, output_token=391, logprobs={391: Logprob(logprob=0.0, rank=-1, decoded_token=None)})], prompt_logprobs=None)], sampled_token_probs=None, sampled_token_ids=None, spec_decode_worker_metrics=None), SamplerOutput(outputs=[CompletionSequenceGroupOutput(samples=[SequenceOutput(parent_seq_id=1, output_token=-1, logprobs={-1: Logprob(logprob=0.0, rank=-1, decoded_token=None)})], prompt_logprobs=None), CompletionSequenceGroupOutput(samples=[SequenceOutput(parent_seq_id=0, output_token=414, logprobs={414: Logprob(logprob=0.0, rank=-1, decoded_token=None)})], prompt_logprobs=None)], sampled_token_probs=None, sampled_token_ids=None, spec_decode_worker_metrics=None)]
LLM process output indices range(0, 2)
OUTPUTS PER SEQ GROUP (ONE IS PROMPT THE OTHER NOT) [[CompletionSequenceGroupOutput(samples=[SequenceOutput(parent_seq_id=1, output_token=-1, logprobs={-1: Logprob(logprob=0.0, rank=-1, decoded_token=None)})], prompt_logprobs=None), CompletionSequenceGroupOutput(samples=[SequenceOutput(parent_seq_id=1, output_token=-1, logprobs={-1: Logprob(logprob=0.0, rank=-1, decoded_token=None)})], prompt_logprobs=None)], [CompletionSequenceGroupOutput(samples=[SequenceOutput(parent_seq_id=0, output_token=391, logprobs={391: Logprob(logprob=0.0, rank=-1, decoded_token=None)})], prompt_logprobs=None), CompletionSequenceGroupOutput(samples=[SequenceOutput(parent_seq_id=0, output_token=414, logprobs={414: Logprob(logprob=0.0, rank=-1, decoded_token=None)})], prompt_logprobs=None)]]
output has multiples True 2
Updating computed tokens manually
META DO_SAMPLE False
OUT [CompletionSequenceGroupOutput(samples=[SequenceOutput(parent_seq_id=1, output_token=-1, logprobs={-1: Logprob(logprob=0.0, rank=-1, decoded_token=None)})], prompt_logprobs=None), CompletionSequenceGroupOutput(samples=[SequenceOutput(parent_seq_id=1, output_token=-1, logprobs={-1: Logprob(logprob=0.0, rank=-1, decoded_token=None)})], prompt_logprobs=None)] 



SEQ GROUP DIO CANE SHOULD SAMPLE HERE False
SequenceGroup(request_id=1, sampling_params=SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=True, max_tokens=32, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), guided_decoding=None, num_seqs=1)
output has multiples True 2
SEQ GROUP DIO CANE SHOULD SAMPLE HERE True
SequenceGroup(request_id=0, sampling_params=SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=True, max_tokens=32, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), guided_decoding=None, num_seqs=1)
PROMPTS (is prompt, K, token_chunk, do_sample):  [(True, 0, 1, False), (True, 0, 2, True), (False, None, 1, True)]
SPECULATIING WITH REQ METADATA [SequenceGroupMetadata(request_id='2', is_prompt=True, seq_data={2: SequenceData(prompt_token_ids=array('l', [1, 450, 6673, 310, 278, 3303, 3900, 338, 263, 767, 29892, 825, 29879, 670, 1024, 29973]), output_token_ids=(), cumulative_logprob=0.0, get_num_computed_tokens=0}, sampling_params=SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=True, max_tokens=32, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), guided_decoding=None, block_tables={2: [3]}, do_sample=False, pooling_params=None, lora_request=None, computed_block_nums=[], state=SequenceGroupState(num_steps=1, current_step=0), multi_modal_data={}, encoder_seq_data=None, cross_block_table=None, prompt_adapter_request=None, token_chunk_size=1, num_speculative_tokens=0), SequenceGroupMetadata(request_id='1', is_prompt=True, seq_data={1: SequenceData(prompt_token_ids=array('l', [1, 450, 5434, 310, 319, 29902, 338, 29892, 297, 596, 1914, 3838, 29901, 29871]), output_token_ids=(), cumulative_logprob=0.0, get_num_computed_tokens=12}, sampling_params=SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=True, max_tokens=32, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), guided_decoding=None, block_tables={1: [2]}, do_sample=True, pooling_params=None, lora_request=None, computed_block_nums=[], state=SequenceGroupState(num_steps=1, current_step=0), multi_modal_data={}, encoder_seq_data=None, cross_block_table=None, prompt_adapter_request=None, token_chunk_size=2, num_speculative_tokens=0), SequenceGroupMetadata(request_id='0', is_prompt=False, seq_data={0: SequenceData(prompt_token_ids=array('l', [1, 450, 6673, 310, 278, 3303, 3900, 338, 263, 767, 29892, 825, 29879, 670, 1024, 29973]), output_token_ids=(13, 1576, 261, 29901, 450, 759, 391, 414), cumulative_logprob=0.0, get_num_computed_tokens=24}, sampling_params=SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=True, max_tokens=32, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), guided_decoding=None, block_tables={0: [0, 1]}, do_sample=True, pooling_params=None, lora_request=None, computed_block_nums=[], state=SequenceGroupState(num_steps=1, current_step=0), multi_modal_data={}, encoder_seq_data=None, cross_block_table=None, prompt_adapter_request=None, token_chunk_size=1, num_speculative_tokens=None)]
SCORING ON THE UNION OF REQUESTS (includes expanded batch), is prompt? [False, False, False, False, False, False, True, True]
SCORING ON THE UNION OF REQUESTS (includes expanded batch), do_sample? [True, True, True, True, True, True, False, True]
MIXEEED BATCH BOYS!
Proposal len X do_sample X is_prompt [(0, (False, True)), (0, (True, True)), (5, (True, False))]
Target probs distr tensor(0., device='cuda:0') tensor(3.1250e-05, device='cuda:0') tensor(1., device='cuda:0') torch.Size([1, 6, 32000])
Draft probs distr tensor(0., device='cuda:0') tensor(3.1250e-05, device='cuda:0') tensor(1., device='cuda:0') torch.Size([1, 5, 32000])
VERIFY STEP ENDED tensor([[29896,    -1,    -1,    -1,    -1,    -1],
        [29896,    -1,    -1,    -1,    -1,    -1],
        [29889,    -1,    -1,    -1,    -1,    -1]], device='cuda:0')
SAMPLER OUTPUT LIST 1x3
LLM STep outputs [SamplerOutput(outputs=[CompletionSequenceGroupOutput(samples=[SequenceOutput(parent_seq_id=2, output_token=29896, logprobs={29896: Logprob(logprob=0.0, rank=-1, decoded_token=None)})], prompt_logprobs=None), CompletionSequenceGroupOutput(samples=[SequenceOutput(parent_seq_id=1, output_token=29896, logprobs={29896: Logprob(logprob=0.0, rank=-1, decoded_token=None)})], prompt_logprobs=None), CompletionSequenceGroupOutput(samples=[SequenceOutput(parent_seq_id=0, output_token=29889, logprobs={29889: Logprob(logprob=0.0, rank=-1, decoded_token=None)})], prompt_logprobs=None)], sampled_token_probs=None, sampled_token_ids=None, spec_decode_worker_metrics=None)]
LLM process output indices range(0, 3)
OUTPUTS PER SEQ GROUP (ONE IS PROMPT THE OTHER NOT) [SamplerOutput(outputs=[CompletionSequenceGroupOutput(samples=[SequenceOutput(parent_seq_id=2, output_token=29896, logprobs={29896: Logprob(logprob=0.0, rank=-1, decoded_token=None)})], prompt_logprobs=None), CompletionSequenceGroupOutput(samples=[SequenceOutput(parent_seq_id=1, output_token=29896, logprobs={29896: Logprob(logprob=0.0, rank=-1, decoded_token=None)})], prompt_logprobs=None), CompletionSequenceGroupOutput(samples=[SequenceOutput(parent_seq_id=0, output_token=29889, logprobs={29889: Logprob(logprob=0.0, rank=-1, decoded_token=None)})], prompt_logprobs=None)], sampled_token_probs=None, sampled_token_ids=None, spec_decode_worker_metrics=None)]
output has multiples False 1
Updating computed tokens manually
META DO_SAMPLE False
OUT [CompletionSequenceGroupOutput(samples=[SequenceOutput(parent_seq_id=2, output_token=29896, logprobs={29896: Logprob(logprob=0.0, rank=-1, decoded_token=None)})], prompt_logprobs=None)] 



SEQ GROUP DIO CANE SHOULD SAMPLE HERE False
SequenceGroup(request_id=2, sampling_params=SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=True, max_tokens=32, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), guided_decoding=None, num_seqs=1)
output has multiples False 1
Updating computed tokens manually
META DO_SAMPLE True
OUT [CompletionSequenceGroupOutput(samples=[SequenceOutput(parent_seq_id=1, output_token=29896, logprobs={29896: Logprob(logprob=0.0, rank=-1, decoded_token=None)})], prompt_logprobs=None)] 



SEQ GROUP DIO CANE SHOULD SAMPLE HERE True
SequenceGroup(request_id=1, sampling_params=SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=True, max_tokens=32, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), guided_decoding=None, num_seqs=1)
output has multiples False 1
SEQ GROUP DIO CANE SHOULD SAMPLE HERE True
SequenceGroup(request_id=0, sampling_params=SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=True, max_tokens=32, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), guided_decoding=None, num_seqs=1)
PROMPTS (is prompt, K, token_chunk, do_sample):  [(True, 0, 2, False), (False, None, 1, True), (False, None, 1, True)]
SPECULATIING WITH REQ METADATA [SequenceGroupMetadata(request_id='2', is_prompt=True, seq_data={2: SequenceData(prompt_token_ids=array('l', [1, 450, 6673, 310, 278, 3303, 3900, 338, 263, 767, 29892, 825, 29879, 670, 1024, 29973]), output_token_ids=(), cumulative_logprob=0.0, get_num_computed_tokens=1}, sampling_params=SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=True, max_tokens=32, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), guided_decoding=None, block_tables={2: [3]}, do_sample=False, pooling_params=None, lora_request=None, computed_block_nums=[], state=SequenceGroupState(num_steps=1, current_step=0), multi_modal_data={}, encoder_seq_data=None, cross_block_table=None, prompt_adapter_request=None, token_chunk_size=2, num_speculative_tokens=0), SequenceGroupMetadata(request_id='0', is_prompt=False, seq_data={0: SequenceData(prompt_token_ids=array('l', [1, 450, 6673, 310, 278, 3303, 3900, 338, 263, 767, 29892, 825, 29879, 670, 1024, 29973]), output_token_ids=(13, 1576, 261, 29901, 450, 759, 391, 414, 29889), cumulative_logprob=0.0, get_num_computed_tokens=25}, sampling_params=SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=True, max_tokens=32, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), guided_decoding=None, block_tables={0: [0, 1]}, do_sample=True, pooling_params=None, lora_request=None, computed_block_nums=[], state=SequenceGroupState(num_steps=1, current_step=0), multi_modal_data={}, encoder_seq_data=None, cross_block_table=None, prompt_adapter_request=None, token_chunk_size=1, num_speculative_tokens=None), SequenceGroupMetadata(request_id='1', is_prompt=False, seq_data={1: SequenceData(prompt_token_ids=array('l', [1, 450, 5434, 310, 319, 29902, 338, 29892, 297, 596, 1914, 3838, 29901, 29871]), output_token_ids=(29896,), cumulative_logprob=0.0, get_num_computed_tokens=15}, sampling_params=SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=True, max_tokens=32, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), guided_decoding=None, block_tables={1: [2, 4]}, do_sample=True, pooling_params=None, lora_request=None, computed_block_nums=[], state=SequenceGroupState(num_steps=1, current_step=0), multi_modal_data={}, encoder_seq_data=None, cross_block_table=None, prompt_adapter_request=None, token_chunk_size=1, num_speculative_tokens=None)]
SCORING ON THE UNION OF REQUESTS (includes expanded batch), is prompt? [False, False, False, False, False, False, False, False, False, False, False, False, True]
SCORING ON THE UNION OF REQUESTS (includes expanded batch), do_sample? [True, True, True, True, True, True, True, True, True, True, True, True, False]
MIXEEED BATCH BOYS!
Proposal len X do_sample X is_prompt [(0, (False, True)), (5, (True, False)), (5, (True, False))]
Target probs distr tensor(0., device='cuda:0') tensor(3.1250e-05, device='cuda:0') tensor(1., device='cuda:0') torch.Size([2, 6, 32000])
Draft probs distr tensor(0., device='cuda:0') tensor(3.1250e-05, device='cuda:0') tensor(1., device='cuda:0') torch.Size([2, 5, 32000])
VERIFY STEP ENDED tensor([[   -1,    -1,    -1,    -1,    -1,    -1],
        [  450, 16912,    -1,    -1,    -1,    -1],
        [29900,    -1,    -1,    -1,    -1,    -1]], device='cuda:0')
SAMPLER OUTPUT LIST 2x3
LLM STep outputs [SamplerOutput(outputs=[CompletionSequenceGroupOutput(samples=[SequenceOutput(parent_seq_id=2, output_token=-1, logprobs={-1: Logprob(logprob=0.0, rank=-1, decoded_token=None)})], prompt_logprobs=None), CompletionSequenceGroupOutput(samples=[SequenceOutput(parent_seq_id=0, output_token=450, logprobs={450: Logprob(logprob=0.0, rank=-1, decoded_token=None)})], prompt_logprobs=None), CompletionSequenceGroupOutput(samples=[SequenceOutput(parent_seq_id=1, output_token=29900, logprobs={29900: Logprob(logprob=0.0, rank=-1, decoded_token=None)})], prompt_logprobs=None)], sampled_token_probs=None, sampled_token_ids=None, spec_decode_worker_metrics=None), SamplerOutput(outputs=[CompletionSequenceGroupOutput(samples=[SequenceOutput(parent_seq_id=2, output_token=-1, logprobs={-1: Logprob(logprob=0.0, rank=-1, decoded_token=None)})], prompt_logprobs=None), CompletionSequenceGroupOutput(samples=[SequenceOutput(parent_seq_id=0, output_token=16912, logprobs={16912: Logprob(logprob=0.0, rank=-1, decoded_token=None)})], prompt_logprobs=None), CompletionSequenceGroupOutput(samples=[SequenceOutput(parent_seq_id=1, output_token=-1, logprobs={-1: Logprob(logprob=0.0, rank=-1, decoded_token=None)})], prompt_logprobs=None)], sampled_token_probs=None, sampled_token_ids=None, spec_decode_worker_metrics=None)]
LLM process output indices range(0, 3)
OUTPUTS PER SEQ GROUP (ONE IS PROMPT THE OTHER NOT) [[CompletionSequenceGroupOutput(samples=[SequenceOutput(parent_seq_id=2, output_token=-1, logprobs={-1: Logprob(logprob=0.0, rank=-1, decoded_token=None)})], prompt_logprobs=None), CompletionSequenceGroupOutput(samples=[SequenceOutput(parent_seq_id=2, output_token=-1, logprobs={-1: Logprob(logprob=0.0, rank=-1, decoded_token=None)})], prompt_logprobs=None)], [CompletionSequenceGroupOutput(samples=[SequenceOutput(parent_seq_id=0, output_token=450, logprobs={450: Logprob(logprob=0.0, rank=-1, decoded_token=None)})], prompt_logprobs=None), CompletionSequenceGroupOutput(samples=[SequenceOutput(parent_seq_id=0, output_token=16912, logprobs={16912: Logprob(logprob=0.0, rank=-1, decoded_token=None)})], prompt_logprobs=None)], [CompletionSequenceGroupOutput(samples=[SequenceOutput(parent_seq_id=1, output_token=29900, logprobs={29900: Logprob(logprob=0.0, rank=-1, decoded_token=None)})], prompt_logprobs=None), CompletionSequenceGroupOutput(samples=[SequenceOutput(parent_seq_id=1, output_token=-1, logprobs={-1: Logprob(logprob=0.0, rank=-1, decoded_token=None)})], prompt_logprobs=None)]]
output has multiples True 2
Updating computed tokens manually
META DO_SAMPLE False
OUT [CompletionSequenceGroupOutput(samples=[SequenceOutput(parent_seq_id=2, output_token=-1, logprobs={-1: Logprob(logprob=0.0, rank=-1, decoded_token=None)})], prompt_logprobs=None), CompletionSequenceGroupOutput(samples=[SequenceOutput(parent_seq_id=2, output_token=-1, logprobs={-1: Logprob(logprob=0.0, rank=-1, decoded_token=None)})], prompt_logprobs=None)] 



SEQ GROUP DIO CANE SHOULD SAMPLE HERE False
SequenceGroup(request_id=2, sampling_params=SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=True, max_tokens=32, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), guided_decoding=None, num_seqs=1)
output has multiples True 2
SEQ GROUP DIO CANE SHOULD SAMPLE HERE True
SequenceGroup(request_id=0, sampling_params=SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=True, max_tokens=32, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), guided_decoding=None, num_seqs=1)
output has multiples True 2
SEQ GROUP DIO CANE SHOULD SAMPLE HERE True
SequenceGroup(request_id=1, sampling_params=SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=True, max_tokens=32, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), guided_decoding=None, num_seqs=1)
PROMPTS (is prompt, K, token_chunk, do_sample):  [(True, 0, 2, False), (False, None, 1, True), (False, None, 1, True)]
SPECULATIING WITH REQ METADATA [SequenceGroupMetadata(request_id='2', is_prompt=True, seq_data={2: SequenceData(prompt_token_ids=array('l', [1, 450, 6673, 310, 278, 3303, 3900, 338, 263, 767, 29892, 825, 29879, 670, 1024, 29973]), output_token_ids=(), cumulative_logprob=0.0, get_num_computed_tokens=3}, sampling_params=SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=True, max_tokens=32, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), guided_decoding=None, block_tables={2: [3]}, do_sample=False, pooling_params=None, lora_request=None, computed_block_nums=[], state=SequenceGroupState(num_steps=1, current_step=0), multi_modal_data={}, encoder_seq_data=None, cross_block_table=None, prompt_adapter_request=None, token_chunk_size=2, num_speculative_tokens=0), SequenceGroupMetadata(request_id='0', is_prompt=False, seq_data={0: SequenceData(prompt_token_ids=array('l', [1, 450, 6673, 310, 278, 3303, 3900, 338, 263, 767, 29892, 825, 29879, 670, 1024, 29973]), output_token_ids=(13, 1576, 261, 29901, 450, 759, 391, 414, 29889, 450, 16912), cumulative_logprob=0.0, get_num_computed_tokens=27}, sampling_params=SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=True, max_tokens=32, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), guided_decoding=None, block_tables={0: [0, 1]}, do_sample=True, pooling_params=None, lora_request=None, computed_block_nums=[], state=SequenceGroupState(num_steps=1, current_step=0), multi_modal_data={}, encoder_seq_data=None, cross_block_table=None, prompt_adapter_request=None, token_chunk_size=1, num_speculative_tokens=None), SequenceGroupMetadata(request_id='1', is_prompt=False, seq_data={1: SequenceData(prompt_token_ids=array('l', [1, 450, 5434, 310, 319, 29902, 338, 29892, 297, 596, 1914, 3838, 29901, 29871]), output_token_ids=(29896, 29900), cumulative_logprob=0.0, get_num_computed_tokens=16}, sampling_params=SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=True, max_tokens=32, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), guided_decoding=None, block_tables={1: [2, 4]}, do_sample=True, pooling_params=None, lora_request=None, computed_block_nums=[], state=SequenceGroupState(num_steps=1, current_step=0), multi_modal_data={}, encoder_seq_data=None, cross_block_table=None, prompt_adapter_request=None, token_chunk_size=1, num_speculative_tokens=None)]
SCORING ON THE UNION OF REQUESTS (includes expanded batch), is prompt? [False, False, False, False, False, False, False, False, False, False, False, False, True]
SCORING ON THE UNION OF REQUESTS (includes expanded batch), do_sample? [True, True, True, True, True, True, True, True, True, True, True, True, False]
MIXEEED BATCH BOYS!
Proposal len X do_sample X is_prompt [(0, (False, True)), (5, (True, False)), (5, (True, False))]
Target probs distr tensor(0., device='cuda:0') tensor(3.1250e-05, device='cuda:0') tensor(1., device='cuda:0') torch.Size([2, 6, 32000])
Draft probs distr tensor(0., device='cuda:0') tensor(3.1250e-05, device='cuda:0') tensor(1., device='cuda:0') torch.Size([2, 5, 32000])
VERIFY STEP ENDED tensor([[   -1,    -1,    -1,    -1,    -1,    -1],
        [29889, 29884,    -1,    -1,    -1,    -1],
        [29901,    -1,    -1,    -1,    -1,    -1]], device='cuda:0')
SAMPLER OUTPUT LIST 2x3
LLM STep outputs [SamplerOutput(outputs=[CompletionSequenceGroupOutput(samples=[SequenceOutput(parent_seq_id=2, output_token=-1, logprobs={-1: Logprob(logprob=0.0, rank=-1, decoded_token=None)})], prompt_logprobs=None), CompletionSequenceGroupOutput(samples=[SequenceOutput(parent_seq_id=0, output_token=29889, logprobs={29889: Logprob(logprob=0.0, rank=-1, decoded_token=None)})], prompt_logprobs=None), CompletionSequenceGroupOutput(samples=[SequenceOutput(parent_seq_id=1, output_token=29901, logprobs={29901: Logprob(logprob=0.0, rank=-1, decoded_token=None)})], prompt_logprobs=None)], sampled_token_probs=None, sampled_token_ids=None, spec_decode_worker_metrics=None), SamplerOutput(outputs=[CompletionSequenceGroupOutput(samples=[SequenceOutput(parent_seq_id=2, output_token=-1, logprobs={-1: Logprob(logprob=0.0, rank=-1, decoded_token=None)})], prompt_logprobs=None), CompletionSequenceGroupOutput(samples=[SequenceOutput(parent_seq_id=0, output_token=29884, logprobs={29884: Logprob(logprob=0.0, rank=-1, decoded_token=None)})], prompt_logprobs=None), CompletionSequenceGroupOutput(samples=[SequenceOutput(parent_seq_id=1, output_token=-1, logprobs={-1: Logprob(logprob=0.0, rank=-1, decoded_token=None)})], prompt_logprobs=None)], sampled_token_probs=None, sampled_token_ids=None, spec_decode_worker_metrics=None)]
LLM process output indices range(0, 3)
OUTPUTS PER SEQ GROUP (ONE IS PROMPT THE OTHER NOT) [[CompletionSequenceGroupOutput(samples=[SequenceOutput(parent_seq_id=2, output_token=-1, logprobs={-1: Logprob(logprob=0.0, rank=-1, decoded_token=None)})], prompt_logprobs=None), CompletionSequenceGroupOutput(samples=[SequenceOutput(parent_seq_id=2, output_token=-1, logprobs={-1: Logprob(logprob=0.0, rank=-1, decoded_token=None)})], prompt_logprobs=None)], [CompletionSequenceGroupOutput(samples=[SequenceOutput(parent_seq_id=0, output_token=29889, logprobs={29889: Logprob(logprob=0.0, rank=-1, decoded_token=None)})], prompt_logprobs=None), CompletionSequenceGroupOutput(samples=[SequenceOutput(parent_seq_id=0, output_token=29884, logprobs={29884: Logprob(logprob=0.0, rank=-1, decoded_token=None)})], prompt_logprobs=None)], [CompletionSequenceGroupOutput(samples=[SequenceOutput(parent_seq_id=1, output_token=29901, logprobs={29901: Logprob(logprob=0.0, rank=-1, decoded_token=None)})], prompt_logprobs=None), CompletionSequenceGroupOutput(samples=[SequenceOutput(parent_seq_id=1, output_token=-1, logprobs={-1: Logprob(logprob=0.0, rank=-1, decoded_token=None)})], prompt_logprobs=None)]]
output has multiples True 2
Updating computed tokens manually
META DO_SAMPLE False
OUT [CompletionSequenceGroupOutput(samples=[SequenceOutput(parent_seq_id=2, output_token=-1, logprobs={-1: Logprob(logprob=0.0, rank=-1, decoded_token=None)})], prompt_logprobs=None), CompletionSequenceGroupOutput(samples=[SequenceOutput(parent_seq_id=2, output_token=-1, logprobs={-1: Logprob(logprob=0.0, rank=-1, decoded_token=None)})], prompt_logprobs=None)] 



SEQ GROUP DIO CANE SHOULD SAMPLE HERE False
SequenceGroup(request_id=2, sampling_params=SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=True, max_tokens=32, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), guided_decoding=None, num_seqs=1)
output has multiples True 2
SEQ GROUP DIO CANE SHOULD SAMPLE HERE True
SequenceGroup(request_id=0, sampling_params=SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=True, max_tokens=32, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), guided_decoding=None, num_seqs=1)
output has multiples True 2
SEQ GROUP DIO CANE SHOULD SAMPLE HERE True
SequenceGroup(request_id=1, sampling_params=SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=True, max_tokens=32, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), guided_decoding=None, num_seqs=1)
PROMPTS (is prompt, K, token_chunk, do_sample):  [(True, 0, 2, False), (False, None, 1, True), (False, None, 1, True)]
SPECULATIING WITH REQ METADATA [SequenceGroupMetadata(request_id='2', is_prompt=True, seq_data={2: SequenceData(prompt_token_ids=array('l', [1, 450, 6673, 310, 278, 3303, 3900, 338, 263, 767, 29892, 825, 29879, 670, 1024, 29973]), output_token_ids=(), cumulative_logprob=0.0, get_num_computed_tokens=5}, sampling_params=SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=True, max_tokens=32, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), guided_decoding=None, block_tables={2: [3]}, do_sample=False, pooling_params=None, lora_request=None, computed_block_nums=[], state=SequenceGroupState(num_steps=1, current_step=0), multi_modal_data={}, encoder_seq_data=None, cross_block_table=None, prompt_adapter_request=None, token_chunk_size=2, num_speculative_tokens=0), SequenceGroupMetadata(request_id='0', is_prompt=False, seq_data={0: SequenceData(prompt_token_ids=array('l', [1, 450, 6673, 310, 278, 3303, 3900, 338, 263, 767, 29892, 825, 29879, 670, 1024, 29973]), output_token_ids=(13, 1576, 261, 29901, 450, 759, 391, 414, 29889, 450, 16912, 29889, 29884), cumulative_logprob=0.0, get_num_computed_tokens=29}, sampling_params=SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=True, max_tokens=32, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), guided_decoding=None, block_tables={0: [0, 1, 5]}, do_sample=True, pooling_params=None, lora_request=None, computed_block_nums=[], state=SequenceGroupState(num_steps=1, current_step=0), multi_modal_data={}, encoder_seq_data=None, cross_block_table=None, prompt_adapter_request=None, token_chunk_size=1, num_speculative_tokens=None), SequenceGroupMetadata(request_id='1', is_prompt=False, seq_data={1: SequenceData(prompt_token_ids=array('l', [1, 450, 5434, 310, 319, 29902, 338, 29892, 297, 596, 1914, 3838, 29901, 29871]), output_token_ids=(29896, 29900, 29901), cumulative_logprob=0.0, get_num_computed_tokens=17}, sampling_params=SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=True, max_tokens=32, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), guided_decoding=None, block_tables={1: [2, 4]}, do_sample=True, pooling_params=None, lora_request=None, computed_block_nums=[], state=SequenceGroupState(num_steps=1, current_step=0), multi_modal_data={}, encoder_seq_data=None, cross_block_table=None, prompt_adapter_request=None, token_chunk_size=1, num_speculative_tokens=None)]
SCORING ON THE UNION OF REQUESTS (includes expanded batch), is prompt? [False, False, False, False, False, False, False, False, False, False, False, False, True]
SCORING ON THE UNION OF REQUESTS (includes expanded batch), do_sample? [True, True, True, True, True, True, True, True, True, True, True, True, False]
MIXEEED BATCH BOYS!
Proposal len X do_sample X is_prompt [(0, (False, True)), (5, (True, False)), (5, (True, False))]
Target probs distr tensor(0., device='cuda:0') tensor(3.1250e-05, device='cuda:0') tensor(1., device='cuda:0') torch.Size([2, 6, 32000])
Draft probs distr tensor(0., device='cuda:0') tensor(3.1250e-05, device='cuda:0') tensor(1., device='cuda:0') torch.Size([2, 5, 32000])
VERIFY STEP ENDED tensor([[   -1,    -1,    -1,    -1,    -1,    -1],
        [29889, 29884,    -1,    -1,    -1,    -1],
        [  450,    -1,    -1,    -1,    -1,    -1]], device='cuda:0')
SAMPLER OUTPUT LIST 2x3
LLM STep outputs [SamplerOutput(outputs=[CompletionSequenceGroupOutput(samples=[SequenceOutput(parent_seq_id=2, output_token=-1, logprobs={-1: Logprob(logprob=0.0, rank=-1, decoded_token=None)})], prompt_logprobs=None), CompletionSequenceGroupOutput(samples=[SequenceOutput(parent_seq_id=0, output_token=29889, logprobs={29889: Logprob(logprob=0.0, rank=-1, decoded_token=None)})], prompt_logprobs=None), CompletionSequenceGroupOutput(samples=[SequenceOutput(parent_seq_id=1, output_token=450, logprobs={450: Logprob(logprob=0.0, rank=-1, decoded_token=None)})], prompt_logprobs=None)], sampled_token_probs=None, sampled_token_ids=None, spec_decode_worker_metrics=None), SamplerOutput(outputs=[CompletionSequenceGroupOutput(samples=[SequenceOutput(parent_seq_id=2, output_token=-1, logprobs={-1: Logprob(logprob=0.0, rank=-1, decoded_token=None)})], prompt_logprobs=None), CompletionSequenceGroupOutput(samples=[SequenceOutput(parent_seq_id=0, output_token=29884, logprobs={29884: Logprob(logprob=0.0, rank=-1, decoded_token=None)})], prompt_logprobs=None), CompletionSequenceGroupOutput(samples=[SequenceOutput(parent_seq_id=1, output_token=-1, logprobs={-1: Logprob(logprob=0.0, rank=-1, decoded_token=None)})], prompt_logprobs=None)], sampled_token_probs=None, sampled_token_ids=None, spec_decode_worker_metrics=None)]
LLM process output indices range(0, 3)
OUTPUTS PER SEQ GROUP (ONE IS PROMPT THE OTHER NOT) [[CompletionSequenceGroupOutput(samples=[SequenceOutput(parent_seq_id=2, output_token=-1, logprobs={-1: Logprob(logprob=0.0, rank=-1, decoded_token=None)})], prompt_logprobs=None), CompletionSequenceGroupOutput(samples=[SequenceOutput(parent_seq_id=2, output_token=-1, logprobs={-1: Logprob(logprob=0.0, rank=-1, decoded_token=None)})], prompt_logprobs=None)], [CompletionSequenceGroupOutput(samples=[SequenceOutput(parent_seq_id=0, output_token=29889, logprobs={29889: Logprob(logprob=0.0, rank=-1, decoded_token=None)})], prompt_logprobs=None), CompletionSequenceGroupOutput(samples=[SequenceOutput(parent_seq_id=0, output_token=29884, logprobs={29884: Logprob(logprob=0.0, rank=-1, decoded_token=None)})], prompt_logprobs=None)], [CompletionSequenceGroupOutput(samples=[SequenceOutput(parent_seq_id=1, output_token=450, logprobs={450: Logprob(logprob=0.0, rank=-1, decoded_token=None)})], prompt_logprobs=None), CompletionSequenceGroupOutput(samples=[SequenceOutput(parent_seq_id=1, output_token=-1, logprobs={-1: Logprob(logprob=0.0, rank=-1, decoded_token=None)})], prompt_logprobs=None)]]
output has multiples True 2
Updating computed tokens manually
META DO_SAMPLE False
OUT [CompletionSequenceGroupOutput(samples=[SequenceOutput(parent_seq_id=2, output_token=-1, logprobs={-1: Logprob(logprob=0.0, rank=-1, decoded_token=None)})], prompt_logprobs=None), CompletionSequenceGroupOutput(samples=[SequenceOutput(parent_seq_id=2, output_token=-1, logprobs={-1: Logprob(logprob=0.0, rank=-1, decoded_token=None)})], prompt_logprobs=None)] 



SEQ GROUP DIO CANE SHOULD SAMPLE HERE False
SequenceGroup(request_id=2, sampling_params=SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=True, max_tokens=32, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), guided_decoding=None, num_seqs=1)
output has multiples True 2
SEQ GROUP DIO CANE SHOULD SAMPLE HERE True
SequenceGroup(request_id=0, sampling_params=SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=True, max_tokens=32, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), guided_decoding=None, num_seqs=1)
output has multiples True 2
SEQ GROUP DIO CANE SHOULD SAMPLE HERE True
SequenceGroup(request_id=1, sampling_params=SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=True, max_tokens=32, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), guided_decoding=None, num_seqs=1)
PROMPTS (is prompt, K, token_chunk, do_sample):  [(True, 0, 2, False), (False, None, 1, True), (False, None, 1, True)]
SPECULATIING WITH REQ METADATA [SequenceGroupMetadata(request_id='2', is_prompt=True, seq_data={2: SequenceData(prompt_token_ids=array('l', [1, 450, 6673, 310, 278, 3303, 3900, 338, 263, 767, 29892, 825, 29879, 670, 1024, 29973]), output_token_ids=(), cumulative_logprob=0.0, get_num_computed_tokens=7}, sampling_params=SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=True, max_tokens=32, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), guided_decoding=None, block_tables={2: [3]}, do_sample=False, pooling_params=None, lora_request=None, computed_block_nums=[], state=SequenceGroupState(num_steps=1, current_step=0), multi_modal_data={}, encoder_seq_data=None, cross_block_table=None, prompt_adapter_request=None, token_chunk_size=2, num_speculative_tokens=0), SequenceGroupMetadata(request_id='0', is_prompt=False, seq_data={0: SequenceData(prompt_token_ids=array('l', [1, 450, 6673, 310, 278, 3303, 3900, 338, 263, 767, 29892, 825, 29879, 670, 1024, 29973]), output_token_ids=(13, 1576, 261, 29901, 450, 759, 391, 414, 29889, 450, 16912, 29889, 29884, 29889, 29884), cumulative_logprob=0.0, get_num_computed_tokens=31}, sampling_params=SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=True, max_tokens=32, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), guided_decoding=None, block_tables={0: [0, 1, 5]}, do_sample=True, pooling_params=None, lora_request=None, computed_block_nums=[], state=SequenceGroupState(num_steps=1, current_step=0), multi_modal_data={}, encoder_seq_data=None, cross_block_table=None, prompt_adapter_request=None, token_chunk_size=1, num_speculative_tokens=None), SequenceGroupMetadata(request_id='1', is_prompt=False, seq_data={1: SequenceData(prompt_token_ids=array('l', [1, 450, 5434, 310, 319, 29902, 338, 29892, 297, 596, 1914, 3838, 29901, 29871]), output_token_ids=(29896, 29900, 29901, 450), cumulative_logprob=0.0, get_num_computed_tokens=18}, sampling_params=SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=True, max_tokens=32, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), guided_decoding=None, block_tables={1: [2, 4]}, do_sample=True, pooling_params=None, lora_request=None, computed_block_nums=[], state=SequenceGroupState(num_steps=1, current_step=0), multi_modal_data={}, encoder_seq_data=None, cross_block_table=None, prompt_adapter_request=None, token_chunk_size=1, num_speculative_tokens=None)]
SCORING ON THE UNION OF REQUESTS (includes expanded batch), is prompt? [False, False, False, False, False, False, False, False, False, False, False, False, True]
SCORING ON THE UNION OF REQUESTS (includes expanded batch), do_sample? [True, True, True, True, True, True, True, True, True, True, True, True, False]
MIXEEED BATCH BOYS!
Proposal len X do_sample X is_prompt [(0, (False, True)), (5, (True, False)), (5, (True, False))]
Target probs distr tensor(0., device='cuda:0') tensor(3.1250e-05, device='cuda:0') tensor(1., device='cuda:0') torch.Size([2, 6, 32000])
Draft probs distr tensor(0., device='cuda:0') tensor(3.1250e-05, device='cuda:0') tensor(1., device='cuda:0') torch.Size([2, 5, 32000])
VERIFY STEP ENDED tensor([[   -1,    -1,    -1,    -1,    -1,    -1],
        [29889, 29884,    -1,    -1,    -1,    -1],
        [29877,    -1,    -1,    -1,    -1,    -1]], device='cuda:0')
SAMPLER OUTPUT LIST 2x3
LLM STep outputs [SamplerOutput(outputs=[CompletionSequenceGroupOutput(samples=[SequenceOutput(parent_seq_id=2, output_token=-1, logprobs={-1: Logprob(logprob=0.0, rank=-1, decoded_token=None)})], prompt_logprobs=None), CompletionSequenceGroupOutput(samples=[SequenceOutput(parent_seq_id=0, output_token=29889, logprobs={29889: Logprob(logprob=0.0, rank=-1, decoded_token=None)})], prompt_logprobs=None), CompletionSequenceGroupOutput(samples=[SequenceOutput(parent_seq_id=1, output_token=29877, logprobs={29877: Logprob(logprob=0.0, rank=-1, decoded_token=None)})], prompt_logprobs=None)], sampled_token_probs=None, sampled_token_ids=None, spec_decode_worker_metrics=None), SamplerOutput(outputs=[CompletionSequenceGroupOutput(samples=[SequenceOutput(parent_seq_id=2, output_token=-1, logprobs={-1: Logprob(logprob=0.0, rank=-1, decoded_token=None)})], prompt_logprobs=None), CompletionSequenceGroupOutput(samples=[SequenceOutput(parent_seq_id=0, output_token=29884, logprobs={29884: Logprob(logprob=0.0, rank=-1, decoded_token=None)})], prompt_logprobs=None), CompletionSequenceGroupOutput(samples=[SequenceOutput(parent_seq_id=1, output_token=-1, logprobs={-1: Logprob(logprob=0.0, rank=-1, decoded_token=None)})], prompt_logprobs=None)], sampled_token_probs=None, sampled_token_ids=None, spec_decode_worker_metrics=None)]
LLM process output indices range(0, 3)
OUTPUTS PER SEQ GROUP (ONE IS PROMPT THE OTHER NOT) [[CompletionSequenceGroupOutput(samples=[SequenceOutput(parent_seq_id=2, output_token=-1, logprobs={-1: Logprob(logprob=0.0, rank=-1, decoded_token=None)})], prompt_logprobs=None), CompletionSequenceGroupOutput(samples=[SequenceOutput(parent_seq_id=2, output_token=-1, logprobs={-1: Logprob(logprob=0.0, rank=-1, decoded_token=None)})], prompt_logprobs=None)], [CompletionSequenceGroupOutput(samples=[SequenceOutput(parent_seq_id=0, output_token=29889, logprobs={29889: Logprob(logprob=0.0, rank=-1, decoded_token=None)})], prompt_logprobs=None), CompletionSequenceGroupOutput(samples=[SequenceOutput(parent_seq_id=0, output_token=29884, logprobs={29884: Logprob(logprob=0.0, rank=-1, decoded_token=None)})], prompt_logprobs=None)], [CompletionSequenceGroupOutput(samples=[SequenceOutput(parent_seq_id=1, output_token=29877, logprobs={29877: Logprob(logprob=0.0, rank=-1, decoded_token=None)})], prompt_logprobs=None), CompletionSequenceGroupOutput(samples=[SequenceOutput(parent_seq_id=1, output_token=-1, logprobs={-1: Logprob(logprob=0.0, rank=-1, decoded_token=None)})], prompt_logprobs=None)]]
output has multiples True 2
Updating computed tokens manually
META DO_SAMPLE False
OUT [CompletionSequenceGroupOutput(samples=[SequenceOutput(parent_seq_id=2, output_token=-1, logprobs={-1: Logprob(logprob=0.0, rank=-1, decoded_token=None)})], prompt_logprobs=None), CompletionSequenceGroupOutput(samples=[SequenceOutput(parent_seq_id=2, output_token=-1, logprobs={-1: Logprob(logprob=0.0, rank=-1, decoded_token=None)})], prompt_logprobs=None)] 



SEQ GROUP DIO CANE SHOULD SAMPLE HERE False
SequenceGroup(request_id=2, sampling_params=SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=True, max_tokens=32, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), guided_decoding=None, num_seqs=1)
output has multiples True 2
SEQ GROUP DIO CANE SHOULD SAMPLE HERE True
SequenceGroup(request_id=0, sampling_params=SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=True, max_tokens=32, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), guided_decoding=None, num_seqs=1)
output has multiples True 2
SEQ GROUP DIO CANE SHOULD SAMPLE HERE True
SequenceGroup(request_id=1, sampling_params=SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=True, max_tokens=32, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), guided_decoding=None, num_seqs=1)
PROMPTS (is prompt, K, token_chunk, do_sample):  [(True, 0, 2, False), (False, None, 1, True), (False, None, 1, True)]
SPECULATIING WITH REQ METADATA [SequenceGroupMetadata(request_id='2', is_prompt=True, seq_data={2: SequenceData(prompt_token_ids=array('l', [1, 450, 6673, 310, 278, 3303, 3900, 338, 263, 767, 29892, 825, 29879, 670, 1024, 29973]), output_token_ids=(), cumulative_logprob=0.0, get_num_computed_tokens=9}, sampling_params=SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=True, max_tokens=32, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), guided_decoding=None, block_tables={2: [3]}, do_sample=False, pooling_params=None, lora_request=None, computed_block_nums=[], state=SequenceGroupState(num_steps=1, current_step=0), multi_modal_data={}, encoder_seq_data=None, cross_block_table=None, prompt_adapter_request=None, token_chunk_size=2, num_speculative_tokens=0), SequenceGroupMetadata(request_id='0', is_prompt=False, seq_data={0: SequenceData(prompt_token_ids=array('l', [1, 450, 6673, 310, 278, 3303, 3900, 338, 263, 767, 29892, 825, 29879, 670, 1024, 29973]), output_token_ids=(13, 1576, 261, 29901, 450, 759, 391, 414, 29889, 450, 16912, 29889, 29884, 29889, 29884, 29889, 29884), cumulative_logprob=0.0, get_num_computed_tokens=33}, sampling_params=SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=True, max_tokens=32, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), guided_decoding=None, block_tables={0: [0, 1, 5]}, do_sample=True, pooling_params=None, lora_request=None, computed_block_nums=[], state=SequenceGroupState(num_steps=1, current_step=0), multi_modal_data={}, encoder_seq_data=None, cross_block_table=None, prompt_adapter_request=None, token_chunk_size=1, num_speculative_tokens=None), SequenceGroupMetadata(request_id='1', is_prompt=False, seq_data={1: SequenceData(prompt_token_ids=array('l', [1, 450, 5434, 310, 319, 29902, 338, 29892, 297, 596, 1914, 3838, 29901, 29871]), output_token_ids=(29896, 29900, 29901, 450, 29877), cumulative_logprob=0.0, get_num_computed_tokens=19}, sampling_params=SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=True, max_tokens=32, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), guided_decoding=None, block_tables={1: [2, 4]}, do_sample=True, pooling_params=None, lora_request=None, computed_block_nums=[], state=SequenceGroupState(num_steps=1, current_step=0), multi_modal_data={}, encoder_seq_data=None, cross_block_table=None, prompt_adapter_request=None, token_chunk_size=1, num_speculative_tokens=None)]
SCORING ON THE UNION OF REQUESTS (includes expanded batch), is prompt? [False, False, False, False, False, False, False, False, False, False, False, False, True]
SCORING ON THE UNION OF REQUESTS (includes expanded batch), do_sample? [True, True, True, True, True, True, True, True, True, True, True, True, False]
MIXEEED BATCH BOYS!
Proposal len X do_sample X is_prompt [(0, (False, True)), (5, (True, False)), (5, (True, False))]
Target probs distr tensor(0., device='cuda:0') tensor(3.1250e-05, device='cuda:0') tensor(1., device='cuda:0') torch.Size([2, 6, 32000])
Draft probs distr tensor(0., device='cuda:0') tensor(3.1250e-05, device='cuda:0') tensor(1., device='cuda:0') torch.Size([2, 5, 32000])
VERIFY STEP ENDED tensor([[   -1,    -1,    -1,    -1,    -1,    -1],
        [  338,    -1,    -1,    -1,    -1,    -1],
        [29889,    -1,    -1,    -1,    -1,    -1]], device='cuda:0')
SAMPLER OUTPUT LIST 1x3
LLM STep outputs [SamplerOutput(outputs=[CompletionSequenceGroupOutput(samples=[SequenceOutput(parent_seq_id=2, output_token=-1, logprobs={-1: Logprob(logprob=0.0, rank=-1, decoded_token=None)})], prompt_logprobs=None), CompletionSequenceGroupOutput(samples=[SequenceOutput(parent_seq_id=0, output_token=338, logprobs={338: Logprob(logprob=0.0, rank=-1, decoded_token=None)})], prompt_logprobs=None), CompletionSequenceGroupOutput(samples=[SequenceOutput(parent_seq_id=1, output_token=29889, logprobs={29889: Logprob(logprob=0.0, rank=-1, decoded_token=None)})], prompt_logprobs=None)], sampled_token_probs=None, sampled_token_ids=None, spec_decode_worker_metrics=None)]
LLM process output indices range(0, 3)
OUTPUTS PER SEQ GROUP (ONE IS PROMPT THE OTHER NOT) [SamplerOutput(outputs=[CompletionSequenceGroupOutput(samples=[SequenceOutput(parent_seq_id=2, output_token=-1, logprobs={-1: Logprob(logprob=0.0, rank=-1, decoded_token=None)})], prompt_logprobs=None), CompletionSequenceGroupOutput(samples=[SequenceOutput(parent_seq_id=0, output_token=338, logprobs={338: Logprob(logprob=0.0, rank=-1, decoded_token=None)})], prompt_logprobs=None), CompletionSequenceGroupOutput(samples=[SequenceOutput(parent_seq_id=1, output_token=29889, logprobs={29889: Logprob(logprob=0.0, rank=-1, decoded_token=None)})], prompt_logprobs=None)], sampled_token_probs=None, sampled_token_ids=None, spec_decode_worker_metrics=None)]
output has multiples False 1
Updating computed tokens manually
META DO_SAMPLE False
OUT [CompletionSequenceGroupOutput(samples=[SequenceOutput(parent_seq_id=2, output_token=-1, logprobs={-1: Logprob(logprob=0.0, rank=-1, decoded_token=None)})], prompt_logprobs=None)] 



SEQ GROUP DIO CANE SHOULD SAMPLE HERE False
SequenceGroup(request_id=2, sampling_params=SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=True, max_tokens=32, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), guided_decoding=None, num_seqs=1)
output has multiples False 1
SEQ GROUP DIO CANE SHOULD SAMPLE HERE True
SequenceGroup(request_id=0, sampling_params=SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=True, max_tokens=32, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), guided_decoding=None, num_seqs=1)
output has multiples False 1
SEQ GROUP DIO CANE SHOULD SAMPLE HERE True
SequenceGroup(request_id=1, sampling_params=SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=True, max_tokens=32, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), guided_decoding=None, num_seqs=1)
PROMPTS (is prompt, K, token_chunk, do_sample):  [(True, 0, 2, False), (False, None, 1, True), (False, None, 1, True)]
SPECULATIING WITH REQ METADATA [SequenceGroupMetadata(request_id='2', is_prompt=True, seq_data={2: SequenceData(prompt_token_ids=array('l', [1, 450, 6673, 310, 278, 3303, 3900, 338, 263, 767, 29892, 825, 29879, 670, 1024, 29973]), output_token_ids=(), cumulative_logprob=0.0, get_num_computed_tokens=11}, sampling_params=SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=True, max_tokens=32, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), guided_decoding=None, block_tables={2: [3]}, do_sample=False, pooling_params=None, lora_request=None, computed_block_nums=[], state=SequenceGroupState(num_steps=1, current_step=0), multi_modal_data={}, encoder_seq_data=None, cross_block_table=None, prompt_adapter_request=None, token_chunk_size=2, num_speculative_tokens=0), SequenceGroupMetadata(request_id='0', is_prompt=False, seq_data={0: SequenceData(prompt_token_ids=array('l', [1, 450, 6673, 310, 278, 3303, 3900, 338, 263, 767, 29892, 825, 29879, 670, 1024, 29973]), output_token_ids=(13, 1576, 261, 29901, 450, 759, 391, 414, 29889, 450, 16912, 29889, 29884, 29889, 29884, 29889, 29884, 338), cumulative_logprob=0.0, get_num_computed_tokens=34}, sampling_params=SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=True, max_tokens=32, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), guided_decoding=None, block_tables={0: [0, 1, 5]}, do_sample=True, pooling_params=None, lora_request=None, computed_block_nums=[], state=SequenceGroupState(num_steps=1, current_step=0), multi_modal_data={}, encoder_seq_data=None, cross_block_table=None, prompt_adapter_request=None, token_chunk_size=1, num_speculative_tokens=None), SequenceGroupMetadata(request_id='1', is_prompt=False, seq_data={1: SequenceData(prompt_token_ids=array('l', [1, 450, 5434, 310, 319, 29902, 338, 29892, 297, 596, 1914, 3838, 29901, 29871]), output_token_ids=(29896, 29900, 29901, 450, 29877, 29889), cumulative_logprob=0.0, get_num_computed_tokens=20}, sampling_params=SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=True, max_tokens=32, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), guided_decoding=None, block_tables={1: [2, 4]}, do_sample=True, pooling_params=None, lora_request=None, computed_block_nums=[], state=SequenceGroupState(num_steps=1, current_step=0), multi_modal_data={}, encoder_seq_data=None, cross_block_table=None, prompt_adapter_request=None, token_chunk_size=1, num_speculative_tokens=None)]
SCORING ON THE UNION OF REQUESTS (includes expanded batch), is prompt? [False, False, False, False, False, False, False, False, False, False, False, False, True]
SCORING ON THE UNION OF REQUESTS (includes expanded batch), do_sample? [True, True, True, True, True, True, True, True, True, True, True, True, False]
MIXEEED BATCH BOYS!
Proposal len X do_sample X is_prompt [(0, (False, True)), (5, (True, False)), (5, (True, False))]
Target probs distr tensor(0., device='cuda:0') tensor(3.1250e-05, device='cuda:0') tensor(1., device='cuda:0') torch.Size([2, 6, 32000])
Draft probs distr tensor(0., device='cuda:0') tensor(3.1250e-05, device='cuda:0') tensor(1., device='cuda:0') torch.Size([2, 5, 32000])
VERIFY STEP ENDED tensor([[   -1,    -1,    -1,    -1,    -1,    -1],
        [  263, 29884,    -1,    -1,    -1,    -1],
        [29884,    -1,    -1,    -1,    -1,    -1]], device='cuda:0')
SAMPLER OUTPUT LIST 2x3
LLM STep outputs [SamplerOutput(outputs=[CompletionSequenceGroupOutput(samples=[SequenceOutput(parent_seq_id=2, output_token=-1, logprobs={-1: Logprob(logprob=0.0, rank=-1, decoded_token=None)})], prompt_logprobs=None), CompletionSequenceGroupOutput(samples=[SequenceOutput(parent_seq_id=0, output_token=263, logprobs={263: Logprob(logprob=0.0, rank=-1, decoded_token=None)})], prompt_logprobs=None), CompletionSequenceGroupOutput(samples=[SequenceOutput(parent_seq_id=1, output_token=29884, logprobs={29884: Logprob(logprob=0.0, rank=-1, decoded_token=None)})], prompt_logprobs=None)], sampled_token_probs=None, sampled_token_ids=None, spec_decode_worker_metrics=None), SamplerOutput(outputs=[CompletionSequenceGroupOutput(samples=[SequenceOutput(parent_seq_id=2, output_token=-1, logprobs={-1: Logprob(logprob=0.0, rank=-1, decoded_token=None)})], prompt_logprobs=None), CompletionSequenceGroupOutput(samples=[SequenceOutput(parent_seq_id=0, output_token=29884, logprobs={29884: Logprob(logprob=0.0, rank=-1, decoded_token=None)})], prompt_logprobs=None), CompletionSequenceGroupOutput(samples=[SequenceOutput(parent_seq_id=1, output_token=-1, logprobs={-1: Logprob(logprob=0.0, rank=-1, decoded_token=None)})], prompt_logprobs=None)], sampled_token_probs=None, sampled_token_ids=None, spec_decode_worker_metrics=None)]
LLM process output indices range(0, 3)
OUTPUTS PER SEQ GROUP (ONE IS PROMPT THE OTHER NOT) [[CompletionSequenceGroupOutput(samples=[SequenceOutput(parent_seq_id=2, output_token=-1, logprobs={-1: Logprob(logprob=0.0, rank=-1, decoded_token=None)})], prompt_logprobs=None), CompletionSequenceGroupOutput(samples=[SequenceOutput(parent_seq_id=2, output_token=-1, logprobs={-1: Logprob(logprob=0.0, rank=-1, decoded_token=None)})], prompt_logprobs=None)], [CompletionSequenceGroupOutput(samples=[SequenceOutput(parent_seq_id=0, output_token=263, logprobs={263: Logprob(logprob=0.0, rank=-1, decoded_token=None)})], prompt_logprobs=None), CompletionSequenceGroupOutput(samples=[SequenceOutput(parent_seq_id=0, output_token=29884, logprobs={29884: Logprob(logprob=0.0, rank=-1, decoded_token=None)})], prompt_logprobs=None)], [CompletionSequenceGroupOutput(samples=[SequenceOutput(parent_seq_id=1, output_token=29884, logprobs={29884: Logprob(logprob=0.0, rank=-1, decoded_token=None)})], prompt_logprobs=None), CompletionSequenceGroupOutput(samples=[SequenceOutput(parent_seq_id=1, output_token=-1, logprobs={-1: Logprob(logprob=0.0, rank=-1, decoded_token=None)})], prompt_logprobs=None)]]
output has multiples True 2
Updating computed tokens manually
META DO_SAMPLE False
OUT [CompletionSequenceGroupOutput(samples=[SequenceOutput(parent_seq_id=2, output_token=-1, logprobs={-1: Logprob(logprob=0.0, rank=-1, decoded_token=None)})], prompt_logprobs=None), CompletionSequenceGroupOutput(samples=[SequenceOutput(parent_seq_id=2, output_token=-1, logprobs={-1: Logprob(logprob=0.0, rank=-1, decoded_token=None)})], prompt_logprobs=None)] 



SEQ GROUP DIO CANE SHOULD SAMPLE HERE False
SequenceGroup(request_id=2, sampling_params=SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=True, max_tokens=32, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), guided_decoding=None, num_seqs=1)
output has multiples True 2
SEQ GROUP DIO CANE SHOULD SAMPLE HERE True
SequenceGroup(request_id=0, sampling_params=SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=True, max_tokens=32, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), guided_decoding=None, num_seqs=1)
output has multiples True 2
SEQ GROUP DIO CANE SHOULD SAMPLE HERE True
SequenceGroup(request_id=1, sampling_params=SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=True, max_tokens=32, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), guided_decoding=None, num_seqs=1)
PROMPTS (is prompt, K, token_chunk, do_sample):  [(True, 0, 2, False), (False, None, 1, True), (False, None, 1, True)]
SPECULATIING WITH REQ METADATA [SequenceGroupMetadata(request_id='2', is_prompt=True, seq_data={2: SequenceData(prompt_token_ids=array('l', [1, 450, 6673, 310, 278, 3303, 3900, 338, 263, 767, 29892, 825, 29879, 670, 1024, 29973]), output_token_ids=(), cumulative_logprob=0.0, get_num_computed_tokens=13}, sampling_params=SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=True, max_tokens=32, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), guided_decoding=None, block_tables={2: [3]}, do_sample=False, pooling_params=None, lora_request=None, computed_block_nums=[], state=SequenceGroupState(num_steps=1, current_step=0), multi_modal_data={}, encoder_seq_data=None, cross_block_table=None, prompt_adapter_request=None, token_chunk_size=2, num_speculative_tokens=0), SequenceGroupMetadata(request_id='0', is_prompt=False, seq_data={0: SequenceData(prompt_token_ids=array('l', [1, 450, 6673, 310, 278, 3303, 3900, 338, 263, 767, 29892, 825, 29879, 670, 1024, 29973]), output_token_ids=(13, 1576, 261, 29901, 450, 759, 391, 414, 29889, 450, 16912, 29889, 29884, 29889, 29884, 29889, 29884, 338, 263, 29884), cumulative_logprob=0.0, get_num_computed_tokens=36}, sampling_params=SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=True, max_tokens=32, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), guided_decoding=None, block_tables={0: [0, 1, 5]}, do_sample=True, pooling_params=None, lora_request=None, computed_block_nums=[], state=SequenceGroupState(num_steps=1, current_step=0), multi_modal_data={}, encoder_seq_data=None, cross_block_table=None, prompt_adapter_request=None, token_chunk_size=1, num_speculative_tokens=None), SequenceGroupMetadata(request_id='1', is_prompt=False, seq_data={1: SequenceData(prompt_token_ids=array('l', [1, 450, 5434, 310, 319, 29902, 338, 29892, 297, 596, 1914, 3838, 29901, 29871]), output_token_ids=(29896, 29900, 29901, 450, 29877, 29889, 29884), cumulative_logprob=0.0, get_num_computed_tokens=21}, sampling_params=SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=True, max_tokens=32, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), guided_decoding=None, block_tables={1: [2, 4]}, do_sample=True, pooling_params=None, lora_request=None, computed_block_nums=[], state=SequenceGroupState(num_steps=1, current_step=0), multi_modal_data={}, encoder_seq_data=None, cross_block_table=None, prompt_adapter_request=None, token_chunk_size=1, num_speculative_tokens=None)]
SCORING ON THE UNION OF REQUESTS (includes expanded batch), is prompt? [False, False, False, False, False, False, False, False, False, False, False, False, True]
SCORING ON THE UNION OF REQUESTS (includes expanded batch), do_sample? [True, True, True, True, True, True, True, True, True, True, True, True, False]
MIXEEED BATCH BOYS!
Proposal len X do_sample X is_prompt [(0, (False, True)), (5, (True, False)), (5, (True, False))]
Target probs distr tensor(0., device='cuda:0') tensor(3.1250e-05, device='cuda:0') tensor(1., device='cuda:0') torch.Size([2, 6, 32000])
Draft probs distr tensor(0., device='cuda:0') tensor(3.1250e-05, device='cuda:0') tensor(1., device='cuda:0') torch.Size([2, 5, 32000])
VERIFY STEP ENDED tensor([[   -1,    -1,    -1,    -1,    -1,    -1],
        [29889, 16912,    -1,    -1,    -1,    -1],
        [29889, 29879,    -1,    -1,    -1,    -1]], device='cuda:0')
SAMPLER OUTPUT LIST 2x3
LLM STep outputs [SamplerOutput(outputs=[CompletionSequenceGroupOutput(samples=[SequenceOutput(parent_seq_id=2, output_token=-1, logprobs={-1: Logprob(logprob=0.0, rank=-1, decoded_token=None)})], prompt_logprobs=None), CompletionSequenceGroupOutput(samples=[SequenceOutput(parent_seq_id=0, output_token=29889, logprobs={29889: Logprob(logprob=0.0, rank=-1, decoded_token=None)})], prompt_logprobs=None), CompletionSequenceGroupOutput(samples=[SequenceOutput(parent_seq_id=1, output_token=29889, logprobs={29889: Logprob(logprob=0.0, rank=-1, decoded_token=None)})], prompt_logprobs=None)], sampled_token_probs=None, sampled_token_ids=None, spec_decode_worker_metrics=None), SamplerOutput(outputs=[CompletionSequenceGroupOutput(samples=[SequenceOutput(parent_seq_id=2, output_token=-1, logprobs={-1: Logprob(logprob=0.0, rank=-1, decoded_token=None)})], prompt_logprobs=None), CompletionSequenceGroupOutput(samples=[SequenceOutput(parent_seq_id=0, output_token=16912, logprobs={16912: Logprob(logprob=0.0, rank=-1, decoded_token=None)})], prompt_logprobs=None), CompletionSequenceGroupOutput(samples=[SequenceOutput(parent_seq_id=1, output_token=29879, logprobs={29879: Logprob(logprob=0.0, rank=-1, decoded_token=None)})], prompt_logprobs=None)], sampled_token_probs=None, sampled_token_ids=None, spec_decode_worker_metrics=None)]
LLM process output indices range(0, 3)
OUTPUTS PER SEQ GROUP (ONE IS PROMPT THE OTHER NOT) [[CompletionSequenceGroupOutput(samples=[SequenceOutput(parent_seq_id=2, output_token=-1, logprobs={-1: Logprob(logprob=0.0, rank=-1, decoded_token=None)})], prompt_logprobs=None), CompletionSequenceGroupOutput(samples=[SequenceOutput(parent_seq_id=2, output_token=-1, logprobs={-1: Logprob(logprob=0.0, rank=-1, decoded_token=None)})], prompt_logprobs=None)], [CompletionSequenceGroupOutput(samples=[SequenceOutput(parent_seq_id=0, output_token=29889, logprobs={29889: Logprob(logprob=0.0, rank=-1, decoded_token=None)})], prompt_logprobs=None), CompletionSequenceGroupOutput(samples=[SequenceOutput(parent_seq_id=0, output_token=16912, logprobs={16912: Logprob(logprob=0.0, rank=-1, decoded_token=None)})], prompt_logprobs=None)], [CompletionSequenceGroupOutput(samples=[SequenceOutput(parent_seq_id=1, output_token=29889, logprobs={29889: Logprob(logprob=0.0, rank=-1, decoded_token=None)})], prompt_logprobs=None), CompletionSequenceGroupOutput(samples=[SequenceOutput(parent_seq_id=1, output_token=29879, logprobs={29879: Logprob(logprob=0.0, rank=-1, decoded_token=None)})], prompt_logprobs=None)]]
output has multiples True 2
Updating computed tokens manually
META DO_SAMPLE False
OUT [CompletionSequenceGroupOutput(samples=[SequenceOutput(parent_seq_id=2, output_token=-1, logprobs={-1: Logprob(logprob=0.0, rank=-1, decoded_token=None)})], prompt_logprobs=None), CompletionSequenceGroupOutput(samples=[SequenceOutput(parent_seq_id=2, output_token=-1, logprobs={-1: Logprob(logprob=0.0, rank=-1, decoded_token=None)})], prompt_logprobs=None)] 



SEQ GROUP DIO CANE SHOULD SAMPLE HERE False
SequenceGroup(request_id=2, sampling_params=SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=True, max_tokens=32, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), guided_decoding=None, num_seqs=1)
output has multiples True 2
SEQ GROUP DIO CANE SHOULD SAMPLE HERE True
SequenceGroup(request_id=0, sampling_params=SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=True, max_tokens=32, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), guided_decoding=None, num_seqs=1)
output has multiples True 2
SEQ GROUP DIO CANE SHOULD SAMPLE HERE True
SequenceGroup(request_id=1, sampling_params=SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=True, max_tokens=32, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), guided_decoding=None, num_seqs=1)
PROMPTS (is prompt, K, token_chunk, do_sample):  [(True, 0, 1, False), (True, 0, 1, True), (False, None, 1, True), (False, None, 1, True)]
SPECULATIING WITH REQ METADATA [SequenceGroupMetadata(request_id='3', is_prompt=True, seq_data={3: SequenceData(prompt_token_ids=array('l', [1, 450, 5434, 310, 319, 29902, 338, 29892, 297, 596, 1914, 3838, 29901, 29871]), output_token_ids=(), cumulative_logprob=0.0, get_num_computed_tokens=0}, sampling_params=SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=True, max_tokens=32, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), guided_decoding=None, block_tables={3: [6]}, do_sample=False, pooling_params=None, lora_request=None, computed_block_nums=[], state=SequenceGroupState(num_steps=1, current_step=0), multi_modal_data={}, encoder_seq_data=None, cross_block_table=None, prompt_adapter_request=None, token_chunk_size=1, num_speculative_tokens=0), SequenceGroupMetadata(request_id='2', is_prompt=True, seq_data={2: SequenceData(prompt_token_ids=array('l', [1, 450, 6673, 310, 278, 3303, 3900, 338, 263, 767, 29892, 825, 29879, 670, 1024, 29973]), output_token_ids=(), cumulative_logprob=0.0, get_num_computed_tokens=15}, sampling_params=SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=True, max_tokens=32, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), guided_decoding=None, block_tables={2: [3]}, do_sample=True, pooling_params=None, lora_request=None, computed_block_nums=[], state=SequenceGroupState(num_steps=1, current_step=0), multi_modal_data={}, encoder_seq_data=None, cross_block_table=None, prompt_adapter_request=None, token_chunk_size=1, num_speculative_tokens=0), SequenceGroupMetadata(request_id='0', is_prompt=False, seq_data={0: SequenceData(prompt_token_ids=array('l', [1, 450, 6673, 310, 278, 3303, 3900, 338, 263, 767, 29892, 825, 29879, 670, 1024, 29973]), output_token_ids=(13, 1576, 261, 29901, 450, 759, 391, 414, 29889, 450, 16912, 29889, 29884, 29889, 29884, 29889, 29884, 338, 263, 29884, 29889, 16912), cumulative_logprob=0.0, get_num_computed_tokens=38}, sampling_params=SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=True, max_tokens=32, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), guided_decoding=None, block_tables={0: [0, 1, 5]}, do_sample=True, pooling_params=None, lora_request=None, computed_block_nums=[], state=SequenceGroupState(num_steps=1, current_step=0), multi_modal_data={}, encoder_seq_data=None, cross_block_table=None, prompt_adapter_request=None, token_chunk_size=1, num_speculative_tokens=None), SequenceGroupMetadata(request_id='1', is_prompt=False, seq_data={1: SequenceData(prompt_token_ids=array('l', [1, 450, 5434, 310, 319, 29902, 338, 29892, 297, 596, 1914, 3838, 29901, 29871]), output_token_ids=(29896, 29900, 29901, 450, 29877, 29889, 29884, 29889, 29879), cumulative_logprob=0.0, get_num_computed_tokens=23}, sampling_params=SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=True, max_tokens=32, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), guided_decoding=None, block_tables={1: [2, 4]}, do_sample=True, pooling_params=None, lora_request=None, computed_block_nums=[], state=SequenceGroupState(num_steps=1, current_step=0), multi_modal_data={}, encoder_seq_data=None, cross_block_table=None, prompt_adapter_request=None, token_chunk_size=1, num_speculative_tokens=None)]
SCORING ON THE UNION OF REQUESTS (includes expanded batch), is prompt? [False, False, False, False, False, False, False, False, False, False, False, False, True, True]
SCORING ON THE UNION OF REQUESTS (includes expanded batch), do_sample? [True, True, True, True, True, True, True, True, True, True, True, True, False, True]
MIXEEED BATCH BOYS!
Proposal len X do_sample X is_prompt [(0, (False, True)), (0, (True, True)), (5, (True, False)), (5, (True, False))]
Target probs distr tensor(0., device='cuda:0') tensor(3.1250e-05, device='cuda:0') tensor(1., device='cuda:0') torch.Size([2, 6, 32000])
Draft probs distr tensor(0., device='cuda:0') tensor(3.1250e-05, device='cuda:0') tensor(1., device='cuda:0') torch.Size([2, 5, 32000])
VERIFY STEP ENDED tensor([[   13,    -1,    -1,    -1,    -1,    -1],
        [   13,    -1,    -1,    -1,    -1,    -1],
        [30010,    -1,    -1,    -1,    -1,    -1],
        [29889,    13, 29900,    -1,    -1,    -1]], device='cuda:0')
SAMPLER OUTPUT LIST 3x4
LLM STep outputs [SamplerOutput(outputs=[CompletionSequenceGroupOutput(samples=[SequenceOutput(parent_seq_id=3, output_token=13, logprobs={13: Logprob(logprob=0.0, rank=-1, decoded_token=None)})], prompt_logprobs=None), CompletionSequenceGroupOutput(samples=[SequenceOutput(parent_seq_id=2, output_token=13, logprobs={13: Logprob(logprob=0.0, rank=-1, decoded_token=None)})], prompt_logprobs=None), CompletionSequenceGroupOutput(samples=[SequenceOutput(parent_seq_id=0, output_token=30010, logprobs={30010: Logprob(logprob=0.0, rank=-1, decoded_token=None)})], prompt_logprobs=None), CompletionSequenceGroupOutput(samples=[SequenceOutput(parent_seq_id=1, output_token=29889, logprobs={29889: Logprob(logprob=0.0, rank=-1, decoded_token=None)})], prompt_logprobs=None)], sampled_token_probs=None, sampled_token_ids=None, spec_decode_worker_metrics=None), SamplerOutput(outputs=[CompletionSequenceGroupOutput(samples=[SequenceOutput(parent_seq_id=3, output_token=-1, logprobs={-1: Logprob(logprob=0.0, rank=-1, decoded_token=None)})], prompt_logprobs=None), CompletionSequenceGroupOutput(samples=[SequenceOutput(parent_seq_id=2, output_token=-1, logprobs={-1: Logprob(logprob=0.0, rank=-1, decoded_token=None)})], prompt_logprobs=None), CompletionSequenceGroupOutput(samples=[SequenceOutput(parent_seq_id=0, output_token=-1, logprobs={-1: Logprob(logprob=0.0, rank=-1, decoded_token=None)})], prompt_logprobs=None), CompletionSequenceGroupOutput(samples=[SequenceOutput(parent_seq_id=1, output_token=13, logprobs={13: Logprob(logprob=0.0, rank=-1, decoded_token=None)})], prompt_logprobs=None)], sampled_token_probs=None, sampled_token_ids=None, spec_decode_worker_metrics=None), SamplerOutput(outputs=[CompletionSequenceGroupOutput(samples=[SequenceOutput(parent_seq_id=3, output_token=-1, logprobs={-1: Logprob(logprob=0.0, rank=-1, decoded_token=None)})], prompt_logprobs=None), CompletionSequenceGroupOutput(samples=[SequenceOutput(parent_seq_id=2, output_token=-1, logprobs={-1: Logprob(logprob=0.0, rank=-1, decoded_token=None)})], prompt_logprobs=None), CompletionSequenceGroupOutput(samples=[SequenceOutput(parent_seq_id=0, output_token=-1, logprobs={-1: Logprob(logprob=0.0, rank=-1, decoded_token=None)})], prompt_logprobs=None), CompletionSequenceGroupOutput(samples=[SequenceOutput(parent_seq_id=1, output_token=29900, logprobs={29900: Logprob(logprob=0.0, rank=-1, decoded_token=None)})], prompt_logprobs=None)], sampled_token_probs=None, sampled_token_ids=None, spec_decode_worker_metrics=None)]
LLM process output indices range(0, 4)
OUTPUTS PER SEQ GROUP (ONE IS PROMPT THE OTHER NOT) [[CompletionSequenceGroupOutput(samples=[SequenceOutput(parent_seq_id=3, output_token=13, logprobs={13: Logprob(logprob=0.0, rank=-1, decoded_token=None)})], prompt_logprobs=None), CompletionSequenceGroupOutput(samples=[SequenceOutput(parent_seq_id=3, output_token=-1, logprobs={-1: Logprob(logprob=0.0, rank=-1, decoded_token=None)})], prompt_logprobs=None), CompletionSequenceGroupOutput(samples=[SequenceOutput(parent_seq_id=3, output_token=-1, logprobs={-1: Logprob(logprob=0.0, rank=-1, decoded_token=None)})], prompt_logprobs=None)], [CompletionSequenceGroupOutput(samples=[SequenceOutput(parent_seq_id=2, output_token=13, logprobs={13: Logprob(logprob=0.0, rank=-1, decoded_token=None)})], prompt_logprobs=None), CompletionSequenceGroupOutput(samples=[SequenceOutput(parent_seq_id=2, output_token=-1, logprobs={-1: Logprob(logprob=0.0, rank=-1, decoded_token=None)})], prompt_logprobs=None), CompletionSequenceGroupOutput(samples=[SequenceOutput(parent_seq_id=2, output_token=-1, logprobs={-1: Logprob(logprob=0.0, rank=-1, decoded_token=None)})], prompt_logprobs=None)], [CompletionSequenceGroupOutput(samples=[SequenceOutput(parent_seq_id=0, output_token=30010, logprobs={30010: Logprob(logprob=0.0, rank=-1, decoded_token=None)})], prompt_logprobs=None), CompletionSequenceGroupOutput(samples=[SequenceOutput(parent_seq_id=0, output_token=-1, logprobs={-1: Logprob(logprob=0.0, rank=-1, decoded_token=None)})], prompt_logprobs=None), CompletionSequenceGroupOutput(samples=[SequenceOutput(parent_seq_id=0, output_token=-1, logprobs={-1: Logprob(logprob=0.0, rank=-1, decoded_token=None)})], prompt_logprobs=None)], [CompletionSequenceGroupOutput(samples=[SequenceOutput(parent_seq_id=1, output_token=29889, logprobs={29889: Logprob(logprob=0.0, rank=-1, decoded_token=None)})], prompt_logprobs=None), CompletionSequenceGroupOutput(samples=[SequenceOutput(parent_seq_id=1, output_token=13, logprobs={13: Logprob(logprob=0.0, rank=-1, decoded_token=None)})], prompt_logprobs=None), CompletionSequenceGroupOutput(samples=[SequenceOutput(parent_seq_id=1, output_token=29900, logprobs={29900: Logprob(logprob=0.0, rank=-1, decoded_token=None)})], prompt_logprobs=None)]]
output has multiples True 3
Updating computed tokens manually
META DO_SAMPLE False
OUT [CompletionSequenceGroupOutput(samples=[SequenceOutput(parent_seq_id=3, output_token=13, logprobs={13: Logprob(logprob=0.0, rank=-1, decoded_token=None)})], prompt_logprobs=None), CompletionSequenceGroupOutput(samples=[SequenceOutput(parent_seq_id=3, output_token=-1, logprobs={-1: Logprob(logprob=0.0, rank=-1, decoded_token=None)})], prompt_logprobs=None), CompletionSequenceGroupOutput(samples=[SequenceOutput(parent_seq_id=3, output_token=-1, logprobs={-1: Logprob(logprob=0.0, rank=-1, decoded_token=None)})], prompt_logprobs=None)] 



SEQ GROUP DIO CANE SHOULD SAMPLE HERE False
SequenceGroup(request_id=3, sampling_params=SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=True, max_tokens=32, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), guided_decoding=None, num_seqs=1)
output has multiples True 3
Updating computed tokens manually
META DO_SAMPLE True
OUT [CompletionSequenceGroupOutput(samples=[SequenceOutput(parent_seq_id=2, output_token=13, logprobs={13: Logprob(logprob=0.0, rank=-1, decoded_token=None)})], prompt_logprobs=None), CompletionSequenceGroupOutput(samples=[SequenceOutput(parent_seq_id=2, output_token=-1, logprobs={-1: Logprob(logprob=0.0, rank=-1, decoded_token=None)})], prompt_logprobs=None), CompletionSequenceGroupOutput(samples=[SequenceOutput(parent_seq_id=2, output_token=-1, logprobs={-1: Logprob(logprob=0.0, rank=-1, decoded_token=None)})], prompt_logprobs=None)] 



SEQ GROUP DIO CANE SHOULD SAMPLE HERE True
SequenceGroup(request_id=2, sampling_params=SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=True, max_tokens=32, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), guided_decoding=None, num_seqs=1)
output has multiples True 3
SEQ GROUP DIO CANE SHOULD SAMPLE HERE True
SequenceGroup(request_id=0, sampling_params=SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=True, max_tokens=32, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), guided_decoding=None, num_seqs=1)
output has multiples True 3
SEQ GROUP DIO CANE SHOULD SAMPLE HERE True
SequenceGroup(request_id=1, sampling_params=SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=True, max_tokens=32, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), guided_decoding=None, num_seqs=1)
PROMPTS (is prompt, K, token_chunk, do_sample):  [(True, 0, 1, False), (False, None, 1, True), (False, None, 1, True), (False, None, 1, True)]
SPECULATIING WITH REQ METADATA [SequenceGroupMetadata(request_id='3', is_prompt=True, seq_data={3: SequenceData(prompt_token_ids=array('l', [1, 450, 5434, 310, 319, 29902, 338, 29892, 297, 596, 1914, 3838, 29901, 29871]), output_token_ids=(), cumulative_logprob=0.0, get_num_computed_tokens=1}, sampling_params=SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=True, max_tokens=32, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), guided_decoding=None, block_tables={3: [6]}, do_sample=False, pooling_params=None, lora_request=None, computed_block_nums=[], state=SequenceGroupState(num_steps=1, current_step=0), multi_modal_data={}, encoder_seq_data=None, cross_block_table=None, prompt_adapter_request=None, token_chunk_size=1, num_speculative_tokens=0), SequenceGroupMetadata(request_id='0', is_prompt=False, seq_data={0: SequenceData(prompt_token_ids=array('l', [1, 450, 6673, 310, 278, 3303, 3900, 338, 263, 767, 29892, 825, 29879, 670, 1024, 29973]), output_token_ids=(13, 1576, 261, 29901, 450, 759, 391, 414, 29889, 450, 16912, 29889, 29884, 29889, 29884, 29889, 29884, 338, 263, 29884, 29889, 16912, 30010), cumulative_logprob=0.0, get_num_computed_tokens=39}, sampling_params=SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=True, max_tokens=32, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), guided_decoding=None, block_tables={0: [0, 1, 5]}, do_sample=True, pooling_params=None, lora_request=None, computed_block_nums=[], state=SequenceGroupState(num_steps=1, current_step=0), multi_modal_data={}, encoder_seq_data=None, cross_block_table=None, prompt_adapter_request=None, token_chunk_size=1, num_speculative_tokens=None), SequenceGroupMetadata(request_id='1', is_prompt=False, seq_data={1: SequenceData(prompt_token_ids=array('l', [1, 450, 5434, 310, 319, 29902, 338, 29892, 297, 596, 1914, 3838, 29901, 29871]), output_token_ids=(29896, 29900, 29901, 450, 29877, 29889, 29884, 29889, 29879, 29889, 13, 29900), cumulative_logprob=0.0, get_num_computed_tokens=26}, sampling_params=SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=True, max_tokens=32, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), guided_decoding=None, block_tables={1: [2, 4]}, do_sample=True, pooling_params=None, lora_request=None, computed_block_nums=[], state=SequenceGroupState(num_steps=1, current_step=0), multi_modal_data={}, encoder_seq_data=None, cross_block_table=None, prompt_adapter_request=None, token_chunk_size=1, num_speculative_tokens=None), SequenceGroupMetadata(request_id='2', is_prompt=False, seq_data={2: SequenceData(prompt_token_ids=array('l', [1, 450, 6673, 310, 278, 3303, 3900, 338, 263, 767, 29892, 825, 29879, 670, 1024, 29973]), output_token_ids=(13,), cumulative_logprob=0.0, get_num_computed_tokens=17}, sampling_params=SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=True, max_tokens=32, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), guided_decoding=None, block_tables={2: [3, 7]}, do_sample=True, pooling_params=None, lora_request=None, computed_block_nums=[], state=SequenceGroupState(num_steps=1, current_step=0), multi_modal_data={}, encoder_seq_data=None, cross_block_table=None, prompt_adapter_request=None, token_chunk_size=1, num_speculative_tokens=None)]
SCORING ON THE UNION OF REQUESTS (includes expanded batch), is prompt? [False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, True]
SCORING ON THE UNION OF REQUESTS (includes expanded batch), do_sample? [True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, False]
MIXEEED BATCH BOYS!
Proposal len X do_sample X is_prompt [(0, (False, True)), (5, (True, False)), (5, (True, False)), (5, (True, False))]
Target probs distr tensor(0., device='cuda:0') tensor(3.1250e-05, device='cuda:0') tensor(1., device='cuda:0') torch.Size([3, 6, 32000])
Draft probs distr tensor(0., device='cuda:0') tensor(3.1250e-05, device='cuda:0') tensor(1., device='cuda:0') torch.Size([3, 5, 32000])
VERIFY STEP ENDED tensor([[   -1,    -1,    -1,    -1,    -1,    -1],
        [29879,   263,    -1,    -1,    -1,    -1],
        [29900,    -1,    -1,    -1,    -1,    -1],
        [ 1576,  7178,    -1,    -1,    -1,    -1]], device='cuda:0')
SAMPLER OUTPUT LIST 2x4
LLM STep outputs [SamplerOutput(outputs=[CompletionSequenceGroupOutput(samples=[SequenceOutput(parent_seq_id=3, output_token=-1, logprobs={-1: Logprob(logprob=0.0, rank=-1, decoded_token=None)})], prompt_logprobs=None), CompletionSequenceGroupOutput(samples=[SequenceOutput(parent_seq_id=0, output_token=29879, logprobs={29879: Logprob(logprob=0.0, rank=-1, decoded_token=None)})], prompt_logprobs=None), CompletionSequenceGroupOutput(samples=[SequenceOutput(parent_seq_id=1, output_token=29900, logprobs={29900: Logprob(logprob=0.0, rank=-1, decoded_token=None)})], prompt_logprobs=None), CompletionSequenceGroupOutput(samples=[SequenceOutput(parent_seq_id=2, output_token=1576, logprobs={1576: Logprob(logprob=0.0, rank=-1, decoded_token=None)})], prompt_logprobs=None)], sampled_token_probs=None, sampled_token_ids=None, spec_decode_worker_metrics=None), SamplerOutput(outputs=[CompletionSequenceGroupOutput(samples=[SequenceOutput(parent_seq_id=3, output_token=-1, logprobs={-1: Logprob(logprob=0.0, rank=-1, decoded_token=None)})], prompt_logprobs=None), CompletionSequenceGroupOutput(samples=[SequenceOutput(parent_seq_id=0, output_token=263, logprobs={263: Logprob(logprob=0.0, rank=-1, decoded_token=None)})], prompt_logprobs=None), CompletionSequenceGroupOutput(samples=[SequenceOutput(parent_seq_id=1, output_token=-1, logprobs={-1: Logprob(logprob=0.0, rank=-1, decoded_token=None)})], prompt_logprobs=None), CompletionSequenceGroupOutput(samples=[SequenceOutput(parent_seq_id=2, output_token=7178, logprobs={7178: Logprob(logprob=0.0, rank=-1, decoded_token=None)})], prompt_logprobs=None)], sampled_token_probs=None, sampled_token_ids=None, spec_decode_worker_metrics=None)]
LLM process output indices range(0, 4)
OUTPUTS PER SEQ GROUP (ONE IS PROMPT THE OTHER NOT) [[CompletionSequenceGroupOutput(samples=[SequenceOutput(parent_seq_id=3, output_token=-1, logprobs={-1: Logprob(logprob=0.0, rank=-1, decoded_token=None)})], prompt_logprobs=None), CompletionSequenceGroupOutput(samples=[SequenceOutput(parent_seq_id=3, output_token=-1, logprobs={-1: Logprob(logprob=0.0, rank=-1, decoded_token=None)})], prompt_logprobs=None)], [CompletionSequenceGroupOutput(samples=[SequenceOutput(parent_seq_id=0, output_token=29879, logprobs={29879: Logprob(logprob=0.0, rank=-1, decoded_token=None)})], prompt_logprobs=None), CompletionSequenceGroupOutput(samples=[SequenceOutput(parent_seq_id=0, output_token=263, logprobs={263: Logprob(logprob=0.0, rank=-1, decoded_token=None)})], prompt_logprobs=None)], [CompletionSequenceGroupOutput(samples=[SequenceOutput(parent_seq_id=1, output_token=29900, logprobs={29900: Logprob(logprob=0.0, rank=-1, decoded_token=None)})], prompt_logprobs=None), CompletionSequenceGroupOutput(samples=[SequenceOutput(parent_seq_id=1, output_token=-1, logprobs={-1: Logprob(logprob=0.0, rank=-1, decoded_token=None)})], prompt_logprobs=None)], [CompletionSequenceGroupOutput(samples=[SequenceOutput(parent_seq_id=2, output_token=1576, logprobs={1576: Logprob(logprob=0.0, rank=-1, decoded_token=None)})], prompt_logprobs=None), CompletionSequenceGroupOutput(samples=[SequenceOutput(parent_seq_id=2, output_token=7178, logprobs={7178: Logprob(logprob=0.0, rank=-1, decoded_token=None)})], prompt_logprobs=None)]]
output has multiples True 2
Updating computed tokens manually
META DO_SAMPLE False
OUT [CompletionSequenceGroupOutput(samples=[SequenceOutput(parent_seq_id=3, output_token=-1, logprobs={-1: Logprob(logprob=0.0, rank=-1, decoded_token=None)})], prompt_logprobs=None), CompletionSequenceGroupOutput(samples=[SequenceOutput(parent_seq_id=3, output_token=-1, logprobs={-1: Logprob(logprob=0.0, rank=-1, decoded_token=None)})], prompt_logprobs=None)] 



SEQ GROUP DIO CANE SHOULD SAMPLE HERE False
SequenceGroup(request_id=3, sampling_params=SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=True, max_tokens=32, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), guided_decoding=None, num_seqs=1)
output has multiples True 2
SEQ GROUP DIO CANE SHOULD SAMPLE HERE True
SequenceGroup(request_id=0, sampling_params=SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=True, max_tokens=32, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), guided_decoding=None, num_seqs=1)
output has multiples True 2
SEQ GROUP DIO CANE SHOULD SAMPLE HERE True
SequenceGroup(request_id=1, sampling_params=SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=True, max_tokens=32, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), guided_decoding=None, num_seqs=1)
output has multiples True 2
SEQ GROUP DIO CANE SHOULD SAMPLE HERE True
SequenceGroup(request_id=2, sampling_params=SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=True, max_tokens=32, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), guided_decoding=None, num_seqs=1)
PROMPTS (is prompt, K, token_chunk, do_sample):  [(True, 0, 1, False), (False, None, 1, True), (False, None, 1, True), (False, None, 1, True)]
SPECULATIING WITH REQ METADATA [SequenceGroupMetadata(request_id='3', is_prompt=True, seq_data={3: SequenceData(prompt_token_ids=array('l', [1, 450, 5434, 310, 319, 29902, 338, 29892, 297, 596, 1914, 3838, 29901, 29871]), output_token_ids=(), cumulative_logprob=0.0, get_num_computed_tokens=2}, sampling_params=SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=True, max_tokens=32, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), guided_decoding=None, block_tables={3: [6]}, do_sample=False, pooling_params=None, lora_request=None, computed_block_nums=[], state=SequenceGroupState(num_steps=1, current_step=0), multi_modal_data={}, encoder_seq_data=None, cross_block_table=None, prompt_adapter_request=None, token_chunk_size=1, num_speculative_tokens=0), SequenceGroupMetadata(request_id='0', is_prompt=False, seq_data={0: SequenceData(prompt_token_ids=array('l', [1, 450, 6673, 310, 278, 3303, 3900, 338, 263, 767, 29892, 825, 29879, 670, 1024, 29973]), output_token_ids=(13, 1576, 261, 29901, 450, 759, 391, 414, 29889, 450, 16912, 29889, 29884, 29889, 29884, 29889, 29884, 338, 263, 29884, 29889, 16912, 30010, 29879, 263), cumulative_logprob=0.0, get_num_computed_tokens=41}, sampling_params=SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=True, max_tokens=32, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), guided_decoding=None, block_tables={0: [0, 1, 5]}, do_sample=True, pooling_params=None, lora_request=None, computed_block_nums=[], state=SequenceGroupState(num_steps=1, current_step=0), multi_modal_data={}, encoder_seq_data=None, cross_block_table=None, prompt_adapter_request=None, token_chunk_size=1, num_speculative_tokens=None), SequenceGroupMetadata(request_id='1', is_prompt=False, seq_data={1: SequenceData(prompt_token_ids=array('l', [1, 450, 5434, 310, 319, 29902, 338, 29892, 297, 596, 1914, 3838, 29901, 29871]), output_token_ids=(29896, 29900, 29901, 450, 29877, 29889, 29884, 29889, 29879, 29889, 13, 29900, 29900), cumulative_logprob=0.0, get_num_computed_tokens=27}, sampling_params=SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=True, max_tokens=32, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), guided_decoding=None, block_tables={1: [2, 4]}, do_sample=True, pooling_params=None, lora_request=None, computed_block_nums=[], state=SequenceGroupState(num_steps=1, current_step=0), multi_modal_data={}, encoder_seq_data=None, cross_block_table=None, prompt_adapter_request=None, token_chunk_size=1, num_speculative_tokens=None), SequenceGroupMetadata(request_id='2', is_prompt=False, seq_data={2: SequenceData(prompt_token_ids=array('l', [1, 450, 6673, 310, 278, 3303, 3900, 338, 263, 767, 29892, 825, 29879, 670, 1024, 29973]), output_token_ids=(13, 1576, 7178), cumulative_logprob=0.0, get_num_computed_tokens=19}, sampling_params=SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=True, max_tokens=32, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), guided_decoding=None, block_tables={2: [3, 7]}, do_sample=True, pooling_params=None, lora_request=None, computed_block_nums=[], state=SequenceGroupState(num_steps=1, current_step=0), multi_modal_data={}, encoder_seq_data=None, cross_block_table=None, prompt_adapter_request=None, token_chunk_size=1, num_speculative_tokens=None)]
SCORING ON THE UNION OF REQUESTS (includes expanded batch), is prompt? [False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, True]
SCORING ON THE UNION OF REQUESTS (includes expanded batch), do_sample? [True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, False]
MIXEEED BATCH BOYS!
Proposal len X do_sample X is_prompt [(0, (False, True)), (5, (True, False)), (5, (True, False)), (5, (True, False))]
Target probs distr tensor(0., device='cuda:0') tensor(3.1250e-05, device='cuda:0') tensor(1., device='cuda:0') torch.Size([3, 6, 32000])
Draft probs distr tensor(0., device='cuda:0') tensor(3.1250e-05, device='cuda:0') tensor(1., device='cuda:0') torch.Size([3, 5, 32000])
VERIFY STEP ENDED tensor([[   -1,    -1,    -1,    -1,    -1,    -1],
        [29884, 29889, 29884, 29889, 29884, 29889],
        [29901, 29871,    -1,    -1,    -1,    -1],
        [  310,   278,  3303,  3900, 29892,    -1]], device='cuda:0')
SAMPLER OUTPUT LIST 6x4
LLM STep outputs [SamplerOutput(outputs=[CompletionSequenceGroupOutput(samples=[SequenceOutput(parent_seq_id=3, output_token=-1, logprobs={-1: Logprob(logprob=0.0, rank=-1, decoded_token=None)})], prompt_logprobs=None), CompletionSequenceGroupOutput(samples=[SequenceOutput(parent_seq_id=0, output_token=29884, logprobs={29884: Logprob(logprob=0.0, rank=-1, decoded_token=None)})], prompt_logprobs=None), CompletionSequenceGroupOutput(samples=[SequenceOutput(parent_seq_id=1, output_token=29901, logprobs={29901: Logprob(logprob=0.0, rank=-1, decoded_token=None)})], prompt_logprobs=None), CompletionSequenceGroupOutput(samples=[SequenceOutput(parent_seq_id=2, output_token=310, logprobs={310: Logprob(logprob=0.0, rank=-1, decoded_token=None)})], prompt_logprobs=None)], sampled_token_probs=None, sampled_token_ids=None, spec_decode_worker_metrics=None), SamplerOutput(outputs=[CompletionSequenceGroupOutput(samples=[SequenceOutput(parent_seq_id=3, output_token=-1, logprobs={-1: Logprob(logprob=0.0, rank=-1, decoded_token=None)})], prompt_logprobs=None), CompletionSequenceGroupOutput(samples=[SequenceOutput(parent_seq_id=0, output_token=29889, logprobs={29889: Logprob(logprob=0.0, rank=-1, decoded_token=None)})], prompt_logprobs=None), CompletionSequenceGroupOutput(samples=[SequenceOutput(parent_seq_id=1, output_token=29871, logprobs={29871: Logprob(logprob=0.0, rank=-1, decoded_token=None)})], prompt_logprobs=None), CompletionSequenceGroupOutput(samples=[SequenceOutput(parent_seq_id=2, output_token=278, logprobs={278: Logprob(logprob=0.0, rank=-1, decoded_token=None)})], prompt_logprobs=None)], sampled_token_probs=None, sampled_token_ids=None, spec_decode_worker_metrics=None), SamplerOutput(outputs=[CompletionSequenceGroupOutput(samples=[SequenceOutput(parent_seq_id=3, output_token=-1, logprobs={-1: Logprob(logprob=0.0, rank=-1, decoded_token=None)})], prompt_logprobs=None), CompletionSequenceGroupOutput(samples=[SequenceOutput(parent_seq_id=0, output_token=29884, logprobs={29884: Logprob(logprob=0.0, rank=-1, decoded_token=None)})], prompt_logprobs=None), CompletionSequenceGroupOutput(samples=[SequenceOutput(parent_seq_id=1, output_token=-1, logprobs={-1: Logprob(logprob=0.0, rank=-1, decoded_token=None)})], prompt_logprobs=None), CompletionSequenceGroupOutput(samples=[SequenceOutput(parent_seq_id=2, output_token=3303, logprobs={3303: Logprob(logprob=0.0, rank=-1, decoded_token=None)})], prompt_logprobs=None)], sampled_token_probs=None, sampled_token_ids=None, spec_decode_worker_metrics=None), SamplerOutput(outputs=[CompletionSequenceGroupOutput(samples=[SequenceOutput(parent_seq_id=3, output_token=-1, logprobs={-1: Logprob(logprob=0.0, rank=-1, decoded_token=None)})], prompt_logprobs=None), CompletionSequenceGroupOutput(samples=[SequenceOutput(parent_seq_id=0, output_token=29889, logprobs={29889: Logprob(logprob=0.0, rank=-1, decoded_token=None)})], prompt_logprobs=None), CompletionSequenceGroupOutput(samples=[SequenceOutput(parent_seq_id=1, output_token=-1, logprobs={-1: Logprob(logprob=0.0, rank=-1, decoded_token=None)})], prompt_logprobs=None), CompletionSequenceGroupOutput(samples=[SequenceOutput(parent_seq_id=2, output_token=3900, logprobs={3900: Logprob(logprob=0.0, rank=-1, decoded_token=None)})], prompt_logprobs=None)], sampled_token_probs=None, sampled_token_ids=None, spec_decode_worker_metrics=None), SamplerOutput(outputs=[CompletionSequenceGroupOutput(samples=[SequenceOutput(parent_seq_id=3, output_token=-1, logprobs={-1: Logprob(logprob=0.0, rank=-1, decoded_token=None)})], prompt_logprobs=None), CompletionSequenceGroupOutput(samples=[SequenceOutput(parent_seq_id=0, output_token=29884, logprobs={29884: Logprob(logprob=0.0, rank=-1, decoded_token=None)})], prompt_logprobs=None), CompletionSequenceGroupOutput(samples=[SequenceOutput(parent_seq_id=1, output_token=-1, logprobs={-1: Logprob(logprob=0.0, rank=-1, decoded_token=None)})], prompt_logprobs=None), CompletionSequenceGroupOutput(samples=[SequenceOutput(parent_seq_id=2, output_token=29892, logprobs={29892: Logprob(logprob=0.0, rank=-1, decoded_token=None)})], prompt_logprobs=None)], sampled_token_probs=None, sampled_token_ids=None, spec_decode_worker_metrics=None), SamplerOutput(outputs=[CompletionSequenceGroupOutput(samples=[SequenceOutput(parent_seq_id=3, output_token=-1, logprobs={-1: Logprob(logprob=0.0, rank=-1, decoded_token=None)})], prompt_logprobs=None), CompletionSequenceGroupOutput(samples=[SequenceOutput(parent_seq_id=0, output_token=29889, logprobs={29889: Logprob(logprob=0.0, rank=-1, decoded_token=None)})], prompt_logprobs=None), CompletionSequenceGroupOutput(samples=[SequenceOutput(parent_seq_id=1, output_token=-1, logprobs={-1: Logprob(logprob=0.0, rank=-1, decoded_token=None)})], prompt_logprobs=None), CompletionSequenceGroupOutput(samples=[SequenceOutput(parent_seq_id=2, output_token=-1, logprobs={-1: Logprob(logprob=0.0, rank=-1, decoded_token=None)})], prompt_logprobs=None)], sampled_token_probs=None, sampled_token_ids=None, spec_decode_worker_metrics=None)]
LLM process output indices range(0, 4)
OUTPUTS PER SEQ GROUP (ONE IS PROMPT THE OTHER NOT) [[CompletionSequenceGroupOutput(samples=[SequenceOutput(parent_seq_id=3, output_token=-1, logprobs={-1: Logprob(logprob=0.0, rank=-1, decoded_token=None)})], prompt_logprobs=None), CompletionSequenceGroupOutput(samples=[SequenceOutput(parent_seq_id=3, output_token=-1, logprobs={-1: Logprob(logprob=0.0, rank=-1, decoded_token=None)})], prompt_logprobs=None), CompletionSequenceGroupOutput(samples=[SequenceOutput(parent_seq_id=3, output_token=-1, logprobs={-1: Logprob(logprob=0.0, rank=-1, decoded_token=None)})], prompt_logprobs=None), CompletionSequenceGroupOutput(samples=[SequenceOutput(parent_seq_id=3, output_token=-1, logprobs={-1: Logprob(logprob=0.0, rank=-1, decoded_token=None)})], prompt_logprobs=None), CompletionSequenceGroupOutput(samples=[SequenceOutput(parent_seq_id=3, output_token=-1, logprobs={-1: Logprob(logprob=0.0, rank=-1, decoded_token=None)})], prompt_logprobs=None), CompletionSequenceGroupOutput(samples=[SequenceOutput(parent_seq_id=3, output_token=-1, logprobs={-1: Logprob(logprob=0.0, rank=-1, decoded_token=None)})], prompt_logprobs=None)], [CompletionSequenceGroupOutput(samples=[SequenceOutput(parent_seq_id=0, output_token=29884, logprobs={29884: Logprob(logprob=0.0, rank=-1, decoded_token=None)})], prompt_logprobs=None), CompletionSequenceGroupOutput(samples=[SequenceOutput(parent_seq_id=0, output_token=29889, logprobs={29889: Logprob(logprob=0.0, rank=-1, decoded_token=None)})], prompt_logprobs=None), CompletionSequenceGroupOutput(samples=[SequenceOutput(parent_seq_id=0, output_token=29884, logprobs={29884: Logprob(logprob=0.0, rank=-1, decoded_token=None)})], prompt_logprobs=None), CompletionSequenceGroupOutput(samples=[SequenceOutput(parent_seq_id=0, output_token=29889, logprobs={29889: Logprob(logprob=0.0, rank=-1, decoded_token=None)})], prompt_logprobs=None), CompletionSequenceGroupOutput(samples=[SequenceOutput(parent_seq_id=0, output_token=29884, logprobs={29884: Logprob(logprob=0.0, rank=-1, decoded_token=None)})], prompt_logprobs=None), CompletionSequenceGroupOutput(samples=[SequenceOutput(parent_seq_id=0, output_token=29889, logprobs={29889: Logprob(logprob=0.0, rank=-1, decoded_token=None)})], prompt_logprobs=None)], [CompletionSequenceGroupOutput(samples=[SequenceOutput(parent_seq_id=1, output_token=29901, logprobs={29901: Logprob(logprob=0.0, rank=-1, decoded_token=None)})], prompt_logprobs=None), CompletionSequenceGroupOutput(samples=[SequenceOutput(parent_seq_id=1, output_token=29871, logprobs={29871: Logprob(logprob=0.0, rank=-1, decoded_token=None)})], prompt_logprobs=None), CompletionSequenceGroupOutput(samples=[SequenceOutput(parent_seq_id=1, output_token=-1, logprobs={-1: Logprob(logprob=0.0, rank=-1, decoded_token=None)})], prompt_logprobs=None), CompletionSequenceGroupOutput(samples=[SequenceOutput(parent_seq_id=1, output_token=-1, logprobs={-1: Logprob(logprob=0.0, rank=-1, decoded_token=None)})], prompt_logprobs=None), CompletionSequenceGroupOutput(samples=[SequenceOutput(parent_seq_id=1, output_token=-1, logprobs={-1: Logprob(logprob=0.0, rank=-1, decoded_token=None)})], prompt_logprobs=None), CompletionSequenceGroupOutput(samples=[SequenceOutput(parent_seq_id=1, output_token=-1, logprobs={-1: Logprob(logprob=0.0, rank=-1, decoded_token=None)})], prompt_logprobs=None)], [CompletionSequenceGroupOutput(samples=[SequenceOutput(parent_seq_id=2, output_token=310, logprobs={310: Logprob(logprob=0.0, rank=-1, decoded_token=None)})], prompt_logprobs=None), CompletionSequenceGroupOutput(samples=[SequenceOutput(parent_seq_id=2, output_token=278, logprobs={278: Logprob(logprob=0.0, rank=-1, decoded_token=None)})], prompt_logprobs=None), CompletionSequenceGroupOutput(samples=[SequenceOutput(parent_seq_id=2, output_token=3303, logprobs={3303: Logprob(logprob=0.0, rank=-1, decoded_token=None)})], prompt_logprobs=None), CompletionSequenceGroupOutput(samples=[SequenceOutput(parent_seq_id=2, output_token=3900, logprobs={3900: Logprob(logprob=0.0, rank=-1, decoded_token=None)})], prompt_logprobs=None), CompletionSequenceGroupOutput(samples=[SequenceOutput(parent_seq_id=2, output_token=29892, logprobs={29892: Logprob(logprob=0.0, rank=-1, decoded_token=None)})], prompt_logprobs=None), CompletionSequenceGroupOutput(samples=[SequenceOutput(parent_seq_id=2, output_token=-1, logprobs={-1: Logprob(logprob=0.0, rank=-1, decoded_token=None)})], prompt_logprobs=None)]]
output has multiples True 6
Updating computed tokens manually
META DO_SAMPLE False
OUT [CompletionSequenceGroupOutput(samples=[SequenceOutput(parent_seq_id=3, output_token=-1, logprobs={-1: Logprob(logprob=0.0, rank=-1, decoded_token=None)})], prompt_logprobs=None), CompletionSequenceGroupOutput(samples=[SequenceOutput(parent_seq_id=3, output_token=-1, logprobs={-1: Logprob(logprob=0.0, rank=-1, decoded_token=None)})], prompt_logprobs=None), CompletionSequenceGroupOutput(samples=[SequenceOutput(parent_seq_id=3, output_token=-1, logprobs={-1: Logprob(logprob=0.0, rank=-1, decoded_token=None)})], prompt_logprobs=None), CompletionSequenceGroupOutput(samples=[SequenceOutput(parent_seq_id=3, output_token=-1, logprobs={-1: Logprob(logprob=0.0, rank=-1, decoded_token=None)})], prompt_logprobs=None), CompletionSequenceGroupOutput(samples=[SequenceOutput(parent_seq_id=3, output_token=-1, logprobs={-1: Logprob(logprob=0.0, rank=-1, decoded_token=None)})], prompt_logprobs=None), CompletionSequenceGroupOutput(samples=[SequenceOutput(parent_seq_id=3, output_token=-1, logprobs={-1: Logprob(logprob=0.0, rank=-1, decoded_token=None)})], prompt_logprobs=None)] 



SEQ GROUP DIO CANE SHOULD SAMPLE HERE False
SequenceGroup(request_id=3, sampling_params=SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=True, max_tokens=32, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), guided_decoding=None, num_seqs=1)
output has multiples True 6
SEQ GROUP DIO CANE SHOULD SAMPLE HERE True
SequenceGroup(request_id=0, sampling_params=SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=True, max_tokens=32, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), guided_decoding=None, num_seqs=1)
output has multiples True 6
SEQ GROUP DIO CANE SHOULD SAMPLE HERE True
SequenceGroup(request_id=1, sampling_params=SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=True, max_tokens=32, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), guided_decoding=None, num_seqs=1)
output has multiples True 6
SEQ GROUP DIO CANE SHOULD SAMPLE HERE True
SequenceGroup(request_id=2, sampling_params=SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=True, max_tokens=32, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), guided_decoding=None, num_seqs=1)
PROMPTS (is prompt, K, token_chunk, do_sample):  [(True, 0, 1, False), (False, None, 1, True), (False, None, 1, True), (False, None, 1, True)]
SPECULATIING WITH REQ METADATA [SequenceGroupMetadata(request_id='3', is_prompt=True, seq_data={3: SequenceData(prompt_token_ids=array('l', [1, 450, 5434, 310, 319, 29902, 338, 29892, 297, 596, 1914, 3838, 29901, 29871]), output_token_ids=(), cumulative_logprob=0.0, get_num_computed_tokens=3}, sampling_params=SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=True, max_tokens=32, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), guided_decoding=None, block_tables={3: [6]}, do_sample=False, pooling_params=None, lora_request=None, computed_block_nums=[], state=SequenceGroupState(num_steps=1, current_step=0), multi_modal_data={}, encoder_seq_data=None, cross_block_table=None, prompt_adapter_request=None, token_chunk_size=1, num_speculative_tokens=0), SequenceGroupMetadata(request_id='0', is_prompt=False, seq_data={0: SequenceData(prompt_token_ids=array('l', [1, 450, 6673, 310, 278, 3303, 3900, 338, 263, 767, 29892, 825, 29879, 670, 1024, 29973]), output_token_ids=(13, 1576, 261, 29901, 450, 759, 391, 414, 29889, 450, 16912, 29889, 29884, 29889, 29884, 29889, 29884, 338, 263, 29884, 29889, 16912, 30010, 29879, 263, 29884, 29889, 29884, 29889, 29884, 29889), cumulative_logprob=0.0, get_num_computed_tokens=47}, sampling_params=SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=True, max_tokens=32, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), guided_decoding=None, block_tables={0: [0, 1, 5, 8]}, do_sample=True, pooling_params=None, lora_request=None, computed_block_nums=[], state=SequenceGroupState(num_steps=1, current_step=0), multi_modal_data={}, encoder_seq_data=None, cross_block_table=None, prompt_adapter_request=None, token_chunk_size=1, num_speculative_tokens=None), SequenceGroupMetadata(request_id='1', is_prompt=False, seq_data={1: SequenceData(prompt_token_ids=array('l', [1, 450, 5434, 310, 319, 29902, 338, 29892, 297, 596, 1914, 3838, 29901, 29871]), output_token_ids=(29896, 29900, 29901, 450, 29877, 29889, 29884, 29889, 29879, 29889, 13, 29900, 29900, 29901, 29871), cumulative_logprob=0.0, get_num_computed_tokens=29}, sampling_params=SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=True, max_tokens=32, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), guided_decoding=None, block_tables={1: [2, 4, 9]}, do_sample=True, pooling_params=None, lora_request=None, computed_block_nums=[], state=SequenceGroupState(num_steps=1, current_step=0), multi_modal_data={}, encoder_seq_data=None, cross_block_table=None, prompt_adapter_request=None, token_chunk_size=1, num_speculative_tokens=None), SequenceGroupMetadata(request_id='2', is_prompt=False, seq_data={2: SequenceData(prompt_token_ids=array('l', [1, 450, 6673, 310, 278, 3303, 3900, 338, 263, 767, 29892, 825, 29879, 670, 1024, 29973]), output_token_ids=(13, 1576, 7178, 310, 278, 3303, 3900, 29892), cumulative_logprob=0.0, get_num_computed_tokens=24}, sampling_params=SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=True, max_tokens=32, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), guided_decoding=None, block_tables={2: [3, 7]}, do_sample=True, pooling_params=None, lora_request=None, computed_block_nums=[], state=SequenceGroupState(num_steps=1, current_step=0), multi_modal_data={}, encoder_seq_data=None, cross_block_table=None, prompt_adapter_request=None, token_chunk_size=1, num_speculative_tokens=None)]
SCORING ON THE UNION OF REQUESTS (includes expanded batch), is prompt? [False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, True]
SCORING ON THE UNION OF REQUESTS (includes expanded batch), do_sample? [True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, False]
MIXEEED BATCH BOYS!
Proposal len X do_sample X is_prompt [(0, (False, True)), (5, (True, False)), (5, (True, False)), (5, (True, False))]
Target probs distr tensor(0., device='cuda:0') tensor(3.1250e-05, device='cuda:0') tensor(1., device='cuda:0') torch.Size([3, 6, 32000])
Draft probs distr tensor(0., device='cuda:0') tensor(3.1250e-05, device='cuda:0') tensor(1., device='cuda:0') torch.Size([3, 5, 32000])
VERIFY STEP ENDED tensor([[   -1,    -1,    -1,    -1,    -1,    -1],
        [29884, 29889, 29884, 29889, 29884, 29889],
        [29900,    -1,    -1,    -1,    -1,    -1],
        [  278,  7178,   310,   278,  3303,  3900]], device='cuda:0')
SAMPLER OUTPUT LIST 6x4
LLM STep outputs [SamplerOutput(outputs=[CompletionSequenceGroupOutput(samples=[SequenceOutput(parent_seq_id=3, output_token=-1, logprobs={-1: Logprob(logprob=0.0, rank=-1, decoded_token=None)})], prompt_logprobs=None), CompletionSequenceGroupOutput(samples=[SequenceOutput(parent_seq_id=0, output_token=29884, logprobs={29884: Logprob(logprob=0.0, rank=-1, decoded_token=None)})], prompt_logprobs=None), CompletionSequenceGroupOutput(samples=[SequenceOutput(parent_seq_id=1, output_token=29900, logprobs={29900: Logprob(logprob=0.0, rank=-1, decoded_token=None)})], prompt_logprobs=None), CompletionSequenceGroupOutput(samples=[SequenceOutput(parent_seq_id=2, output_token=278, logprobs={278: Logprob(logprob=0.0, rank=-1, decoded_token=None)})], prompt_logprobs=None)], sampled_token_probs=None, sampled_token_ids=None, spec_decode_worker_metrics=None), SamplerOutput(outputs=[CompletionSequenceGroupOutput(samples=[SequenceOutput(parent_seq_id=3, output_token=-1, logprobs={-1: Logprob(logprob=0.0, rank=-1, decoded_token=None)})], prompt_logprobs=None), CompletionSequenceGroupOutput(samples=[SequenceOutput(parent_seq_id=0, output_token=29889, logprobs={29889: Logprob(logprob=0.0, rank=-1, decoded_token=None)})], prompt_logprobs=None), CompletionSequenceGroupOutput(samples=[SequenceOutput(parent_seq_id=1, output_token=-1, logprobs={-1: Logprob(logprob=0.0, rank=-1, decoded_token=None)})], prompt_logprobs=None), CompletionSequenceGroupOutput(samples=[SequenceOutput(parent_seq_id=2, output_token=7178, logprobs={7178: Logprob(logprob=0.0, rank=-1, decoded_token=None)})], prompt_logprobs=None)], sampled_token_probs=None, sampled_token_ids=None, spec_decode_worker_metrics=None), SamplerOutput(outputs=[CompletionSequenceGroupOutput(samples=[SequenceOutput(parent_seq_id=3, output_token=-1, logprobs={-1: Logprob(logprob=0.0, rank=-1, decoded_token=None)})], prompt_logprobs=None), CompletionSequenceGroupOutput(samples=[SequenceOutput(parent_seq_id=0, output_token=29884, logprobs={29884: Logprob(logprob=0.0, rank=-1, decoded_token=None)})], prompt_logprobs=None), CompletionSequenceGroupOutput(samples=[SequenceOutput(parent_seq_id=1, output_token=-1, logprobs={-1: Logprob(logprob=0.0, rank=-1, decoded_token=None)})], prompt_logprobs=None), CompletionSequenceGroupOutput(samples=[SequenceOutput(parent_seq_id=2, output_token=310, logprobs={310: Logprob(logprob=0.0, rank=-1, decoded_token=None)})], prompt_logprobs=None)], sampled_token_probs=None, sampled_token_ids=None, spec_decode_worker_metrics=None), SamplerOutput(outputs=[CompletionSequenceGroupOutput(samples=[SequenceOutput(parent_seq_id=3, output_token=-1, logprobs={-1: Logprob(logprob=0.0, rank=-1, decoded_token=None)})], prompt_logprobs=None), CompletionSequenceGroupOutput(samples=[SequenceOutput(parent_seq_id=0, output_token=29889, logprobs={29889: Logprob(logprob=0.0, rank=-1, decoded_token=None)})], prompt_logprobs=None), CompletionSequenceGroupOutput(samples=[SequenceOutput(parent_seq_id=1, output_token=-1, logprobs={-1: Logprob(logprob=0.0, rank=-1, decoded_token=None)})], prompt_logprobs=None), CompletionSequenceGroupOutput(samples=[SequenceOutput(parent_seq_id=2, output_token=278, logprobs={278: Logprob(logprob=0.0, rank=-1, decoded_token=None)})], prompt_logprobs=None)], sampled_token_probs=None, sampled_token_ids=None, spec_decode_worker_metrics=None), SamplerOutput(outputs=[CompletionSequenceGroupOutput(samples=[SequenceOutput(parent_seq_id=3, output_token=-1, logprobs={-1: Logprob(logprob=0.0, rank=-1, decoded_token=None)})], prompt_logprobs=None), CompletionSequenceGroupOutput(samples=[SequenceOutput(parent_seq_id=0, output_token=29884, logprobs={29884: Logprob(logprob=0.0, rank=-1, decoded_token=None)})], prompt_logprobs=None), CompletionSequenceGroupOutput(samples=[SequenceOutput(parent_seq_id=1, output_token=-1, logprobs={-1: Logprob(logprob=0.0, rank=-1, decoded_token=None)})], prompt_logprobs=None), CompletionSequenceGroupOutput(samples=[SequenceOutput(parent_seq_id=2, output_token=3303, logprobs={3303: Logprob(logprob=0.0, rank=-1, decoded_token=None)})], prompt_logprobs=None)], sampled_token_probs=None, sampled_token_ids=None, spec_decode_worker_metrics=None), SamplerOutput(outputs=[CompletionSequenceGroupOutput(samples=[SequenceOutput(parent_seq_id=3, output_token=-1, logprobs={-1: Logprob(logprob=0.0, rank=-1, decoded_token=None)})], prompt_logprobs=None), CompletionSequenceGroupOutput(samples=[SequenceOutput(parent_seq_id=0, output_token=29889, logprobs={29889: Logprob(logprob=0.0, rank=-1, decoded_token=None)})], prompt_logprobs=None), CompletionSequenceGroupOutput(samples=[SequenceOutput(parent_seq_id=1, output_token=-1, logprobs={-1: Logprob(logprob=0.0, rank=-1, decoded_token=None)})], prompt_logprobs=None), CompletionSequenceGroupOutput(samples=[SequenceOutput(parent_seq_id=2, output_token=3900, logprobs={3900: Logprob(logprob=0.0, rank=-1, decoded_token=None)})], prompt_logprobs=None)], sampled_token_probs=None, sampled_token_ids=None, spec_decode_worker_metrics=None)]
LLM process output indices range(0, 4)
OUTPUTS PER SEQ GROUP (ONE IS PROMPT THE OTHER NOT) [[CompletionSequenceGroupOutput(samples=[SequenceOutput(parent_seq_id=3, output_token=-1, logprobs={-1: Logprob(logprob=0.0, rank=-1, decoded_token=None)})], prompt_logprobs=None), CompletionSequenceGroupOutput(samples=[SequenceOutput(parent_seq_id=3, output_token=-1, logprobs={-1: Logprob(logprob=0.0, rank=-1, decoded_token=None)})], prompt_logprobs=None), CompletionSequenceGroupOutput(samples=[SequenceOutput(parent_seq_id=3, output_token=-1, logprobs={-1: Logprob(logprob=0.0, rank=-1, decoded_token=None)})], prompt_logprobs=None), CompletionSequenceGroupOutput(samples=[SequenceOutput(parent_seq_id=3, output_token=-1, logprobs={-1: Logprob(logprob=0.0, rank=-1, decoded_token=None)})], prompt_logprobs=None), CompletionSequenceGroupOutput(samples=[SequenceOutput(parent_seq_id=3, output_token=-1, logprobs={-1: Logprob(logprob=0.0, rank=-1, decoded_token=None)})], prompt_logprobs=None), CompletionSequenceGroupOutput(samples=[SequenceOutput(parent_seq_id=3, output_token=-1, logprobs={-1: Logprob(logprob=0.0, rank=-1, decoded_token=None)})], prompt_logprobs=None)], [CompletionSequenceGroupOutput(samples=[SequenceOutput(parent_seq_id=0, output_token=29884, logprobs={29884: Logprob(logprob=0.0, rank=-1, decoded_token=None)})], prompt_logprobs=None), CompletionSequenceGroupOutput(samples=[SequenceOutput(parent_seq_id=0, output_token=29889, logprobs={29889: Logprob(logprob=0.0, rank=-1, decoded_token=None)})], prompt_logprobs=None), CompletionSequenceGroupOutput(samples=[SequenceOutput(parent_seq_id=0, output_token=29884, logprobs={29884: Logprob(logprob=0.0, rank=-1, decoded_token=None)})], prompt_logprobs=None), CompletionSequenceGroupOutput(samples=[SequenceOutput(parent_seq_id=0, output_token=29889, logprobs={29889: Logprob(logprob=0.0, rank=-1, decoded_token=None)})], prompt_logprobs=None), CompletionSequenceGroupOutput(samples=[SequenceOutput(parent_seq_id=0, output_token=29884, logprobs={29884: Logprob(logprob=0.0, rank=-1, decoded_token=None)})], prompt_logprobs=None), CompletionSequenceGroupOutput(samples=[SequenceOutput(parent_seq_id=0, output_token=29889, logprobs={29889: Logprob(logprob=0.0, rank=-1, decoded_token=None)})], prompt_logprobs=None)], [CompletionSequenceGroupOutput(samples=[SequenceOutput(parent_seq_id=1, output_token=29900, logprobs={29900: Logprob(logprob=0.0, rank=-1, decoded_token=None)})], prompt_logprobs=None), CompletionSequenceGroupOutput(samples=[SequenceOutput(parent_seq_id=1, output_token=-1, logprobs={-1: Logprob(logprob=0.0, rank=-1, decoded_token=None)})], prompt_logprobs=None), CompletionSequenceGroupOutput(samples=[SequenceOutput(parent_seq_id=1, output_token=-1, logprobs={-1: Logprob(logprob=0.0, rank=-1, decoded_token=None)})], prompt_logprobs=None), CompletionSequenceGroupOutput(samples=[SequenceOutput(parent_seq_id=1, output_token=-1, logprobs={-1: Logprob(logprob=0.0, rank=-1, decoded_token=None)})], prompt_logprobs=None), CompletionSequenceGroupOutput(samples=[SequenceOutput(parent_seq_id=1, output_token=-1, logprobs={-1: Logprob(logprob=0.0, rank=-1, decoded_token=None)})], prompt_logprobs=None), CompletionSequenceGroupOutput(samples=[SequenceOutput(parent_seq_id=1, output_token=-1, logprobs={-1: Logprob(logprob=0.0, rank=-1, decoded_token=None)})], prompt_logprobs=None)], [CompletionSequenceGroupOutput(samples=[SequenceOutput(parent_seq_id=2, output_token=278, logprobs={278: Logprob(logprob=0.0, rank=-1, decoded_token=None)})], prompt_logprobs=None), CompletionSequenceGroupOutput(samples=[SequenceOutput(parent_seq_id=2, output_token=7178, logprobs={7178: Logprob(logprob=0.0, rank=-1, decoded_token=None)})], prompt_logprobs=None), CompletionSequenceGroupOutput(samples=[SequenceOutput(parent_seq_id=2, output_token=310, logprobs={310: Logprob(logprob=0.0, rank=-1, decoded_token=None)})], prompt_logprobs=None), CompletionSequenceGroupOutput(samples=[SequenceOutput(parent_seq_id=2, output_token=278, logprobs={278: Logprob(logprob=0.0, rank=-1, decoded_token=None)})], prompt_logprobs=None), CompletionSequenceGroupOutput(samples=[SequenceOutput(parent_seq_id=2, output_token=3303, logprobs={3303: Logprob(logprob=0.0, rank=-1, decoded_token=None)})], prompt_logprobs=None), CompletionSequenceGroupOutput(samples=[SequenceOutput(parent_seq_id=2, output_token=3900, logprobs={3900: Logprob(logprob=0.0, rank=-1, decoded_token=None)})], prompt_logprobs=None)]]
output has multiples True 6
Updating computed tokens manually
META DO_SAMPLE False
OUT [CompletionSequenceGroupOutput(samples=[SequenceOutput(parent_seq_id=3, output_token=-1, logprobs={-1: Logprob(logprob=0.0, rank=-1, decoded_token=None)})], prompt_logprobs=None), CompletionSequenceGroupOutput(samples=[SequenceOutput(parent_seq_id=3, output_token=-1, logprobs={-1: Logprob(logprob=0.0, rank=-1, decoded_token=None)})], prompt_logprobs=None), CompletionSequenceGroupOutput(samples=[SequenceOutput(parent_seq_id=3, output_token=-1, logprobs={-1: Logprob(logprob=0.0, rank=-1, decoded_token=None)})], prompt_logprobs=None), CompletionSequenceGroupOutput(samples=[SequenceOutput(parent_seq_id=3, output_token=-1, logprobs={-1: Logprob(logprob=0.0, rank=-1, decoded_token=None)})], prompt_logprobs=None), CompletionSequenceGroupOutput(samples=[SequenceOutput(parent_seq_id=3, output_token=-1, logprobs={-1: Logprob(logprob=0.0, rank=-1, decoded_token=None)})], prompt_logprobs=None), CompletionSequenceGroupOutput(samples=[SequenceOutput(parent_seq_id=3, output_token=-1, logprobs={-1: Logprob(logprob=0.0, rank=-1, decoded_token=None)})], prompt_logprobs=None)] 



SEQ GROUP DIO CANE SHOULD SAMPLE HERE False
SequenceGroup(request_id=3, sampling_params=SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=True, max_tokens=32, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), guided_decoding=None, num_seqs=1)
output has multiples True 6
SEQ GROUP DIO CANE SHOULD SAMPLE HERE True
SequenceGroup(request_id=0, sampling_params=SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=True, max_tokens=32, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), guided_decoding=None, num_seqs=1)
output has multiples True 6
SEQ GROUP DIO CANE SHOULD SAMPLE HERE True
SequenceGroup(request_id=1, sampling_params=SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=True, max_tokens=32, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), guided_decoding=None, num_seqs=1)
output has multiples True 6
SEQ GROUP DIO CANE SHOULD SAMPLE HERE True
SequenceGroup(request_id=2, sampling_params=SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=True, max_tokens=32, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), guided_decoding=None, num_seqs=1)
PROMPTS (is prompt, K, token_chunk, do_sample):  [(True, 0, 2, False), (False, None, 1, True), (False, None, 1, True)]
SPECULATIING WITH REQ METADATA [SequenceGroupMetadata(request_id='3', is_prompt=True, seq_data={3: SequenceData(prompt_token_ids=array('l', [1, 450, 5434, 310, 319, 29902, 338, 29892, 297, 596, 1914, 3838, 29901, 29871]), output_token_ids=(), cumulative_logprob=0.0, get_num_computed_tokens=4}, sampling_params=SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=True, max_tokens=32, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), guided_decoding=None, block_tables={3: [6]}, do_sample=False, pooling_params=None, lora_request=None, computed_block_nums=[], state=SequenceGroupState(num_steps=1, current_step=0), multi_modal_data={}, encoder_seq_data=None, cross_block_table=None, prompt_adapter_request=None, token_chunk_size=2, num_speculative_tokens=0), SequenceGroupMetadata(request_id='1', is_prompt=False, seq_data={1: SequenceData(prompt_token_ids=array('l', [1, 450, 5434, 310, 319, 29902, 338, 29892, 297, 596, 1914, 3838, 29901, 29871]), output_token_ids=(29896, 29900, 29901, 450, 29877, 29889, 29884, 29889, 29879, 29889, 13, 29900, 29900, 29901, 29871, 29900), cumulative_logprob=0.0, get_num_computed_tokens=30}, sampling_params=SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=True, max_tokens=32, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), guided_decoding=None, block_tables={1: [2, 4, 9]}, do_sample=True, pooling_params=None, lora_request=None, computed_block_nums=[], state=SequenceGroupState(num_steps=1, current_step=0), multi_modal_data={}, encoder_seq_data=None, cross_block_table=None, prompt_adapter_request=None, token_chunk_size=1, num_speculative_tokens=None), SequenceGroupMetadata(request_id='2', is_prompt=False, seq_data={2: SequenceData(prompt_token_ids=array('l', [1, 450, 6673, 310, 278, 3303, 3900, 338, 263, 767, 29892, 825, 29879, 670, 1024, 29973]), output_token_ids=(13, 1576, 7178, 310, 278, 3303, 3900, 29892, 278, 7178, 310, 278, 3303, 3900), cumulative_logprob=0.0, get_num_computed_tokens=30}, sampling_params=SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=True, max_tokens=32, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), guided_decoding=None, block_tables={2: [3, 7, 8]}, do_sample=True, pooling_params=None, lora_request=None, computed_block_nums=[], state=SequenceGroupState(num_steps=1, current_step=0), multi_modal_data={}, encoder_seq_data=None, cross_block_table=None, prompt_adapter_request=None, token_chunk_size=1, num_speculative_tokens=None)]
SCORING ON THE UNION OF REQUESTS (includes expanded batch), is prompt? [False, False, False, False, False, False, False, False, False, False, False, False, True]
SCORING ON THE UNION OF REQUESTS (includes expanded batch), do_sample? [True, True, True, True, True, True, True, True, True, True, True, True, False]
MIXEEED BATCH BOYS!
Proposal len X do_sample X is_prompt [(0, (False, True)), (5, (True, False)), (5, (True, False))]
Target probs distr tensor(0., device='cuda:0') tensor(3.1250e-05, device='cuda:0') tensor(1., device='cuda:0') torch.Size([2, 6, 32000])
Draft probs distr tensor(0., device='cuda:0') tensor(3.1250e-05, device='cuda:0') tensor(1., device='cuda:0') torch.Size([2, 5, 32000])
VERIFY STEP ENDED tensor([[   -1,    -1,    -1,    -1,    -1,    -1],
        [29900,     0,    -1,    -1,    -1,    -1],
        [    0,    -1,    -1,    -1,    -1,    -1]], device='cuda:0')
SAMPLER OUTPUT LIST 2x3
LLM STep outputs [SamplerOutput(outputs=[CompletionSequenceGroupOutput(samples=[SequenceOutput(parent_seq_id=3, output_token=-1, logprobs={-1: Logprob(logprob=0.0, rank=-1, decoded_token=None)})], prompt_logprobs=None), CompletionSequenceGroupOutput(samples=[SequenceOutput(parent_seq_id=1, output_token=29900, logprobs={29900: Logprob(logprob=0.0, rank=-1, decoded_token=None)})], prompt_logprobs=None), CompletionSequenceGroupOutput(samples=[SequenceOutput(parent_seq_id=2, output_token=0, logprobs={0: Logprob(logprob=0.0, rank=-1, decoded_token=None)})], prompt_logprobs=None)], sampled_token_probs=None, sampled_token_ids=None, spec_decode_worker_metrics=None), SamplerOutput(outputs=[CompletionSequenceGroupOutput(samples=[SequenceOutput(parent_seq_id=3, output_token=-1, logprobs={-1: Logprob(logprob=0.0, rank=-1, decoded_token=None)})], prompt_logprobs=None), CompletionSequenceGroupOutput(samples=[SequenceOutput(parent_seq_id=1, output_token=0, logprobs={0: Logprob(logprob=0.0, rank=-1, decoded_token=None)})], prompt_logprobs=None), CompletionSequenceGroupOutput(samples=[SequenceOutput(parent_seq_id=2, output_token=-1, logprobs={-1: Logprob(logprob=0.0, rank=-1, decoded_token=None)})], prompt_logprobs=None)], sampled_token_probs=None, sampled_token_ids=None, spec_decode_worker_metrics=None)]
LLM process output indices range(0, 3)
OUTPUTS PER SEQ GROUP (ONE IS PROMPT THE OTHER NOT) [[CompletionSequenceGroupOutput(samples=[SequenceOutput(parent_seq_id=3, output_token=-1, logprobs={-1: Logprob(logprob=0.0, rank=-1, decoded_token=None)})], prompt_logprobs=None), CompletionSequenceGroupOutput(samples=[SequenceOutput(parent_seq_id=3, output_token=-1, logprobs={-1: Logprob(logprob=0.0, rank=-1, decoded_token=None)})], prompt_logprobs=None)], [CompletionSequenceGroupOutput(samples=[SequenceOutput(parent_seq_id=1, output_token=29900, logprobs={29900: Logprob(logprob=0.0, rank=-1, decoded_token=None)})], prompt_logprobs=None), CompletionSequenceGroupOutput(samples=[SequenceOutput(parent_seq_id=1, output_token=0, logprobs={0: Logprob(logprob=0.0, rank=-1, decoded_token=None)})], prompt_logprobs=None)], [CompletionSequenceGroupOutput(samples=[SequenceOutput(parent_seq_id=2, output_token=0, logprobs={0: Logprob(logprob=0.0, rank=-1, decoded_token=None)})], prompt_logprobs=None), CompletionSequenceGroupOutput(samples=[SequenceOutput(parent_seq_id=2, output_token=-1, logprobs={-1: Logprob(logprob=0.0, rank=-1, decoded_token=None)})], prompt_logprobs=None)]]
output has multiples True 2
Updating computed tokens manually
META DO_SAMPLE False
OUT [CompletionSequenceGroupOutput(samples=[SequenceOutput(parent_seq_id=3, output_token=-1, logprobs={-1: Logprob(logprob=0.0, rank=-1, decoded_token=None)})], prompt_logprobs=None), CompletionSequenceGroupOutput(samples=[SequenceOutput(parent_seq_id=3, output_token=-1, logprobs={-1: Logprob(logprob=0.0, rank=-1, decoded_token=None)})], prompt_logprobs=None)] 



SEQ GROUP DIO CANE SHOULD SAMPLE HERE False
SequenceGroup(request_id=3, sampling_params=SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=True, max_tokens=32, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), guided_decoding=None, num_seqs=1)
output has multiples True 2
SEQ GROUP DIO CANE SHOULD SAMPLE HERE True
SequenceGroup(request_id=1, sampling_params=SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=True, max_tokens=32, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), guided_decoding=None, num_seqs=1)
output has multiples True 2
SEQ GROUP DIO CANE SHOULD SAMPLE HERE True
SequenceGroup(request_id=2, sampling_params=SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=True, max_tokens=32, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), guided_decoding=None, num_seqs=1)
PROMPTS (is prompt, K, token_chunk, do_sample):  [(True, 0, 2, False), (False, None, 1, True), (False, None, 1, True)]
SPECULATIING WITH REQ METADATA [SequenceGroupMetadata(request_id='3', is_prompt=True, seq_data={3: SequenceData(prompt_token_ids=array('l', [1, 450, 5434, 310, 319, 29902, 338, 29892, 297, 596, 1914, 3838, 29901, 29871]), output_token_ids=(), cumulative_logprob=0.0, get_num_computed_tokens=6}, sampling_params=SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=True, max_tokens=32, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), guided_decoding=None, block_tables={3: [6]}, do_sample=False, pooling_params=None, lora_request=None, computed_block_nums=[], state=SequenceGroupState(num_steps=1, current_step=0), multi_modal_data={}, encoder_seq_data=None, cross_block_table=None, prompt_adapter_request=None, token_chunk_size=2, num_speculative_tokens=0), SequenceGroupMetadata(request_id='1', is_prompt=False, seq_data={1: SequenceData(prompt_token_ids=array('l', [1, 450, 5434, 310, 319, 29902, 338, 29892, 297, 596, 1914, 3838, 29901, 29871]), output_token_ids=(29896, 29900, 29901, 450, 29877, 29889, 29884, 29889, 29879, 29889, 13, 29900, 29900, 29901, 29871, 29900, 29900, 0), cumulative_logprob=0.0, get_num_computed_tokens=32}, sampling_params=SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=True, max_tokens=32, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), guided_decoding=None, block_tables={1: [2, 4, 9]}, do_sample=True, pooling_params=None, lora_request=None, computed_block_nums=[], state=SequenceGroupState(num_steps=1, current_step=0), multi_modal_data={}, encoder_seq_data=None, cross_block_table=None, prompt_adapter_request=None, token_chunk_size=1, num_speculative_tokens=None), SequenceGroupMetadata(request_id='2', is_prompt=False, seq_data={2: SequenceData(prompt_token_ids=array('l', [1, 450, 6673, 310, 278, 3303, 3900, 338, 263, 767, 29892, 825, 29879, 670, 1024, 29973]), output_token_ids=(13, 1576, 7178, 310, 278, 3303, 3900, 29892, 278, 7178, 310, 278, 3303, 3900, 0), cumulative_logprob=0.0, get_num_computed_tokens=31}, sampling_params=SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=True, max_tokens=32, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), guided_decoding=None, block_tables={2: [3, 7, 8]}, do_sample=True, pooling_params=None, lora_request=None, computed_block_nums=[], state=SequenceGroupState(num_steps=1, current_step=0), multi_modal_data={}, encoder_seq_data=None, cross_block_table=None, prompt_adapter_request=None, token_chunk_size=1, num_speculative_tokens=None)]
SCORING ON THE UNION OF REQUESTS (includes expanded batch), is prompt? [False, False, False, False, False, False, False, False, False, False, False, False, True]
SCORING ON THE UNION OF REQUESTS (includes expanded batch), do_sample? [True, True, True, True, True, True, True, True, True, True, True, True, False]
MIXEEED BATCH BOYS!
Proposal len X do_sample X is_prompt [(0, (False, True)), (5, (True, False)), (5, (True, False))]
Target probs distr tensor(0., device='cuda:0') tensor(3.1250e-05, device='cuda:0') tensor(1., device='cuda:0') torch.Size([2, 6, 32000])
Draft probs distr tensor(0., device='cuda:0') tensor(3.1250e-05, device='cuda:0') tensor(1., device='cuda:0') torch.Size([2, 5, 32000])
VERIFY STEP ENDED tensor([[-1, -1, -1, -1, -1, -1],
        [ 0, -1, -1, -1, -1, -1],
        [ 0, -1, -1, -1, -1, -1]], device='cuda:0')
SAMPLER OUTPUT LIST 1x3
LLM STep outputs [SamplerOutput(outputs=[CompletionSequenceGroupOutput(samples=[SequenceOutput(parent_seq_id=3, output_token=-1, logprobs={-1: Logprob(logprob=0.0, rank=-1, decoded_token=None)})], prompt_logprobs=None), CompletionSequenceGroupOutput(samples=[SequenceOutput(parent_seq_id=1, output_token=0, logprobs={0: Logprob(logprob=0.0, rank=-1, decoded_token=None)})], prompt_logprobs=None), CompletionSequenceGroupOutput(samples=[SequenceOutput(parent_seq_id=2, output_token=0, logprobs={0: Logprob(logprob=0.0, rank=-1, decoded_token=None)})], prompt_logprobs=None)], sampled_token_probs=None, sampled_token_ids=None, spec_decode_worker_metrics=None)]
LLM process output indices range(0, 3)
OUTPUTS PER SEQ GROUP (ONE IS PROMPT THE OTHER NOT) [SamplerOutput(outputs=[CompletionSequenceGroupOutput(samples=[SequenceOutput(parent_seq_id=3, output_token=-1, logprobs={-1: Logprob(logprob=0.0, rank=-1, decoded_token=None)})], prompt_logprobs=None), CompletionSequenceGroupOutput(samples=[SequenceOutput(parent_seq_id=1, output_token=0, logprobs={0: Logprob(logprob=0.0, rank=-1, decoded_token=None)})], prompt_logprobs=None), CompletionSequenceGroupOutput(samples=[SequenceOutput(parent_seq_id=2, output_token=0, logprobs={0: Logprob(logprob=0.0, rank=-1, decoded_token=None)})], prompt_logprobs=None)], sampled_token_probs=None, sampled_token_ids=None, spec_decode_worker_metrics=None)]
output has multiples False 1
Updating computed tokens manually
META DO_SAMPLE False
OUT [CompletionSequenceGroupOutput(samples=[SequenceOutput(parent_seq_id=3, output_token=-1, logprobs={-1: Logprob(logprob=0.0, rank=-1, decoded_token=None)})], prompt_logprobs=None)] 



SEQ GROUP DIO CANE SHOULD SAMPLE HERE False
SequenceGroup(request_id=3, sampling_params=SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=True, max_tokens=32, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), guided_decoding=None, num_seqs=1)
output has multiples False 1
SEQ GROUP DIO CANE SHOULD SAMPLE HERE True
SequenceGroup(request_id=1, sampling_params=SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=True, max_tokens=32, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), guided_decoding=None, num_seqs=1)
output has multiples False 1
SEQ GROUP DIO CANE SHOULD SAMPLE HERE True
SequenceGroup(request_id=2, sampling_params=SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=True, max_tokens=32, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), guided_decoding=None, num_seqs=1)
PROMPTS (is prompt, K, token_chunk, do_sample):  [(True, 0, 2, False), (False, None, 1, True), (False, None, 1, True)]
SPECULATIING WITH REQ METADATA [SequenceGroupMetadata(request_id='3', is_prompt=True, seq_data={3: SequenceData(prompt_token_ids=array('l', [1, 450, 5434, 310, 319, 29902, 338, 29892, 297, 596, 1914, 3838, 29901, 29871]), output_token_ids=(), cumulative_logprob=0.0, get_num_computed_tokens=8}, sampling_params=SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=True, max_tokens=32, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), guided_decoding=None, block_tables={3: [6]}, do_sample=False, pooling_params=None, lora_request=None, computed_block_nums=[], state=SequenceGroupState(num_steps=1, current_step=0), multi_modal_data={}, encoder_seq_data=None, cross_block_table=None, prompt_adapter_request=None, token_chunk_size=2, num_speculative_tokens=0), SequenceGroupMetadata(request_id='1', is_prompt=False, seq_data={1: SequenceData(prompt_token_ids=array('l', [1, 450, 5434, 310, 319, 29902, 338, 29892, 297, 596, 1914, 3838, 29901, 29871]), output_token_ids=(29896, 29900, 29901, 450, 29877, 29889, 29884, 29889, 29879, 29889, 13, 29900, 29900, 29901, 29871, 29900, 29900, 0, 0), cumulative_logprob=0.0, get_num_computed_tokens=33}, sampling_params=SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=True, max_tokens=32, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), guided_decoding=None, block_tables={1: [2, 4, 9]}, do_sample=True, pooling_params=None, lora_request=None, computed_block_nums=[], state=SequenceGroupState(num_steps=1, current_step=0), multi_modal_data={}, encoder_seq_data=None, cross_block_table=None, prompt_adapter_request=None, token_chunk_size=1, num_speculative_tokens=None), SequenceGroupMetadata(request_id='2', is_prompt=False, seq_data={2: SequenceData(prompt_token_ids=array('l', [1, 450, 6673, 310, 278, 3303, 3900, 338, 263, 767, 29892, 825, 29879, 670, 1024, 29973]), output_token_ids=(13, 1576, 7178, 310, 278, 3303, 3900, 29892, 278, 7178, 310, 278, 3303, 3900, 0, 0), cumulative_logprob=0.0, get_num_computed_tokens=32}, sampling_params=SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=True, max_tokens=32, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), guided_decoding=None, block_tables={2: [3, 7, 8]}, do_sample=True, pooling_params=None, lora_request=None, computed_block_nums=[], state=SequenceGroupState(num_steps=1, current_step=0), multi_modal_data={}, encoder_seq_data=None, cross_block_table=None, prompt_adapter_request=None, token_chunk_size=1, num_speculative_tokens=None)]
SCORING ON THE UNION OF REQUESTS (includes expanded batch), is prompt? [False, False, False, False, False, False, False, False, False, False, False, False, True]
SCORING ON THE UNION OF REQUESTS (includes expanded batch), do_sample? [True, True, True, True, True, True, True, True, True, True, True, True, False]
MIXEEED BATCH BOYS!
Proposal len X do_sample X is_prompt [(0, (False, True)), (5, (True, False)), (5, (True, False))]
Target probs distr tensor(0., device='cuda:0') tensor(3.1250e-05, device='cuda:0') tensor(1., device='cuda:0') torch.Size([2, 6, 32000])
Draft probs distr tensor(0., device='cuda:0') tensor(3.1250e-05, device='cuda:0') tensor(1., device='cuda:0') torch.Size([2, 5, 32000])
VERIFY STEP ENDED tensor([[-1, -1, -1, -1, -1, -1],
        [ 0, -1, -1, -1, -1, -1],
        [ 0, -1, -1, -1, -1, -1]], device='cuda:0')
SAMPLER OUTPUT LIST 1x3
LLM STep outputs [SamplerOutput(outputs=[CompletionSequenceGroupOutput(samples=[SequenceOutput(parent_seq_id=3, output_token=-1, logprobs={-1: Logprob(logprob=0.0, rank=-1, decoded_token=None)})], prompt_logprobs=None), CompletionSequenceGroupOutput(samples=[SequenceOutput(parent_seq_id=1, output_token=0, logprobs={0: Logprob(logprob=0.0, rank=-1, decoded_token=None)})], prompt_logprobs=None), CompletionSequenceGroupOutput(samples=[SequenceOutput(parent_seq_id=2, output_token=0, logprobs={0: Logprob(logprob=0.0, rank=-1, decoded_token=None)})], prompt_logprobs=None)], sampled_token_probs=None, sampled_token_ids=None, spec_decode_worker_metrics=None)]
LLM process output indices range(0, 3)
OUTPUTS PER SEQ GROUP (ONE IS PROMPT THE OTHER NOT) [SamplerOutput(outputs=[CompletionSequenceGroupOutput(samples=[SequenceOutput(parent_seq_id=3, output_token=-1, logprobs={-1: Logprob(logprob=0.0, rank=-1, decoded_token=None)})], prompt_logprobs=None), CompletionSequenceGroupOutput(samples=[SequenceOutput(parent_seq_id=1, output_token=0, logprobs={0: Logprob(logprob=0.0, rank=-1, decoded_token=None)})], prompt_logprobs=None), CompletionSequenceGroupOutput(samples=[SequenceOutput(parent_seq_id=2, output_token=0, logprobs={0: Logprob(logprob=0.0, rank=-1, decoded_token=None)})], prompt_logprobs=None)], sampled_token_probs=None, sampled_token_ids=None, spec_decode_worker_metrics=None)]
output has multiples False 1
Updating computed tokens manually
META DO_SAMPLE False
OUT [CompletionSequenceGroupOutput(samples=[SequenceOutput(parent_seq_id=3, output_token=-1, logprobs={-1: Logprob(logprob=0.0, rank=-1, decoded_token=None)})], prompt_logprobs=None)] 



SEQ GROUP DIO CANE SHOULD SAMPLE HERE False
SequenceGroup(request_id=3, sampling_params=SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=True, max_tokens=32, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), guided_decoding=None, num_seqs=1)
output has multiples False 1
SEQ GROUP DIO CANE SHOULD SAMPLE HERE True
SequenceGroup(request_id=1, sampling_params=SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=True, max_tokens=32, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), guided_decoding=None, num_seqs=1)
output has multiples False 1
SEQ GROUP DIO CANE SHOULD SAMPLE HERE True
SequenceGroup(request_id=2, sampling_params=SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=True, max_tokens=32, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), guided_decoding=None, num_seqs=1)
PROMPTS (is prompt, K, token_chunk, do_sample):  [(True, 0, 2, False), (False, None, 1, True), (False, None, 1, True)]
SPECULATIING WITH REQ METADATA [SequenceGroupMetadata(request_id='3', is_prompt=True, seq_data={3: SequenceData(prompt_token_ids=array('l', [1, 450, 5434, 310, 319, 29902, 338, 29892, 297, 596, 1914, 3838, 29901, 29871]), output_token_ids=(), cumulative_logprob=0.0, get_num_computed_tokens=10}, sampling_params=SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=True, max_tokens=32, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), guided_decoding=None, block_tables={3: [6]}, do_sample=False, pooling_params=None, lora_request=None, computed_block_nums=[], state=SequenceGroupState(num_steps=1, current_step=0), multi_modal_data={}, encoder_seq_data=None, cross_block_table=None, prompt_adapter_request=None, token_chunk_size=2, num_speculative_tokens=0), SequenceGroupMetadata(request_id='1', is_prompt=False, seq_data={1: SequenceData(prompt_token_ids=array('l', [1, 450, 5434, 310, 319, 29902, 338, 29892, 297, 596, 1914, 3838, 29901, 29871]), output_token_ids=(29896, 29900, 29901, 450, 29877, 29889, 29884, 29889, 29879, 29889, 13, 29900, 29900, 29901, 29871, 29900, 29900, 0, 0, 0), cumulative_logprob=0.0, get_num_computed_tokens=34}, sampling_params=SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=True, max_tokens=32, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), guided_decoding=None, block_tables={1: [2, 4, 9]}, do_sample=True, pooling_params=None, lora_request=None, computed_block_nums=[], state=SequenceGroupState(num_steps=1, current_step=0), multi_modal_data={}, encoder_seq_data=None, cross_block_table=None, prompt_adapter_request=None, token_chunk_size=1, num_speculative_tokens=None), SequenceGroupMetadata(request_id='2', is_prompt=False, seq_data={2: SequenceData(prompt_token_ids=array('l', [1, 450, 6673, 310, 278, 3303, 3900, 338, 263, 767, 29892, 825, 29879, 670, 1024, 29973]), output_token_ids=(13, 1576, 7178, 310, 278, 3303, 3900, 29892, 278, 7178, 310, 278, 3303, 3900, 0, 0, 0), cumulative_logprob=0.0, get_num_computed_tokens=33}, sampling_params=SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=True, max_tokens=32, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), guided_decoding=None, block_tables={2: [3, 7, 8]}, do_sample=True, pooling_params=None, lora_request=None, computed_block_nums=[], state=SequenceGroupState(num_steps=1, current_step=0), multi_modal_data={}, encoder_seq_data=None, cross_block_table=None, prompt_adapter_request=None, token_chunk_size=1, num_speculative_tokens=None)]
SCORING ON THE UNION OF REQUESTS (includes expanded batch), is prompt? [False, False, False, False, False, False, False, False, False, False, False, False, True]
SCORING ON THE UNION OF REQUESTS (includes expanded batch), do_sample? [True, True, True, True, True, True, True, True, True, True, True, True, False]
MIXEEED BATCH BOYS!
Proposal len X do_sample X is_prompt [(0, (False, True)), (5, (True, False)), (5, (True, False))]
Target probs distr tensor(0., device='cuda:0') tensor(3.1250e-05, device='cuda:0') tensor(1., device='cuda:0') torch.Size([2, 6, 32000])
Draft probs distr tensor(0., device='cuda:0') tensor(3.1250e-05, device='cuda:0') tensor(1., device='cuda:0') torch.Size([2, 5, 32000])
VERIFY STEP ENDED tensor([[-1, -1, -1, -1, -1, -1],
        [ 0, -1, -1, -1, -1, -1],
        [ 0, -1, -1, -1, -1, -1]], device='cuda:0')
SAMPLER OUTPUT LIST 1x3
LLM STep outputs [SamplerOutput(outputs=[CompletionSequenceGroupOutput(samples=[SequenceOutput(parent_seq_id=3, output_token=-1, logprobs={-1: Logprob(logprob=0.0, rank=-1, decoded_token=None)})], prompt_logprobs=None), CompletionSequenceGroupOutput(samples=[SequenceOutput(parent_seq_id=1, output_token=0, logprobs={0: Logprob(logprob=0.0, rank=-1, decoded_token=None)})], prompt_logprobs=None), CompletionSequenceGroupOutput(samples=[SequenceOutput(parent_seq_id=2, output_token=0, logprobs={0: Logprob(logprob=0.0, rank=-1, decoded_token=None)})], prompt_logprobs=None)], sampled_token_probs=None, sampled_token_ids=None, spec_decode_worker_metrics=None)]
LLM process output indices range(0, 3)
OUTPUTS PER SEQ GROUP (ONE IS PROMPT THE OTHER NOT) [SamplerOutput(outputs=[CompletionSequenceGroupOutput(samples=[SequenceOutput(parent_seq_id=3, output_token=-1, logprobs={-1: Logprob(logprob=0.0, rank=-1, decoded_token=None)})], prompt_logprobs=None), CompletionSequenceGroupOutput(samples=[SequenceOutput(parent_seq_id=1, output_token=0, logprobs={0: Logprob(logprob=0.0, rank=-1, decoded_token=None)})], prompt_logprobs=None), CompletionSequenceGroupOutput(samples=[SequenceOutput(parent_seq_id=2, output_token=0, logprobs={0: Logprob(logprob=0.0, rank=-1, decoded_token=None)})], prompt_logprobs=None)], sampled_token_probs=None, sampled_token_ids=None, spec_decode_worker_metrics=None)]
output has multiples False 1
Updating computed tokens manually
META DO_SAMPLE False
OUT [CompletionSequenceGroupOutput(samples=[SequenceOutput(parent_seq_id=3, output_token=-1, logprobs={-1: Logprob(logprob=0.0, rank=-1, decoded_token=None)})], prompt_logprobs=None)] 



SEQ GROUP DIO CANE SHOULD SAMPLE HERE False
SequenceGroup(request_id=3, sampling_params=SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=True, max_tokens=32, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), guided_decoding=None, num_seqs=1)
output has multiples False 1
SEQ GROUP DIO CANE SHOULD SAMPLE HERE True
SequenceGroup(request_id=1, sampling_params=SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=True, max_tokens=32, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), guided_decoding=None, num_seqs=1)
output has multiples False 1
SEQ GROUP DIO CANE SHOULD SAMPLE HERE True
SequenceGroup(request_id=2, sampling_params=SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=True, max_tokens=32, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), guided_decoding=None, num_seqs=1)
PROMPTS (is prompt, K, token_chunk, do_sample):  [(True, 0, 2, True), (False, None, 1, True), (False, None, 1, True)]
SPECULATIING WITH REQ METADATA [SequenceGroupMetadata(request_id='3', is_prompt=True, seq_data={3: SequenceData(prompt_token_ids=array('l', [1, 450, 5434, 310, 319, 29902, 338, 29892, 297, 596, 1914, 3838, 29901, 29871]), output_token_ids=(), cumulative_logprob=0.0, get_num_computed_tokens=12}, sampling_params=SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=True, max_tokens=32, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), guided_decoding=None, block_tables={3: [6]}, do_sample=True, pooling_params=None, lora_request=None, computed_block_nums=[], state=SequenceGroupState(num_steps=1, current_step=0), multi_modal_data={}, encoder_seq_data=None, cross_block_table=None, prompt_adapter_request=None, token_chunk_size=2, num_speculative_tokens=0), SequenceGroupMetadata(request_id='1', is_prompt=False, seq_data={1: SequenceData(prompt_token_ids=array('l', [1, 450, 5434, 310, 319, 29902, 338, 29892, 297, 596, 1914, 3838, 29901, 29871]), output_token_ids=(29896, 29900, 29901, 450, 29877, 29889, 29884, 29889, 29879, 29889, 13, 29900, 29900, 29901, 29871, 29900, 29900, 0, 0, 0, 0), cumulative_logprob=0.0, get_num_computed_tokens=35}, sampling_params=SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=True, max_tokens=32, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), guided_decoding=None, block_tables={1: [2, 4, 9]}, do_sample=True, pooling_params=None, lora_request=None, computed_block_nums=[], state=SequenceGroupState(num_steps=1, current_step=0), multi_modal_data={}, encoder_seq_data=None, cross_block_table=None, prompt_adapter_request=None, token_chunk_size=1, num_speculative_tokens=None), SequenceGroupMetadata(request_id='2', is_prompt=False, seq_data={2: SequenceData(prompt_token_ids=array('l', [1, 450, 6673, 310, 278, 3303, 3900, 338, 263, 767, 29892, 825, 29879, 670, 1024, 29973]), output_token_ids=(13, 1576, 7178, 310, 278, 3303, 3900, 29892, 278, 7178, 310, 278, 3303, 3900, 0, 0, 0, 0), cumulative_logprob=0.0, get_num_computed_tokens=34}, sampling_params=SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=True, max_tokens=32, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), guided_decoding=None, block_tables={2: [3, 7, 8]}, do_sample=True, pooling_params=None, lora_request=None, computed_block_nums=[], state=SequenceGroupState(num_steps=1, current_step=0), multi_modal_data={}, encoder_seq_data=None, cross_block_table=None, prompt_adapter_request=None, token_chunk_size=1, num_speculative_tokens=None)]
SCORING ON THE UNION OF REQUESTS (includes expanded batch), is prompt? [False, False, False, False, False, False, False, False, False, False, False, False, True]
SCORING ON THE UNION OF REQUESTS (includes expanded batch), do_sample? [True, True, True, True, True, True, True, True, True, True, True, True, True]
MIXEEED BATCH BOYS!
Proposal len X do_sample X is_prompt [(0, (True, True)), (5, (True, False)), (5, (True, False))]
Target probs distr tensor(0., device='cuda:0') tensor(3.1250e-05, device='cuda:0') tensor(1., device='cuda:0') torch.Size([2, 6, 32000])
Draft probs distr tensor(0., device='cuda:0') tensor(3.1250e-05, device='cuda:0') tensor(1., device='cuda:0') torch.Size([2, 5, 32000])
VERIFY STEP ENDED tensor([[29906,    -1,    -1,    -1,    -1,    -1],
        [    0,    -1,    -1,    -1,    -1,    -1],
        [    0,    -1,    -1,    -1,    -1,    -1]], device='cuda:0')
SAMPLER OUTPUT LIST 1x3
LLM STep outputs [SamplerOutput(outputs=[CompletionSequenceGroupOutput(samples=[SequenceOutput(parent_seq_id=3, output_token=29906, logprobs={29906: Logprob(logprob=0.0, rank=-1, decoded_token=None)})], prompt_logprobs=None), CompletionSequenceGroupOutput(samples=[SequenceOutput(parent_seq_id=1, output_token=0, logprobs={0: Logprob(logprob=0.0, rank=-1, decoded_token=None)})], prompt_logprobs=None), CompletionSequenceGroupOutput(samples=[SequenceOutput(parent_seq_id=2, output_token=0, logprobs={0: Logprob(logprob=0.0, rank=-1, decoded_token=None)})], prompt_logprobs=None)], sampled_token_probs=None, sampled_token_ids=None, spec_decode_worker_metrics=None)]
LLM process output indices range(0, 3)
OUTPUTS PER SEQ GROUP (ONE IS PROMPT THE OTHER NOT) [SamplerOutput(outputs=[CompletionSequenceGroupOutput(samples=[SequenceOutput(parent_seq_id=3, output_token=29906, logprobs={29906: Logprob(logprob=0.0, rank=-1, decoded_token=None)})], prompt_logprobs=None), CompletionSequenceGroupOutput(samples=[SequenceOutput(parent_seq_id=1, output_token=0, logprobs={0: Logprob(logprob=0.0, rank=-1, decoded_token=None)})], prompt_logprobs=None), CompletionSequenceGroupOutput(samples=[SequenceOutput(parent_seq_id=2, output_token=0, logprobs={0: Logprob(logprob=0.0, rank=-1, decoded_token=None)})], prompt_logprobs=None)], sampled_token_probs=None, sampled_token_ids=None, spec_decode_worker_metrics=None)]
output has multiples False 1
Updating computed tokens manually
META DO_SAMPLE True
OUT [CompletionSequenceGroupOutput(samples=[SequenceOutput(parent_seq_id=3, output_token=29906, logprobs={29906: Logprob(logprob=0.0, rank=-1, decoded_token=None)})], prompt_logprobs=None)] 



SEQ GROUP DIO CANE SHOULD SAMPLE HERE True
SequenceGroup(request_id=3, sampling_params=SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=True, max_tokens=32, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), guided_decoding=None, num_seqs=1)
output has multiples False 1
SEQ GROUP DIO CANE SHOULD SAMPLE HERE True
SequenceGroup(request_id=1, sampling_params=SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=True, max_tokens=32, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), guided_decoding=None, num_seqs=1)
output has multiples False 1
SEQ GROUP DIO CANE SHOULD SAMPLE HERE True
SequenceGroup(request_id=2, sampling_params=SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=True, max_tokens=32, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), guided_decoding=None, num_seqs=1)
PROMPTS (is prompt, K, token_chunk, do_sample):  [(False, None, 1, True), (False, None, 1, True), (False, None, 1, True)]
SPECULATIING WITH REQ METADATA [SequenceGroupMetadata(request_id='1', is_prompt=False, seq_data={1: SequenceData(prompt_token_ids=array('l', [1, 450, 5434, 310, 319, 29902, 338, 29892, 297, 596, 1914, 3838, 29901, 29871]), output_token_ids=(29896, 29900, 29901, 450, 29877, 29889, 29884, 29889, 29879, 29889, 13, 29900, 29900, 29901, 29871, 29900, 29900, 0, 0, 0, 0, 0), cumulative_logprob=0.0, get_num_computed_tokens=36}, sampling_params=SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=True, max_tokens=32, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), guided_decoding=None, block_tables={1: [2, 4, 9]}, do_sample=True, pooling_params=None, lora_request=None, computed_block_nums=[], state=SequenceGroupState(num_steps=1, current_step=0), multi_modal_data=None, encoder_seq_data=None, cross_block_table=None, prompt_adapter_request=None, token_chunk_size=1, num_speculative_tokens=None), SequenceGroupMetadata(request_id='2', is_prompt=False, seq_data={2: SequenceData(prompt_token_ids=array('l', [1, 450, 6673, 310, 278, 3303, 3900, 338, 263, 767, 29892, 825, 29879, 670, 1024, 29973]), output_token_ids=(13, 1576, 7178, 310, 278, 3303, 3900, 29892, 278, 7178, 310, 278, 3303, 3900, 0, 0, 0, 0, 0), cumulative_logprob=0.0, get_num_computed_tokens=35}, sampling_params=SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=True, max_tokens=32, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), guided_decoding=None, block_tables={2: [3, 7, 8]}, do_sample=True, pooling_params=None, lora_request=None, computed_block_nums=[], state=SequenceGroupState(num_steps=1, current_step=0), multi_modal_data=None, encoder_seq_data=None, cross_block_table=None, prompt_adapter_request=None, token_chunk_size=1, num_speculative_tokens=None), SequenceGroupMetadata(request_id='3', is_prompt=False, seq_data={3: SequenceData(prompt_token_ids=array('l', [1, 450, 5434, 310, 319, 29902, 338, 29892, 297, 596, 1914, 3838, 29901, 29871]), output_token_ids=(29906,), cumulative_logprob=0.0, get_num_computed_tokens=15}, sampling_params=SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=True, max_tokens=32, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), guided_decoding=None, block_tables={3: [6, 5]}, do_sample=True, pooling_params=None, lora_request=None, computed_block_nums=[], state=SequenceGroupState(num_steps=1, current_step=0), multi_modal_data=None, encoder_seq_data=None, cross_block_table=None, prompt_adapter_request=None, token_chunk_size=1, num_speculative_tokens=None)]
SCORING ON THE UNION OF REQUESTS (includes expanded batch), is prompt? [False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False]
SCORING ON THE UNION OF REQUESTS (includes expanded batch), do_sample? [True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True]
Proposal len X do_sample X is_prompt [(5, (True, False)), (5, (True, False)), (5, (True, False))]
Target probs distr tensor(0., device='cuda:0') tensor(3.1250e-05, device='cuda:0') tensor(1., device='cuda:0') torch.Size([3, 6, 32000])
Draft probs distr tensor(0., device='cuda:0') tensor(3.1250e-05, device='cuda:0') tensor(1., device='cuda:0') torch.Size([3, 5, 32000])
VERIFY STEP ENDED tensor([[    0,    -1,    -1,    -1,    -1,    -1],
        [  450, 29871, 29906, 29900, 29896, 29929],
        [29900, 29896, 29947, 29892,    -1,    -1]], device='cuda:0')
SAMPLER OUTPUT LIST 6x3
LLM STep outputs [SamplerOutput(outputs=[CompletionSequenceGroupOutput(samples=[SequenceOutput(parent_seq_id=1, output_token=0, logprobs={0: Logprob(logprob=0.0, rank=-1, decoded_token=None)})], prompt_logprobs=None), CompletionSequenceGroupOutput(samples=[SequenceOutput(parent_seq_id=2, output_token=450, logprobs={450: Logprob(logprob=0.0, rank=-1, decoded_token=None)})], prompt_logprobs=None), CompletionSequenceGroupOutput(samples=[SequenceOutput(parent_seq_id=3, output_token=29900, logprobs={29900: Logprob(logprob=0.0, rank=-1, decoded_token=None)})], prompt_logprobs=None)], sampled_token_probs=None, sampled_token_ids=None, spec_decode_worker_metrics=None), SamplerOutput(outputs=[CompletionSequenceGroupOutput(samples=[SequenceOutput(parent_seq_id=1, output_token=-1, logprobs={-1: Logprob(logprob=0.0, rank=-1, decoded_token=None)})], prompt_logprobs=None), CompletionSequenceGroupOutput(samples=[SequenceOutput(parent_seq_id=2, output_token=29871, logprobs={29871: Logprob(logprob=0.0, rank=-1, decoded_token=None)})], prompt_logprobs=None), CompletionSequenceGroupOutput(samples=[SequenceOutput(parent_seq_id=3, output_token=29896, logprobs={29896: Logprob(logprob=0.0, rank=-1, decoded_token=None)})], prompt_logprobs=None)], sampled_token_probs=None, sampled_token_ids=None, spec_decode_worker_metrics=None), SamplerOutput(outputs=[CompletionSequenceGroupOutput(samples=[SequenceOutput(parent_seq_id=1, output_token=-1, logprobs={-1: Logprob(logprob=0.0, rank=-1, decoded_token=None)})], prompt_logprobs=None), CompletionSequenceGroupOutput(samples=[SequenceOutput(parent_seq_id=2, output_token=29906, logprobs={29906: Logprob(logprob=0.0, rank=-1, decoded_token=None)})], prompt_logprobs=None), CompletionSequenceGroupOutput(samples=[SequenceOutput(parent_seq_id=3, output_token=29947, logprobs={29947: Logprob(logprob=0.0, rank=-1, decoded_token=None)})], prompt_logprobs=None)], sampled_token_probs=None, sampled_token_ids=None, spec_decode_worker_metrics=None), SamplerOutput(outputs=[CompletionSequenceGroupOutput(samples=[SequenceOutput(parent_seq_id=1, output_token=-1, logprobs={-1: Logprob(logprob=0.0, rank=-1, decoded_token=None)})], prompt_logprobs=None), CompletionSequenceGroupOutput(samples=[SequenceOutput(parent_seq_id=2, output_token=29900, logprobs={29900: Logprob(logprob=0.0, rank=-1, decoded_token=None)})], prompt_logprobs=None), CompletionSequenceGroupOutput(samples=[SequenceOutput(parent_seq_id=3, output_token=29892, logprobs={29892: Logprob(logprob=0.0, rank=-1, decoded_token=None)})], prompt_logprobs=None)], sampled_token_probs=None, sampled_token_ids=None, spec_decode_worker_metrics=None), SamplerOutput(outputs=[CompletionSequenceGroupOutput(samples=[SequenceOutput(parent_seq_id=1, output_token=-1, logprobs={-1: Logprob(logprob=0.0, rank=-1, decoded_token=None)})], prompt_logprobs=None), CompletionSequenceGroupOutput(samples=[SequenceOutput(parent_seq_id=2, output_token=29896, logprobs={29896: Logprob(logprob=0.0, rank=-1, decoded_token=None)})], prompt_logprobs=None), CompletionSequenceGroupOutput(samples=[SequenceOutput(parent_seq_id=3, output_token=-1, logprobs={-1: Logprob(logprob=0.0, rank=-1, decoded_token=None)})], prompt_logprobs=None)], sampled_token_probs=None, sampled_token_ids=None, spec_decode_worker_metrics=None), SamplerOutput(outputs=[CompletionSequenceGroupOutput(samples=[SequenceOutput(parent_seq_id=1, output_token=-1, logprobs={-1: Logprob(logprob=0.0, rank=-1, decoded_token=None)})], prompt_logprobs=None), CompletionSequenceGroupOutput(samples=[SequenceOutput(parent_seq_id=2, output_token=29929, logprobs={29929: Logprob(logprob=0.0, rank=-1, decoded_token=None)})], prompt_logprobs=None), CompletionSequenceGroupOutput(samples=[SequenceOutput(parent_seq_id=3, output_token=-1, logprobs={-1: Logprob(logprob=0.0, rank=-1, decoded_token=None)})], prompt_logprobs=None)], sampled_token_probs=None, sampled_token_ids=None, spec_decode_worker_metrics=None)]
LLM process output indices range(0, 3)
OUTPUTS PER SEQ GROUP (ONE IS PROMPT THE OTHER NOT) [[CompletionSequenceGroupOutput(samples=[SequenceOutput(parent_seq_id=1, output_token=0, logprobs={0: Logprob(logprob=0.0, rank=-1, decoded_token=None)})], prompt_logprobs=None), CompletionSequenceGroupOutput(samples=[SequenceOutput(parent_seq_id=1, output_token=-1, logprobs={-1: Logprob(logprob=0.0, rank=-1, decoded_token=None)})], prompt_logprobs=None), CompletionSequenceGroupOutput(samples=[SequenceOutput(parent_seq_id=1, output_token=-1, logprobs={-1: Logprob(logprob=0.0, rank=-1, decoded_token=None)})], prompt_logprobs=None), CompletionSequenceGroupOutput(samples=[SequenceOutput(parent_seq_id=1, output_token=-1, logprobs={-1: Logprob(logprob=0.0, rank=-1, decoded_token=None)})], prompt_logprobs=None), CompletionSequenceGroupOutput(samples=[SequenceOutput(parent_seq_id=1, output_token=-1, logprobs={-1: Logprob(logprob=0.0, rank=-1, decoded_token=None)})], prompt_logprobs=None), CompletionSequenceGroupOutput(samples=[SequenceOutput(parent_seq_id=1, output_token=-1, logprobs={-1: Logprob(logprob=0.0, rank=-1, decoded_token=None)})], prompt_logprobs=None)], [CompletionSequenceGroupOutput(samples=[SequenceOutput(parent_seq_id=2, output_token=450, logprobs={450: Logprob(logprob=0.0, rank=-1, decoded_token=None)})], prompt_logprobs=None), CompletionSequenceGroupOutput(samples=[SequenceOutput(parent_seq_id=2, output_token=29871, logprobs={29871: Logprob(logprob=0.0, rank=-1, decoded_token=None)})], prompt_logprobs=None), CompletionSequenceGroupOutput(samples=[SequenceOutput(parent_seq_id=2, output_token=29906, logprobs={29906: Logprob(logprob=0.0, rank=-1, decoded_token=None)})], prompt_logprobs=None), CompletionSequenceGroupOutput(samples=[SequenceOutput(parent_seq_id=2, output_token=29900, logprobs={29900: Logprob(logprob=0.0, rank=-1, decoded_token=None)})], prompt_logprobs=None), CompletionSequenceGroupOutput(samples=[SequenceOutput(parent_seq_id=2, output_token=29896, logprobs={29896: Logprob(logprob=0.0, rank=-1, decoded_token=None)})], prompt_logprobs=None), CompletionSequenceGroupOutput(samples=[SequenceOutput(parent_seq_id=2, output_token=29929, logprobs={29929: Logprob(logprob=0.0, rank=-1, decoded_token=None)})], prompt_logprobs=None)], [CompletionSequenceGroupOutput(samples=[SequenceOutput(parent_seq_id=3, output_token=29900, logprobs={29900: Logprob(logprob=0.0, rank=-1, decoded_token=None)})], prompt_logprobs=None), CompletionSequenceGroupOutput(samples=[SequenceOutput(parent_seq_id=3, output_token=29896, logprobs={29896: Logprob(logprob=0.0, rank=-1, decoded_token=None)})], prompt_logprobs=None), CompletionSequenceGroupOutput(samples=[SequenceOutput(parent_seq_id=3, output_token=29947, logprobs={29947: Logprob(logprob=0.0, rank=-1, decoded_token=None)})], prompt_logprobs=None), CompletionSequenceGroupOutput(samples=[SequenceOutput(parent_seq_id=3, output_token=29892, logprobs={29892: Logprob(logprob=0.0, rank=-1, decoded_token=None)})], prompt_logprobs=None), CompletionSequenceGroupOutput(samples=[SequenceOutput(parent_seq_id=3, output_token=-1, logprobs={-1: Logprob(logprob=0.0, rank=-1, decoded_token=None)})], prompt_logprobs=None), CompletionSequenceGroupOutput(samples=[SequenceOutput(parent_seq_id=3, output_token=-1, logprobs={-1: Logprob(logprob=0.0, rank=-1, decoded_token=None)})], prompt_logprobs=None)]]
output has multiples True 6
SEQ GROUP DIO CANE SHOULD SAMPLE HERE True
SequenceGroup(request_id=1, sampling_params=SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=True, max_tokens=32, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), guided_decoding=None, num_seqs=1)
output has multiples True 6
SEQ GROUP DIO CANE SHOULD SAMPLE HERE True
SequenceGroup(request_id=2, sampling_params=SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=True, max_tokens=32, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), guided_decoding=None, num_seqs=1)
output has multiples True 6
SEQ GROUP DIO CANE SHOULD SAMPLE HERE True
SequenceGroup(request_id=3, sampling_params=SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=True, max_tokens=32, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), guided_decoding=None, num_seqs=1)
PROMPTS (is prompt, K, token_chunk, do_sample):  [(False, None, 1, True), (False, None, 1, True), (False, None, 1, True)]
SPECULATIING WITH REQ METADATA [SequenceGroupMetadata(request_id='1', is_prompt=False, seq_data={1: SequenceData(prompt_token_ids=array('l', [1, 450, 5434, 310, 319, 29902, 338, 29892, 297, 596, 1914, 3838, 29901, 29871]), output_token_ids=(29896, 29900, 29901, 450, 29877, 29889, 29884, 29889, 29879, 29889, 13, 29900, 29900, 29901, 29871, 29900, 29900, 0, 0, 0, 0, 0, 0), cumulative_logprob=0.0, get_num_computed_tokens=37}, sampling_params=SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=True, max_tokens=32, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), guided_decoding=None, block_tables={1: [2, 4, 9]}, do_sample=True, pooling_params=None, lora_request=None, computed_block_nums=[], state=SequenceGroupState(num_steps=1, current_step=0), multi_modal_data=None, encoder_seq_data=None, cross_block_table=None, prompt_adapter_request=None, token_chunk_size=1, num_speculative_tokens=None), SequenceGroupMetadata(request_id='2', is_prompt=False, seq_data={2: SequenceData(prompt_token_ids=array('l', [1, 450, 6673, 310, 278, 3303, 3900, 338, 263, 767, 29892, 825, 29879, 670, 1024, 29973]), output_token_ids=(13, 1576, 7178, 310, 278, 3303, 3900, 29892, 278, 7178, 310, 278, 3303, 3900, 0, 0, 0, 0, 0, 450, 29871, 29906, 29900, 29896, 29929), cumulative_logprob=0.0, get_num_computed_tokens=41}, sampling_params=SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=True, max_tokens=32, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), guided_decoding=None, block_tables={2: [3, 7, 8]}, do_sample=True, pooling_params=None, lora_request=None, computed_block_nums=[], state=SequenceGroupState(num_steps=1, current_step=0), multi_modal_data=None, encoder_seq_data=None, cross_block_table=None, prompt_adapter_request=None, token_chunk_size=1, num_speculative_tokens=None), SequenceGroupMetadata(request_id='3', is_prompt=False, seq_data={3: SequenceData(prompt_token_ids=array('l', [1, 450, 5434, 310, 319, 29902, 338, 29892, 297, 596, 1914, 3838, 29901, 29871]), output_token_ids=(29906, 29900, 29896, 29947, 29892), cumulative_logprob=0.0, get_num_computed_tokens=19}, sampling_params=SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=True, max_tokens=32, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), guided_decoding=None, block_tables={3: [6, 5]}, do_sample=True, pooling_params=None, lora_request=None, computed_block_nums=[], state=SequenceGroupState(num_steps=1, current_step=0), multi_modal_data=None, encoder_seq_data=None, cross_block_table=None, prompt_adapter_request=None, token_chunk_size=1, num_speculative_tokens=None)]
SCORING ON THE UNION OF REQUESTS (includes expanded batch), is prompt? [False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False]
SCORING ON THE UNION OF REQUESTS (includes expanded batch), do_sample? [True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True]
Proposal len X do_sample X is_prompt [(5, (True, False)), (5, (True, False)), (5, (True, False))]
Target probs distr tensor(0., device='cuda:0') tensor(3.1250e-05, device='cuda:0') tensor(1., device='cuda:0') torch.Size([3, 6, 32000])
Draft probs distr tensor(0., device='cuda:0') tensor(3.1250e-05, device='cuda:0') tensor(1., device='cuda:0') torch.Size([3, 5, 32000])
VERIFY STEP ENDED tensor([[    0,    -1,    -1,    -1,    -1,    -1],
        [29899, 29906,    -1,    -1,    -1,    -1],
        [29871, 29906, 29900, 29896, 29929, 29889]], device='cuda:0')
SAMPLER OUTPUT LIST 6x3
LLM STep outputs [SamplerOutput(outputs=[CompletionSequenceGroupOutput(samples=[SequenceOutput(parent_seq_id=1, output_token=0, logprobs={0: Logprob(logprob=0.0, rank=-1, decoded_token=None)})], prompt_logprobs=None), CompletionSequenceGroupOutput(samples=[SequenceOutput(parent_seq_id=2, output_token=29899, logprobs={29899: Logprob(logprob=0.0, rank=-1, decoded_token=None)})], prompt_logprobs=None), CompletionSequenceGroupOutput(samples=[SequenceOutput(parent_seq_id=3, output_token=29871, logprobs={29871: Logprob(logprob=0.0, rank=-1, decoded_token=None)})], prompt_logprobs=None)], sampled_token_probs=None, sampled_token_ids=None, spec_decode_worker_metrics=None), SamplerOutput(outputs=[CompletionSequenceGroupOutput(samples=[SequenceOutput(parent_seq_id=1, output_token=-1, logprobs={-1: Logprob(logprob=0.0, rank=-1, decoded_token=None)})], prompt_logprobs=None), CompletionSequenceGroupOutput(samples=[SequenceOutput(parent_seq_id=2, output_token=29906, logprobs={29906: Logprob(logprob=0.0, rank=-1, decoded_token=None)})], prompt_logprobs=None), CompletionSequenceGroupOutput(samples=[SequenceOutput(parent_seq_id=3, output_token=29906, logprobs={29906: Logprob(logprob=0.0, rank=-1, decoded_token=None)})], prompt_logprobs=None)], sampled_token_probs=None, sampled_token_ids=None, spec_decode_worker_metrics=None), SamplerOutput(outputs=[CompletionSequenceGroupOutput(samples=[SequenceOutput(parent_seq_id=1, output_token=-1, logprobs={-1: Logprob(logprob=0.0, rank=-1, decoded_token=None)})], prompt_logprobs=None), CompletionSequenceGroupOutput(samples=[SequenceOutput(parent_seq_id=2, output_token=-1, logprobs={-1: Logprob(logprob=0.0, rank=-1, decoded_token=None)})], prompt_logprobs=None), CompletionSequenceGroupOutput(samples=[SequenceOutput(parent_seq_id=3, output_token=29900, logprobs={29900: Logprob(logprob=0.0, rank=-1, decoded_token=None)})], prompt_logprobs=None)], sampled_token_probs=None, sampled_token_ids=None, spec_decode_worker_metrics=None), SamplerOutput(outputs=[CompletionSequenceGroupOutput(samples=[SequenceOutput(parent_seq_id=1, output_token=-1, logprobs={-1: Logprob(logprob=0.0, rank=-1, decoded_token=None)})], prompt_logprobs=None), CompletionSequenceGroupOutput(samples=[SequenceOutput(parent_seq_id=2, output_token=-1, logprobs={-1: Logprob(logprob=0.0, rank=-1, decoded_token=None)})], prompt_logprobs=None), CompletionSequenceGroupOutput(samples=[SequenceOutput(parent_seq_id=3, output_token=29896, logprobs={29896: Logprob(logprob=0.0, rank=-1, decoded_token=None)})], prompt_logprobs=None)], sampled_token_probs=None, sampled_token_ids=None, spec_decode_worker_metrics=None), SamplerOutput(outputs=[CompletionSequenceGroupOutput(samples=[SequenceOutput(parent_seq_id=1, output_token=-1, logprobs={-1: Logprob(logprob=0.0, rank=-1, decoded_token=None)})], prompt_logprobs=None), CompletionSequenceGroupOutput(samples=[SequenceOutput(parent_seq_id=2, output_token=-1, logprobs={-1: Logprob(logprob=0.0, rank=-1, decoded_token=None)})], prompt_logprobs=None), CompletionSequenceGroupOutput(samples=[SequenceOutput(parent_seq_id=3, output_token=29929, logprobs={29929: Logprob(logprob=0.0, rank=-1, decoded_token=None)})], prompt_logprobs=None)], sampled_token_probs=None, sampled_token_ids=None, spec_decode_worker_metrics=None), SamplerOutput(outputs=[CompletionSequenceGroupOutput(samples=[SequenceOutput(parent_seq_id=1, output_token=-1, logprobs={-1: Logprob(logprob=0.0, rank=-1, decoded_token=None)})], prompt_logprobs=None), CompletionSequenceGroupOutput(samples=[SequenceOutput(parent_seq_id=2, output_token=-1, logprobs={-1: Logprob(logprob=0.0, rank=-1, decoded_token=None)})], prompt_logprobs=None), CompletionSequenceGroupOutput(samples=[SequenceOutput(parent_seq_id=3, output_token=29889, logprobs={29889: Logprob(logprob=0.0, rank=-1, decoded_token=None)})], prompt_logprobs=None)], sampled_token_probs=None, sampled_token_ids=None, spec_decode_worker_metrics=None)]
LLM process output indices range(0, 3)
OUTPUTS PER SEQ GROUP (ONE IS PROMPT THE OTHER NOT) [[CompletionSequenceGroupOutput(samples=[SequenceOutput(parent_seq_id=1, output_token=0, logprobs={0: Logprob(logprob=0.0, rank=-1, decoded_token=None)})], prompt_logprobs=None), CompletionSequenceGroupOutput(samples=[SequenceOutput(parent_seq_id=1, output_token=-1, logprobs={-1: Logprob(logprob=0.0, rank=-1, decoded_token=None)})], prompt_logprobs=None), CompletionSequenceGroupOutput(samples=[SequenceOutput(parent_seq_id=1, output_token=-1, logprobs={-1: Logprob(logprob=0.0, rank=-1, decoded_token=None)})], prompt_logprobs=None), CompletionSequenceGroupOutput(samples=[SequenceOutput(parent_seq_id=1, output_token=-1, logprobs={-1: Logprob(logprob=0.0, rank=-1, decoded_token=None)})], prompt_logprobs=None), CompletionSequenceGroupOutput(samples=[SequenceOutput(parent_seq_id=1, output_token=-1, logprobs={-1: Logprob(logprob=0.0, rank=-1, decoded_token=None)})], prompt_logprobs=None), CompletionSequenceGroupOutput(samples=[SequenceOutput(parent_seq_id=1, output_token=-1, logprobs={-1: Logprob(logprob=0.0, rank=-1, decoded_token=None)})], prompt_logprobs=None)], [CompletionSequenceGroupOutput(samples=[SequenceOutput(parent_seq_id=2, output_token=29899, logprobs={29899: Logprob(logprob=0.0, rank=-1, decoded_token=None)})], prompt_logprobs=None), CompletionSequenceGroupOutput(samples=[SequenceOutput(parent_seq_id=2, output_token=29906, logprobs={29906: Logprob(logprob=0.0, rank=-1, decoded_token=None)})], prompt_logprobs=None), CompletionSequenceGroupOutput(samples=[SequenceOutput(parent_seq_id=2, output_token=-1, logprobs={-1: Logprob(logprob=0.0, rank=-1, decoded_token=None)})], prompt_logprobs=None), CompletionSequenceGroupOutput(samples=[SequenceOutput(parent_seq_id=2, output_token=-1, logprobs={-1: Logprob(logprob=0.0, rank=-1, decoded_token=None)})], prompt_logprobs=None), CompletionSequenceGroupOutput(samples=[SequenceOutput(parent_seq_id=2, output_token=-1, logprobs={-1: Logprob(logprob=0.0, rank=-1, decoded_token=None)})], prompt_logprobs=None), CompletionSequenceGroupOutput(samples=[SequenceOutput(parent_seq_id=2, output_token=-1, logprobs={-1: Logprob(logprob=0.0, rank=-1, decoded_token=None)})], prompt_logprobs=None)], [CompletionSequenceGroupOutput(samples=[SequenceOutput(parent_seq_id=3, output_token=29871, logprobs={29871: Logprob(logprob=0.0, rank=-1, decoded_token=None)})], prompt_logprobs=None), CompletionSequenceGroupOutput(samples=[SequenceOutput(parent_seq_id=3, output_token=29906, logprobs={29906: Logprob(logprob=0.0, rank=-1, decoded_token=None)})], prompt_logprobs=None), CompletionSequenceGroupOutput(samples=[SequenceOutput(parent_seq_id=3, output_token=29900, logprobs={29900: Logprob(logprob=0.0, rank=-1, decoded_token=None)})], prompt_logprobs=None), CompletionSequenceGroupOutput(samples=[SequenceOutput(parent_seq_id=3, output_token=29896, logprobs={29896: Logprob(logprob=0.0, rank=-1, decoded_token=None)})], prompt_logprobs=None), CompletionSequenceGroupOutput(samples=[SequenceOutput(parent_seq_id=3, output_token=29929, logprobs={29929: Logprob(logprob=0.0, rank=-1, decoded_token=None)})], prompt_logprobs=None), CompletionSequenceGroupOutput(samples=[SequenceOutput(parent_seq_id=3, output_token=29889, logprobs={29889: Logprob(logprob=0.0, rank=-1, decoded_token=None)})], prompt_logprobs=None)]]
output has multiples True 6
SEQ GROUP DIO CANE SHOULD SAMPLE HERE True
SequenceGroup(request_id=1, sampling_params=SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=True, max_tokens=32, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), guided_decoding=None, num_seqs=1)
output has multiples True 6
SEQ GROUP DIO CANE SHOULD SAMPLE HERE True
SequenceGroup(request_id=2, sampling_params=SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=True, max_tokens=32, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), guided_decoding=None, num_seqs=1)
output has multiples True 6
SEQ GROUP DIO CANE SHOULD SAMPLE HERE True
SequenceGroup(request_id=3, sampling_params=SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=True, max_tokens=32, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), guided_decoding=None, num_seqs=1)
PROMPTS (is prompt, K, token_chunk, do_sample):  [(False, None, 1, True), (False, None, 1, True), (False, None, 1, True)]
SPECULATIING WITH REQ METADATA [SequenceGroupMetadata(request_id='1', is_prompt=False, seq_data={1: SequenceData(prompt_token_ids=array('l', [1, 450, 5434, 310, 319, 29902, 338, 29892, 297, 596, 1914, 3838, 29901, 29871]), output_token_ids=(29896, 29900, 29901, 450, 29877, 29889, 29884, 29889, 29879, 29889, 13, 29900, 29900, 29901, 29871, 29900, 29900, 0, 0, 0, 0, 0, 0, 0), cumulative_logprob=0.0, get_num_computed_tokens=38}, sampling_params=SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=True, max_tokens=32, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), guided_decoding=None, block_tables={1: [2, 4, 9]}, do_sample=True, pooling_params=None, lora_request=None, computed_block_nums=[], state=SequenceGroupState(num_steps=1, current_step=0), multi_modal_data=None, encoder_seq_data=None, cross_block_table=None, prompt_adapter_request=None, token_chunk_size=1, num_speculative_tokens=None), SequenceGroupMetadata(request_id='2', is_prompt=False, seq_data={2: SequenceData(prompt_token_ids=array('l', [1, 450, 6673, 310, 278, 3303, 3900, 338, 263, 767, 29892, 825, 29879, 670, 1024, 29973]), output_token_ids=(13, 1576, 7178, 310, 278, 3303, 3900, 29892, 278, 7178, 310, 278, 3303, 3900, 0, 0, 0, 0, 0, 450, 29871, 29906, 29900, 29896, 29929, 29899, 29906), cumulative_logprob=0.0, get_num_computed_tokens=43}, sampling_params=SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=True, max_tokens=32, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), guided_decoding=None, block_tables={2: [3, 7, 8]}, do_sample=True, pooling_params=None, lora_request=None, computed_block_nums=[], state=SequenceGroupState(num_steps=1, current_step=0), multi_modal_data=None, encoder_seq_data=None, cross_block_table=None, prompt_adapter_request=None, token_chunk_size=1, num_speculative_tokens=None), SequenceGroupMetadata(request_id='3', is_prompt=False, seq_data={3: SequenceData(prompt_token_ids=array('l', [1, 450, 5434, 310, 319, 29902, 338, 29892, 297, 596, 1914, 3838, 29901, 29871]), output_token_ids=(29906, 29900, 29896, 29947, 29892, 29871, 29906, 29900, 29896, 29929, 29889), cumulative_logprob=0.0, get_num_computed_tokens=25}, sampling_params=SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=True, max_tokens=32, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), guided_decoding=None, block_tables={3: [6, 5]}, do_sample=True, pooling_params=None, lora_request=None, computed_block_nums=[], state=SequenceGroupState(num_steps=1, current_step=0), multi_modal_data=None, encoder_seq_data=None, cross_block_table=None, prompt_adapter_request=None, token_chunk_size=1, num_speculative_tokens=None)]
SCORING ON THE UNION OF REQUESTS (includes expanded batch), is prompt? [False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False]
SCORING ON THE UNION OF REQUESTS (includes expanded batch), do_sample? [True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True]
Proposal len X do_sample X is_prompt [(5, (True, False)), (5, (True, False)), (5, (True, False))]
Target probs distr tensor(0., device='cuda:0') tensor(3.1250e-05, device='cuda:0') tensor(1., device='cuda:0') torch.Size([3, 6, 32000])
Draft probs distr tensor(0., device='cuda:0') tensor(3.1250e-05, device='cuda:0') tensor(1., device='cuda:0') torch.Size([3, 5, 32000])
VERIFY STEP ENDED tensor([[    0,    -1,    -1,    -1,    -1,    -1],
        [29900, 29906,    -1,    -1,    -1,    -1],
        [   13,  1576,    -1,    -1,    -1,    -1]], device='cuda:0')
SAMPLER OUTPUT LIST 2x3
LLM STep outputs [SamplerOutput(outputs=[CompletionSequenceGroupOutput(samples=[SequenceOutput(parent_seq_id=1, output_token=0, logprobs={0: Logprob(logprob=0.0, rank=-1, decoded_token=None)})], prompt_logprobs=None), CompletionSequenceGroupOutput(samples=[SequenceOutput(parent_seq_id=2, output_token=29900, logprobs={29900: Logprob(logprob=0.0, rank=-1, decoded_token=None)})], prompt_logprobs=None), CompletionSequenceGroupOutput(samples=[SequenceOutput(parent_seq_id=3, output_token=13, logprobs={13: Logprob(logprob=0.0, rank=-1, decoded_token=None)})], prompt_logprobs=None)], sampled_token_probs=None, sampled_token_ids=None, spec_decode_worker_metrics=None), SamplerOutput(outputs=[CompletionSequenceGroupOutput(samples=[SequenceOutput(parent_seq_id=1, output_token=-1, logprobs={-1: Logprob(logprob=0.0, rank=-1, decoded_token=None)})], prompt_logprobs=None), CompletionSequenceGroupOutput(samples=[SequenceOutput(parent_seq_id=2, output_token=29906, logprobs={29906: Logprob(logprob=0.0, rank=-1, decoded_token=None)})], prompt_logprobs=None), CompletionSequenceGroupOutput(samples=[SequenceOutput(parent_seq_id=3, output_token=1576, logprobs={1576: Logprob(logprob=0.0, rank=-1, decoded_token=None)})], prompt_logprobs=None)], sampled_token_probs=None, sampled_token_ids=None, spec_decode_worker_metrics=None)]
LLM process output indices range(0, 3)
OUTPUTS PER SEQ GROUP (ONE IS PROMPT THE OTHER NOT) [[CompletionSequenceGroupOutput(samples=[SequenceOutput(parent_seq_id=1, output_token=0, logprobs={0: Logprob(logprob=0.0, rank=-1, decoded_token=None)})], prompt_logprobs=None), CompletionSequenceGroupOutput(samples=[SequenceOutput(parent_seq_id=1, output_token=-1, logprobs={-1: Logprob(logprob=0.0, rank=-1, decoded_token=None)})], prompt_logprobs=None)], [CompletionSequenceGroupOutput(samples=[SequenceOutput(parent_seq_id=2, output_token=29900, logprobs={29900: Logprob(logprob=0.0, rank=-1, decoded_token=None)})], prompt_logprobs=None), CompletionSequenceGroupOutput(samples=[SequenceOutput(parent_seq_id=2, output_token=29906, logprobs={29906: Logprob(logprob=0.0, rank=-1, decoded_token=None)})], prompt_logprobs=None)], [CompletionSequenceGroupOutput(samples=[SequenceOutput(parent_seq_id=3, output_token=13, logprobs={13: Logprob(logprob=0.0, rank=-1, decoded_token=None)})], prompt_logprobs=None), CompletionSequenceGroupOutput(samples=[SequenceOutput(parent_seq_id=3, output_token=1576, logprobs={1576: Logprob(logprob=0.0, rank=-1, decoded_token=None)})], prompt_logprobs=None)]]
output has multiples True 2
SEQ GROUP DIO CANE SHOULD SAMPLE HERE True
SequenceGroup(request_id=1, sampling_params=SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=True, max_tokens=32, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), guided_decoding=None, num_seqs=1)
output has multiples True 2
SEQ GROUP DIO CANE SHOULD SAMPLE HERE True
SequenceGroup(request_id=2, sampling_params=SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=True, max_tokens=32, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), guided_decoding=None, num_seqs=1)
output has multiples True 2
SEQ GROUP DIO CANE SHOULD SAMPLE HERE True
SequenceGroup(request_id=3, sampling_params=SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=True, max_tokens=32, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), guided_decoding=None, num_seqs=1)
PROMPTS (is prompt, K, token_chunk, do_sample):  [(False, None, 1, True), (False, None, 1, True), (False, None, 1, True)]
SPECULATIING WITH REQ METADATA [SequenceGroupMetadata(request_id='1', is_prompt=False, seq_data={1: SequenceData(prompt_token_ids=array('l', [1, 450, 5434, 310, 319, 29902, 338, 29892, 297, 596, 1914, 3838, 29901, 29871]), output_token_ids=(29896, 29900, 29901, 450, 29877, 29889, 29884, 29889, 29879, 29889, 13, 29900, 29900, 29901, 29871, 29900, 29900, 0, 0, 0, 0, 0, 0, 0, 0), cumulative_logprob=0.0, get_num_computed_tokens=39}, sampling_params=SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=True, max_tokens=32, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), guided_decoding=None, block_tables={1: [2, 4, 9]}, do_sample=True, pooling_params=None, lora_request=None, computed_block_nums=[], state=SequenceGroupState(num_steps=1, current_step=0), multi_modal_data=None, encoder_seq_data=None, cross_block_table=None, prompt_adapter_request=None, token_chunk_size=1, num_speculative_tokens=None), SequenceGroupMetadata(request_id='2', is_prompt=False, seq_data={2: SequenceData(prompt_token_ids=array('l', [1, 450, 6673, 310, 278, 3303, 3900, 338, 263, 767, 29892, 825, 29879, 670, 1024, 29973]), output_token_ids=(13, 1576, 7178, 310, 278, 3303, 3900, 29892, 278, 7178, 310, 278, 3303, 3900, 0, 0, 0, 0, 0, 450, 29871, 29906, 29900, 29896, 29929, 29899, 29906, 29900, 29906), cumulative_logprob=0.0, get_num_computed_tokens=45}, sampling_params=SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=True, max_tokens=32, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), guided_decoding=None, block_tables={2: [3, 7, 8, 1]}, do_sample=True, pooling_params=None, lora_request=None, computed_block_nums=[], state=SequenceGroupState(num_steps=1, current_step=0), multi_modal_data=None, encoder_seq_data=None, cross_block_table=None, prompt_adapter_request=None, token_chunk_size=1, num_speculative_tokens=None), SequenceGroupMetadata(request_id='3', is_prompt=False, seq_data={3: SequenceData(prompt_token_ids=array('l', [1, 450, 5434, 310, 319, 29902, 338, 29892, 297, 596, 1914, 3838, 29901, 29871]), output_token_ids=(29906, 29900, 29896, 29947, 29892, 29871, 29906, 29900, 29896, 29929, 29889, 13, 1576), cumulative_logprob=0.0, get_num_computed_tokens=27}, sampling_params=SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=True, max_tokens=32, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), guided_decoding=None, block_tables={3: [6, 5]}, do_sample=True, pooling_params=None, lora_request=None, computed_block_nums=[], state=SequenceGroupState(num_steps=1, current_step=0), multi_modal_data=None, encoder_seq_data=None, cross_block_table=None, prompt_adapter_request=None, token_chunk_size=1, num_speculative_tokens=None)]
SCORING ON THE UNION OF REQUESTS (includes expanded batch), is prompt? [False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False]
SCORING ON THE UNION OF REQUESTS (includes expanded batch), do_sample? [True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True]
Proposal len X do_sample X is_prompt [(5, (True, False)), (5, (True, False)), (5, (True, False))]
Target probs distr tensor(0., device='cuda:0') tensor(3.1250e-05, device='cuda:0') tensor(1., device='cuda:0') torch.Size([3, 6, 32000])
Draft probs distr tensor(0., device='cuda:0') tensor(3.1250e-05, device='cuda:0') tensor(1., device='cuda:0') torch.Size([3, 5, 32000])
VERIFY STEP ENDED tensor([[    0,    -1,    -1,    -1,    -1,    -1],
        [29900,  3762,    -1,    -1,    -1,    -1],
        [ 3186,    -1,    -1,    -1,    -1,    -1]], device='cuda:0')
SAMPLER OUTPUT LIST 2x3
LLM STep outputs [SamplerOutput(outputs=[CompletionSequenceGroupOutput(samples=[SequenceOutput(parent_seq_id=1, output_token=0, logprobs={0: Logprob(logprob=0.0, rank=-1, decoded_token=None)})], prompt_logprobs=None), CompletionSequenceGroupOutput(samples=[SequenceOutput(parent_seq_id=2, output_token=29900, logprobs={29900: Logprob(logprob=0.0, rank=-1, decoded_token=None)})], prompt_logprobs=None), CompletionSequenceGroupOutput(samples=[SequenceOutput(parent_seq_id=3, output_token=3186, logprobs={3186: Logprob(logprob=0.0, rank=-1, decoded_token=None)})], prompt_logprobs=None)], sampled_token_probs=None, sampled_token_ids=None, spec_decode_worker_metrics=None), SamplerOutput(outputs=[CompletionSequenceGroupOutput(samples=[SequenceOutput(parent_seq_id=1, output_token=-1, logprobs={-1: Logprob(logprob=0.0, rank=-1, decoded_token=None)})], prompt_logprobs=None), CompletionSequenceGroupOutput(samples=[SequenceOutput(parent_seq_id=2, output_token=3762, logprobs={3762: Logprob(logprob=0.0, rank=-1, decoded_token=None)})], prompt_logprobs=None), CompletionSequenceGroupOutput(samples=[SequenceOutput(parent_seq_id=3, output_token=-1, logprobs={-1: Logprob(logprob=0.0, rank=-1, decoded_token=None)})], prompt_logprobs=None)], sampled_token_probs=None, sampled_token_ids=None, spec_decode_worker_metrics=None)]
LLM process output indices range(0, 3)
OUTPUTS PER SEQ GROUP (ONE IS PROMPT THE OTHER NOT) [[CompletionSequenceGroupOutput(samples=[SequenceOutput(parent_seq_id=1, output_token=0, logprobs={0: Logprob(logprob=0.0, rank=-1, decoded_token=None)})], prompt_logprobs=None), CompletionSequenceGroupOutput(samples=[SequenceOutput(parent_seq_id=1, output_token=-1, logprobs={-1: Logprob(logprob=0.0, rank=-1, decoded_token=None)})], prompt_logprobs=None)], [CompletionSequenceGroupOutput(samples=[SequenceOutput(parent_seq_id=2, output_token=29900, logprobs={29900: Logprob(logprob=0.0, rank=-1, decoded_token=None)})], prompt_logprobs=None), CompletionSequenceGroupOutput(samples=[SequenceOutput(parent_seq_id=2, output_token=3762, logprobs={3762: Logprob(logprob=0.0, rank=-1, decoded_token=None)})], prompt_logprobs=None)], [CompletionSequenceGroupOutput(samples=[SequenceOutput(parent_seq_id=3, output_token=3186, logprobs={3186: Logprob(logprob=0.0, rank=-1, decoded_token=None)})], prompt_logprobs=None), CompletionSequenceGroupOutput(samples=[SequenceOutput(parent_seq_id=3, output_token=-1, logprobs={-1: Logprob(logprob=0.0, rank=-1, decoded_token=None)})], prompt_logprobs=None)]]
output has multiples True 2
SEQ GROUP DIO CANE SHOULD SAMPLE HERE True
SequenceGroup(request_id=1, sampling_params=SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=True, max_tokens=32, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), guided_decoding=None, num_seqs=1)
output has multiples True 2
SEQ GROUP DIO CANE SHOULD SAMPLE HERE True
SequenceGroup(request_id=2, sampling_params=SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=True, max_tokens=32, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), guided_decoding=None, num_seqs=1)
output has multiples True 2
SEQ GROUP DIO CANE SHOULD SAMPLE HERE True
SequenceGroup(request_id=3, sampling_params=SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=True, max_tokens=32, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), guided_decoding=None, num_seqs=1)
PROMPTS (is prompt, K, token_chunk, do_sample):  [(False, None, 1, True), (False, None, 1, True), (False, None, 1, True)]
SPECULATIING WITH REQ METADATA [SequenceGroupMetadata(request_id='1', is_prompt=False, seq_data={1: SequenceData(prompt_token_ids=array('l', [1, 450, 5434, 310, 319, 29902, 338, 29892, 297, 596, 1914, 3838, 29901, 29871]), output_token_ids=(29896, 29900, 29901, 450, 29877, 29889, 29884, 29889, 29879, 29889, 13, 29900, 29900, 29901, 29871, 29900, 29900, 0, 0, 0, 0, 0, 0, 0, 0, 0), cumulative_logprob=0.0, get_num_computed_tokens=40}, sampling_params=SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=True, max_tokens=32, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), guided_decoding=None, block_tables={1: [2, 4, 9]}, do_sample=True, pooling_params=None, lora_request=None, computed_block_nums=[], state=SequenceGroupState(num_steps=1, current_step=0), multi_modal_data=None, encoder_seq_data=None, cross_block_table=None, prompt_adapter_request=None, token_chunk_size=1, num_speculative_tokens=None), SequenceGroupMetadata(request_id='2', is_prompt=False, seq_data={2: SequenceData(prompt_token_ids=array('l', [1, 450, 6673, 310, 278, 3303, 3900, 338, 263, 767, 29892, 825, 29879, 670, 1024, 29973]), output_token_ids=(13, 1576, 7178, 310, 278, 3303, 3900, 29892, 278, 7178, 310, 278, 3303, 3900, 0, 0, 0, 0, 0, 450, 29871, 29906, 29900, 29896, 29929, 29899, 29906, 29900, 29906, 29900, 3762), cumulative_logprob=0.0, get_num_computed_tokens=47}, sampling_params=SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=True, max_tokens=32, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), guided_decoding=None, block_tables={2: [3, 7, 8, 1]}, do_sample=True, pooling_params=None, lora_request=None, computed_block_nums=[], state=SequenceGroupState(num_steps=1, current_step=0), multi_modal_data=None, encoder_seq_data=None, cross_block_table=None, prompt_adapter_request=None, token_chunk_size=1, num_speculative_tokens=None), SequenceGroupMetadata(request_id='3', is_prompt=False, seq_data={3: SequenceData(prompt_token_ids=array('l', [1, 450, 5434, 310, 319, 29902, 338, 29892, 297, 596, 1914, 3838, 29901, 29871]), output_token_ids=(29906, 29900, 29896, 29947, 29892, 29871, 29906, 29900, 29896, 29929, 29889, 13, 1576, 3186), cumulative_logprob=0.0, get_num_computed_tokens=28}, sampling_params=SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=True, max_tokens=32, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), guided_decoding=None, block_tables={3: [6, 5, 0]}, do_sample=True, pooling_params=None, lora_request=None, computed_block_nums=[], state=SequenceGroupState(num_steps=1, current_step=0), multi_modal_data=None, encoder_seq_data=None, cross_block_table=None, prompt_adapter_request=None, token_chunk_size=1, num_speculative_tokens=None)]
SCORING ON THE UNION OF REQUESTS (includes expanded batch), is prompt? [False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False]
SCORING ON THE UNION OF REQUESTS (includes expanded batch), do_sample? [True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True]
Proposal len X do_sample X is_prompt [(5, (True, False)), (5, (True, False)), (5, (True, False))]
Target probs distr tensor(0., device='cuda:0') tensor(3.1250e-05, device='cuda:0') tensor(1., device='cuda:0') torch.Size([3, 6, 32000])
Draft probs distr tensor(0., device='cuda:0') tensor(3.1250e-05, device='cuda:0') tensor(1., device='cuda:0') torch.Size([3, 5, 32000])
VERIFY STEP ENDED tensor([[   0,   -1,   -1,   -1,   -1,   -1],
        [1629,  338,  263,  931,   -1,   -1],
        [ 338,  297,   -1,   -1,   -1,   -1]], device='cuda:0')
SAMPLER OUTPUT LIST 4x3
LLM STep outputs [SamplerOutput(outputs=[CompletionSequenceGroupOutput(samples=[SequenceOutput(parent_seq_id=1, output_token=0, logprobs={0: Logprob(logprob=0.0, rank=-1, decoded_token=None)})], prompt_logprobs=None), CompletionSequenceGroupOutput(samples=[SequenceOutput(parent_seq_id=2, output_token=1629, logprobs={1629: Logprob(logprob=0.0, rank=-1, decoded_token=None)})], prompt_logprobs=None), CompletionSequenceGroupOutput(samples=[SequenceOutput(parent_seq_id=3, output_token=338, logprobs={338: Logprob(logprob=0.0, rank=-1, decoded_token=None)})], prompt_logprobs=None)], sampled_token_probs=None, sampled_token_ids=None, spec_decode_worker_metrics=None), SamplerOutput(outputs=[CompletionSequenceGroupOutput(samples=[SequenceOutput(parent_seq_id=1, output_token=-1, logprobs={-1: Logprob(logprob=0.0, rank=-1, decoded_token=None)})], prompt_logprobs=None), CompletionSequenceGroupOutput(samples=[SequenceOutput(parent_seq_id=2, output_token=338, logprobs={338: Logprob(logprob=0.0, rank=-1, decoded_token=None)})], prompt_logprobs=None), CompletionSequenceGroupOutput(samples=[SequenceOutput(parent_seq_id=3, output_token=297, logprobs={297: Logprob(logprob=0.0, rank=-1, decoded_token=None)})], prompt_logprobs=None)], sampled_token_probs=None, sampled_token_ids=None, spec_decode_worker_metrics=None), SamplerOutput(outputs=[CompletionSequenceGroupOutput(samples=[SequenceOutput(parent_seq_id=1, output_token=-1, logprobs={-1: Logprob(logprob=0.0, rank=-1, decoded_token=None)})], prompt_logprobs=None), CompletionSequenceGroupOutput(samples=[SequenceOutput(parent_seq_id=2, output_token=263, logprobs={263: Logprob(logprob=0.0, rank=-1, decoded_token=None)})], prompt_logprobs=None), CompletionSequenceGroupOutput(samples=[SequenceOutput(parent_seq_id=3, output_token=-1, logprobs={-1: Logprob(logprob=0.0, rank=-1, decoded_token=None)})], prompt_logprobs=None)], sampled_token_probs=None, sampled_token_ids=None, spec_decode_worker_metrics=None), SamplerOutput(outputs=[CompletionSequenceGroupOutput(samples=[SequenceOutput(parent_seq_id=1, output_token=-1, logprobs={-1: Logprob(logprob=0.0, rank=-1, decoded_token=None)})], prompt_logprobs=None), CompletionSequenceGroupOutput(samples=[SequenceOutput(parent_seq_id=2, output_token=931, logprobs={931: Logprob(logprob=0.0, rank=-1, decoded_token=None)})], prompt_logprobs=None), CompletionSequenceGroupOutput(samples=[SequenceOutput(parent_seq_id=3, output_token=-1, logprobs={-1: Logprob(logprob=0.0, rank=-1, decoded_token=None)})], prompt_logprobs=None)], sampled_token_probs=None, sampled_token_ids=None, spec_decode_worker_metrics=None)]
LLM process output indices range(0, 3)
OUTPUTS PER SEQ GROUP (ONE IS PROMPT THE OTHER NOT) [[CompletionSequenceGroupOutput(samples=[SequenceOutput(parent_seq_id=1, output_token=0, logprobs={0: Logprob(logprob=0.0, rank=-1, decoded_token=None)})], prompt_logprobs=None), CompletionSequenceGroupOutput(samples=[SequenceOutput(parent_seq_id=1, output_token=-1, logprobs={-1: Logprob(logprob=0.0, rank=-1, decoded_token=None)})], prompt_logprobs=None), CompletionSequenceGroupOutput(samples=[SequenceOutput(parent_seq_id=1, output_token=-1, logprobs={-1: Logprob(logprob=0.0, rank=-1, decoded_token=None)})], prompt_logprobs=None), CompletionSequenceGroupOutput(samples=[SequenceOutput(parent_seq_id=1, output_token=-1, logprobs={-1: Logprob(logprob=0.0, rank=-1, decoded_token=None)})], prompt_logprobs=None)], [CompletionSequenceGroupOutput(samples=[SequenceOutput(parent_seq_id=2, output_token=1629, logprobs={1629: Logprob(logprob=0.0, rank=-1, decoded_token=None)})], prompt_logprobs=None), CompletionSequenceGroupOutput(samples=[SequenceOutput(parent_seq_id=2, output_token=338, logprobs={338: Logprob(logprob=0.0, rank=-1, decoded_token=None)})], prompt_logprobs=None), CompletionSequenceGroupOutput(samples=[SequenceOutput(parent_seq_id=2, output_token=263, logprobs={263: Logprob(logprob=0.0, rank=-1, decoded_token=None)})], prompt_logprobs=None), CompletionSequenceGroupOutput(samples=[SequenceOutput(parent_seq_id=2, output_token=931, logprobs={931: Logprob(logprob=0.0, rank=-1, decoded_token=None)})], prompt_logprobs=None)], [CompletionSequenceGroupOutput(samples=[SequenceOutput(parent_seq_id=3, output_token=338, logprobs={338: Logprob(logprob=0.0, rank=-1, decoded_token=None)})], prompt_logprobs=None), CompletionSequenceGroupOutput(samples=[SequenceOutput(parent_seq_id=3, output_token=297, logprobs={297: Logprob(logprob=0.0, rank=-1, decoded_token=None)})], prompt_logprobs=None), CompletionSequenceGroupOutput(samples=[SequenceOutput(parent_seq_id=3, output_token=-1, logprobs={-1: Logprob(logprob=0.0, rank=-1, decoded_token=None)})], prompt_logprobs=None), CompletionSequenceGroupOutput(samples=[SequenceOutput(parent_seq_id=3, output_token=-1, logprobs={-1: Logprob(logprob=0.0, rank=-1, decoded_token=None)})], prompt_logprobs=None)]]
output has multiples True 4
SEQ GROUP DIO CANE SHOULD SAMPLE HERE True
SequenceGroup(request_id=1, sampling_params=SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=True, max_tokens=32, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), guided_decoding=None, num_seqs=1)
output has multiples True 4
SEQ GROUP DIO CANE SHOULD SAMPLE HERE True
SequenceGroup(request_id=2, sampling_params=SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=True, max_tokens=32, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), guided_decoding=None, num_seqs=1)
output has multiples True 4
SEQ GROUP DIO CANE SHOULD SAMPLE HERE True
SequenceGroup(request_id=3, sampling_params=SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=True, max_tokens=32, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), guided_decoding=None, num_seqs=1)
PROMPTS (is prompt, K, token_chunk, do_sample):  [(False, None, 1, True), (False, None, 1, True)]
SPECULATIING WITH REQ METADATA [SequenceGroupMetadata(request_id='1', is_prompt=False, seq_data={1: SequenceData(prompt_token_ids=array('l', [1, 450, 5434, 310, 319, 29902, 338, 29892, 297, 596, 1914, 3838, 29901, 29871]), output_token_ids=(29896, 29900, 29901, 450, 29877, 29889, 29884, 29889, 29879, 29889, 13, 29900, 29900, 29901, 29871, 29900, 29900, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0), cumulative_logprob=0.0, get_num_computed_tokens=41}, sampling_params=SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=True, max_tokens=32, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), guided_decoding=None, block_tables={1: [2, 4, 9]}, do_sample=True, pooling_params=None, lora_request=None, computed_block_nums=[], state=SequenceGroupState(num_steps=1, current_step=0), multi_modal_data=None, encoder_seq_data=None, cross_block_table=None, prompt_adapter_request=None, token_chunk_size=1, num_speculative_tokens=None), SequenceGroupMetadata(request_id='3', is_prompt=False, seq_data={3: SequenceData(prompt_token_ids=array('l', [1, 450, 5434, 310, 319, 29902, 338, 29892, 297, 596, 1914, 3838, 29901, 29871]), output_token_ids=(29906, 29900, 29896, 29947, 29892, 29871, 29906, 29900, 29896, 29929, 29889, 13, 1576, 3186, 338, 297), cumulative_logprob=0.0, get_num_computed_tokens=30}, sampling_params=SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=True, max_tokens=32, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), guided_decoding=None, block_tables={3: [6, 5, 0]}, do_sample=True, pooling_params=None, lora_request=None, computed_block_nums=[], state=SequenceGroupState(num_steps=1, current_step=0), multi_modal_data=None, encoder_seq_data=None, cross_block_table=None, prompt_adapter_request=None, token_chunk_size=1, num_speculative_tokens=None)]
SCORING ON THE UNION OF REQUESTS (includes expanded batch), is prompt? [False, False, False, False, False, False, False, False, False, False, False, False]
SCORING ON THE UNION OF REQUESTS (includes expanded batch), do_sample? [True, True, True, True, True, True, True, True, True, True, True, True]
Proposal len X do_sample X is_prompt [(5, (True, False)), (5, (True, False))]
Target probs distr tensor(0., device='cuda:0') tensor(3.1250e-05, device='cuda:0') tensor(1., device='cuda:0') torch.Size([2, 6, 32000])
Draft probs distr tensor(0., device='cuda:0') tensor(3.1250e-05, device='cuda:0') tensor(1., device='cuda:0') torch.Size([2, 5, 32000])
VERIFY STEP ENDED tensor([[  0,  -1,  -1,  -1,  -1,  -1],
        [278,  -1,  -1,  -1,  -1,  -1]], device='cuda:0')
SAMPLER OUTPUT LIST 1x2
LLM STep outputs [SamplerOutput(outputs=[CompletionSequenceGroupOutput(samples=[SequenceOutput(parent_seq_id=1, output_token=0, logprobs={0: Logprob(logprob=0.0, rank=-1, decoded_token=None)})], prompt_logprobs=None), CompletionSequenceGroupOutput(samples=[SequenceOutput(parent_seq_id=3, output_token=278, logprobs={278: Logprob(logprob=0.0, rank=-1, decoded_token=None)})], prompt_logprobs=None)], sampled_token_probs=None, sampled_token_ids=None, spec_decode_worker_metrics=None)]
LLM process output indices range(0, 2)
OUTPUTS PER SEQ GROUP (ONE IS PROMPT THE OTHER NOT) [SamplerOutput(outputs=[CompletionSequenceGroupOutput(samples=[SequenceOutput(parent_seq_id=1, output_token=0, logprobs={0: Logprob(logprob=0.0, rank=-1, decoded_token=None)})], prompt_logprobs=None), CompletionSequenceGroupOutput(samples=[SequenceOutput(parent_seq_id=3, output_token=278, logprobs={278: Logprob(logprob=0.0, rank=-1, decoded_token=None)})], prompt_logprobs=None)], sampled_token_probs=None, sampled_token_ids=None, spec_decode_worker_metrics=None)]
output has multiples False 1
SEQ GROUP DIO CANE SHOULD SAMPLE HERE True
SequenceGroup(request_id=1, sampling_params=SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=True, max_tokens=32, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), guided_decoding=None, num_seqs=1)
output has multiples False 1
SEQ GROUP DIO CANE SHOULD SAMPLE HERE True
SequenceGroup(request_id=3, sampling_params=SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=True, max_tokens=32, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), guided_decoding=None, num_seqs=1)
PROMPTS (is prompt, K, token_chunk, do_sample):  [(False, None, 1, True), (False, None, 1, True)]
SPECULATIING WITH REQ METADATA [SequenceGroupMetadata(request_id='1', is_prompt=False, seq_data={1: SequenceData(prompt_token_ids=array('l', [1, 450, 5434, 310, 319, 29902, 338, 29892, 297, 596, 1914, 3838, 29901, 29871]), output_token_ids=(29896, 29900, 29901, 450, 29877, 29889, 29884, 29889, 29879, 29889, 13, 29900, 29900, 29901, 29871, 29900, 29900, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0), cumulative_logprob=0.0, get_num_computed_tokens=42}, sampling_params=SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=True, max_tokens=32, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), guided_decoding=None, block_tables={1: [2, 4, 9]}, do_sample=True, pooling_params=None, lora_request=None, computed_block_nums=[], state=SequenceGroupState(num_steps=1, current_step=0), multi_modal_data=None, encoder_seq_data=None, cross_block_table=None, prompt_adapter_request=None, token_chunk_size=1, num_speculative_tokens=None), SequenceGroupMetadata(request_id='3', is_prompt=False, seq_data={3: SequenceData(prompt_token_ids=array('l', [1, 450, 5434, 310, 319, 29902, 338, 29892, 297, 596, 1914, 3838, 29901, 29871]), output_token_ids=(29906, 29900, 29896, 29947, 29892, 29871, 29906, 29900, 29896, 29929, 29889, 13, 1576, 3186, 338, 297, 278), cumulative_logprob=0.0, get_num_computed_tokens=31}, sampling_params=SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=True, max_tokens=32, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), guided_decoding=None, block_tables={3: [6, 5, 0]}, do_sample=True, pooling_params=None, lora_request=None, computed_block_nums=[], state=SequenceGroupState(num_steps=1, current_step=0), multi_modal_data=None, encoder_seq_data=None, cross_block_table=None, prompt_adapter_request=None, token_chunk_size=1, num_speculative_tokens=None)]
SCORING ON THE UNION OF REQUESTS (includes expanded batch), is prompt? [False, False, False, False, False, False, False, False, False, False, False, False]
SCORING ON THE UNION OF REQUESTS (includes expanded batch), do_sample? [True, True, True, True, True, True, True, True, True, True, True, True]
Proposal len X do_sample X is_prompt [(5, (True, False)), (5, (True, False))]
Target probs distr tensor(0., device='cuda:0') tensor(3.1250e-05, device='cuda:0') tensor(1., device='cuda:0') torch.Size([2, 6, 32000])
Draft probs distr tensor(0., device='cuda:0') tensor(3.1250e-05, device='cuda:0') tensor(1., device='cuda:0') torch.Size([2, 5, 32000])
VERIFY STEP ENDED tensor([[    0,    -1,    -1,    -1,    -1,    -1],
        [29214,   310,   263, 19479, 29889,   450]], device='cuda:0')
SAMPLER OUTPUT LIST 6x2
LLM STep outputs [SamplerOutput(outputs=[CompletionSequenceGroupOutput(samples=[SequenceOutput(parent_seq_id=1, output_token=0, logprobs={0: Logprob(logprob=0.0, rank=-1, decoded_token=None)})], prompt_logprobs=None), CompletionSequenceGroupOutput(samples=[SequenceOutput(parent_seq_id=3, output_token=29214, logprobs={29214: Logprob(logprob=0.0, rank=-1, decoded_token=None)})], prompt_logprobs=None)], sampled_token_probs=None, sampled_token_ids=None, spec_decode_worker_metrics=None), SamplerOutput(outputs=[CompletionSequenceGroupOutput(samples=[SequenceOutput(parent_seq_id=1, output_token=-1, logprobs={-1: Logprob(logprob=0.0, rank=-1, decoded_token=None)})], prompt_logprobs=None), CompletionSequenceGroupOutput(samples=[SequenceOutput(parent_seq_id=3, output_token=310, logprobs={310: Logprob(logprob=0.0, rank=-1, decoded_token=None)})], prompt_logprobs=None)], sampled_token_probs=None, sampled_token_ids=None, spec_decode_worker_metrics=None), SamplerOutput(outputs=[CompletionSequenceGroupOutput(samples=[SequenceOutput(parent_seq_id=1, output_token=-1, logprobs={-1: Logprob(logprob=0.0, rank=-1, decoded_token=None)})], prompt_logprobs=None), CompletionSequenceGroupOutput(samples=[SequenceOutput(parent_seq_id=3, output_token=263, logprobs={263: Logprob(logprob=0.0, rank=-1, decoded_token=None)})], prompt_logprobs=None)], sampled_token_probs=None, sampled_token_ids=None, spec_decode_worker_metrics=None), SamplerOutput(outputs=[CompletionSequenceGroupOutput(samples=[SequenceOutput(parent_seq_id=1, output_token=-1, logprobs={-1: Logprob(logprob=0.0, rank=-1, decoded_token=None)})], prompt_logprobs=None), CompletionSequenceGroupOutput(samples=[SequenceOutput(parent_seq_id=3, output_token=19479, logprobs={19479: Logprob(logprob=0.0, rank=-1, decoded_token=None)})], prompt_logprobs=None)], sampled_token_probs=None, sampled_token_ids=None, spec_decode_worker_metrics=None), SamplerOutput(outputs=[CompletionSequenceGroupOutput(samples=[SequenceOutput(parent_seq_id=1, output_token=-1, logprobs={-1: Logprob(logprob=0.0, rank=-1, decoded_token=None)})], prompt_logprobs=None), CompletionSequenceGroupOutput(samples=[SequenceOutput(parent_seq_id=3, output_token=29889, logprobs={29889: Logprob(logprob=0.0, rank=-1, decoded_token=None)})], prompt_logprobs=None)], sampled_token_probs=None, sampled_token_ids=None, spec_decode_worker_metrics=None), SamplerOutput(outputs=[CompletionSequenceGroupOutput(samples=[SequenceOutput(parent_seq_id=1, output_token=-1, logprobs={-1: Logprob(logprob=0.0, rank=-1, decoded_token=None)})], prompt_logprobs=None), CompletionSequenceGroupOutput(samples=[SequenceOutput(parent_seq_id=3, output_token=450, logprobs={450: Logprob(logprob=0.0, rank=-1, decoded_token=None)})], prompt_logprobs=None)], sampled_token_probs=None, sampled_token_ids=None, spec_decode_worker_metrics=None)]
LLM process output indices range(0, 2)
OUTPUTS PER SEQ GROUP (ONE IS PROMPT THE OTHER NOT) [[CompletionSequenceGroupOutput(samples=[SequenceOutput(parent_seq_id=1, output_token=0, logprobs={0: Logprob(logprob=0.0, rank=-1, decoded_token=None)})], prompt_logprobs=None), CompletionSequenceGroupOutput(samples=[SequenceOutput(parent_seq_id=1, output_token=-1, logprobs={-1: Logprob(logprob=0.0, rank=-1, decoded_token=None)})], prompt_logprobs=None), CompletionSequenceGroupOutput(samples=[SequenceOutput(parent_seq_id=1, output_token=-1, logprobs={-1: Logprob(logprob=0.0, rank=-1, decoded_token=None)})], prompt_logprobs=None), CompletionSequenceGroupOutput(samples=[SequenceOutput(parent_seq_id=1, output_token=-1, logprobs={-1: Logprob(logprob=0.0, rank=-1, decoded_token=None)})], prompt_logprobs=None), CompletionSequenceGroupOutput(samples=[SequenceOutput(parent_seq_id=1, output_token=-1, logprobs={-1: Logprob(logprob=0.0, rank=-1, decoded_token=None)})], prompt_logprobs=None), CompletionSequenceGroupOutput(samples=[SequenceOutput(parent_seq_id=1, output_token=-1, logprobs={-1: Logprob(logprob=0.0, rank=-1, decoded_token=None)})], prompt_logprobs=None)], [CompletionSequenceGroupOutput(samples=[SequenceOutput(parent_seq_id=3, output_token=29214, logprobs={29214: Logprob(logprob=0.0, rank=-1, decoded_token=None)})], prompt_logprobs=None), CompletionSequenceGroupOutput(samples=[SequenceOutput(parent_seq_id=3, output_token=310, logprobs={310: Logprob(logprob=0.0, rank=-1, decoded_token=None)})], prompt_logprobs=None), CompletionSequenceGroupOutput(samples=[SequenceOutput(parent_seq_id=3, output_token=263, logprobs={263: Logprob(logprob=0.0, rank=-1, decoded_token=None)})], prompt_logprobs=None), CompletionSequenceGroupOutput(samples=[SequenceOutput(parent_seq_id=3, output_token=19479, logprobs={19479: Logprob(logprob=0.0, rank=-1, decoded_token=None)})], prompt_logprobs=None), CompletionSequenceGroupOutput(samples=[SequenceOutput(parent_seq_id=3, output_token=29889, logprobs={29889: Logprob(logprob=0.0, rank=-1, decoded_token=None)})], prompt_logprobs=None), CompletionSequenceGroupOutput(samples=[SequenceOutput(parent_seq_id=3, output_token=450, logprobs={450: Logprob(logprob=0.0, rank=-1, decoded_token=None)})], prompt_logprobs=None)]]
output has multiples True 6
SEQ GROUP DIO CANE SHOULD SAMPLE HERE True
SequenceGroup(request_id=1, sampling_params=SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=True, max_tokens=32, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), guided_decoding=None, num_seqs=1)
output has multiples True 6
SEQ GROUP DIO CANE SHOULD SAMPLE HERE True
SequenceGroup(request_id=3, sampling_params=SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=True, max_tokens=32, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), guided_decoding=None, num_seqs=1)
PROMPTS (is prompt, K, token_chunk, do_sample):  [(False, None, 1, True), (False, None, 1, True)]
SPECULATIING WITH REQ METADATA [SequenceGroupMetadata(request_id='1', is_prompt=False, seq_data={1: SequenceData(prompt_token_ids=array('l', [1, 450, 5434, 310, 319, 29902, 338, 29892, 297, 596, 1914, 3838, 29901, 29871]), output_token_ids=(29896, 29900, 29901, 450, 29877, 29889, 29884, 29889, 29879, 29889, 13, 29900, 29900, 29901, 29871, 29900, 29900, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0), cumulative_logprob=0.0, get_num_computed_tokens=43}, sampling_params=SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=True, max_tokens=32, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), guided_decoding=None, block_tables={1: [2, 4, 9]}, do_sample=True, pooling_params=None, lora_request=None, computed_block_nums=[], state=SequenceGroupState(num_steps=1, current_step=0), multi_modal_data=None, encoder_seq_data=None, cross_block_table=None, prompt_adapter_request=None, token_chunk_size=1, num_speculative_tokens=None), SequenceGroupMetadata(request_id='3', is_prompt=False, seq_data={3: SequenceData(prompt_token_ids=array('l', [1, 450, 5434, 310, 319, 29902, 338, 29892, 297, 596, 1914, 3838, 29901, 29871]), output_token_ids=(29906, 29900, 29896, 29947, 29892, 29871, 29906, 29900, 29896, 29929, 29889, 13, 1576, 3186, 338, 297, 278, 29214, 310, 263, 19479, 29889, 450), cumulative_logprob=0.0, get_num_computed_tokens=37}, sampling_params=SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=True, max_tokens=32, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), guided_decoding=None, block_tables={3: [6, 5, 0]}, do_sample=True, pooling_params=None, lora_request=None, computed_block_nums=[], state=SequenceGroupState(num_steps=1, current_step=0), multi_modal_data=None, encoder_seq_data=None, cross_block_table=None, prompt_adapter_request=None, token_chunk_size=1, num_speculative_tokens=None)]
SCORING ON THE UNION OF REQUESTS (includes expanded batch), is prompt? [False, False, False, False, False, False, False, False, False, False, False, False]
SCORING ON THE UNION OF REQUESTS (includes expanded batch), do_sample? [True, True, True, True, True, True, True, True, True, True, True, True]
Proposal len X do_sample X is_prompt [(5, (True, False)), (5, (True, False))]
Target probs distr tensor(0., device='cuda:0') tensor(3.1250e-05, device='cuda:0') tensor(1., device='cuda:0') torch.Size([2, 6, 32000])
Draft probs distr tensor(0., device='cuda:0') tensor(3.1250e-05, device='cuda:0') tensor(1., device='cuda:0') torch.Size([2, 5, 32000])
VERIFY STEP ENDED tensor([[    0,    -1,    -1,    -1,    -1,    -1],
        [ 3186,   338,   297,   278, 29214,   310]], device='cuda:0')
SAMPLER OUTPUT LIST 6x2
LLM STep outputs [SamplerOutput(outputs=[CompletionSequenceGroupOutput(samples=[SequenceOutput(parent_seq_id=1, output_token=0, logprobs={0: Logprob(logprob=0.0, rank=-1, decoded_token=None)})], prompt_logprobs=None), CompletionSequenceGroupOutput(samples=[SequenceOutput(parent_seq_id=3, output_token=3186, logprobs={3186: Logprob(logprob=0.0, rank=-1, decoded_token=None)})], prompt_logprobs=None)], sampled_token_probs=None, sampled_token_ids=None, spec_decode_worker_metrics=None), SamplerOutput(outputs=[CompletionSequenceGroupOutput(samples=[SequenceOutput(parent_seq_id=1, output_token=-1, logprobs={-1: Logprob(logprob=0.0, rank=-1, decoded_token=None)})], prompt_logprobs=None), CompletionSequenceGroupOutput(samples=[SequenceOutput(parent_seq_id=3, output_token=338, logprobs={338: Logprob(logprob=0.0, rank=-1, decoded_token=None)})], prompt_logprobs=None)], sampled_token_probs=None, sampled_token_ids=None, spec_decode_worker_metrics=None), SamplerOutput(outputs=[CompletionSequenceGroupOutput(samples=[SequenceOutput(parent_seq_id=1, output_token=-1, logprobs={-1: Logprob(logprob=0.0, rank=-1, decoded_token=None)})], prompt_logprobs=None), CompletionSequenceGroupOutput(samples=[SequenceOutput(parent_seq_id=3, output_token=297, logprobs={297: Logprob(logprob=0.0, rank=-1, decoded_token=None)})], prompt_logprobs=None)], sampled_token_probs=None, sampled_token_ids=None, spec_decode_worker_metrics=None), SamplerOutput(outputs=[CompletionSequenceGroupOutput(samples=[SequenceOutput(parent_seq_id=1, output_token=-1, logprobs={-1: Logprob(logprob=0.0, rank=-1, decoded_token=None)})], prompt_logprobs=None), CompletionSequenceGroupOutput(samples=[SequenceOutput(parent_seq_id=3, output_token=278, logprobs={278: Logprob(logprob=0.0, rank=-1, decoded_token=None)})], prompt_logprobs=None)], sampled_token_probs=None, sampled_token_ids=None, spec_decode_worker_metrics=None), SamplerOutput(outputs=[CompletionSequenceGroupOutput(samples=[SequenceOutput(parent_seq_id=1, output_token=-1, logprobs={-1: Logprob(logprob=0.0, rank=-1, decoded_token=None)})], prompt_logprobs=None), CompletionSequenceGroupOutput(samples=[SequenceOutput(parent_seq_id=3, output_token=29214, logprobs={29214: Logprob(logprob=0.0, rank=-1, decoded_token=None)})], prompt_logprobs=None)], sampled_token_probs=None, sampled_token_ids=None, spec_decode_worker_metrics=None), SamplerOutput(outputs=[CompletionSequenceGroupOutput(samples=[SequenceOutput(parent_seq_id=1, output_token=-1, logprobs={-1: Logprob(logprob=0.0, rank=-1, decoded_token=None)})], prompt_logprobs=None), CompletionSequenceGroupOutput(samples=[SequenceOutput(parent_seq_id=3, output_token=310, logprobs={310: Logprob(logprob=0.0, rank=-1, decoded_token=None)})], prompt_logprobs=None)], sampled_token_probs=None, sampled_token_ids=None, spec_decode_worker_metrics=None)]
LLM process output indices range(0, 2)
OUTPUTS PER SEQ GROUP (ONE IS PROMPT THE OTHER NOT) [[CompletionSequenceGroupOutput(samples=[SequenceOutput(parent_seq_id=1, output_token=0, logprobs={0: Logprob(logprob=0.0, rank=-1, decoded_token=None)})], prompt_logprobs=None), CompletionSequenceGroupOutput(samples=[SequenceOutput(parent_seq_id=1, output_token=-1, logprobs={-1: Logprob(logprob=0.0, rank=-1, decoded_token=None)})], prompt_logprobs=None), CompletionSequenceGroupOutput(samples=[SequenceOutput(parent_seq_id=1, output_token=-1, logprobs={-1: Logprob(logprob=0.0, rank=-1, decoded_token=None)})], prompt_logprobs=None), CompletionSequenceGroupOutput(samples=[SequenceOutput(parent_seq_id=1, output_token=-1, logprobs={-1: Logprob(logprob=0.0, rank=-1, decoded_token=None)})], prompt_logprobs=None), CompletionSequenceGroupOutput(samples=[SequenceOutput(parent_seq_id=1, output_token=-1, logprobs={-1: Logprob(logprob=0.0, rank=-1, decoded_token=None)})], prompt_logprobs=None), CompletionSequenceGroupOutput(samples=[SequenceOutput(parent_seq_id=1, output_token=-1, logprobs={-1: Logprob(logprob=0.0, rank=-1, decoded_token=None)})], prompt_logprobs=None)], [CompletionSequenceGroupOutput(samples=[SequenceOutput(parent_seq_id=3, output_token=3186, logprobs={3186: Logprob(logprob=0.0, rank=-1, decoded_token=None)})], prompt_logprobs=None), CompletionSequenceGroupOutput(samples=[SequenceOutput(parent_seq_id=3, output_token=338, logprobs={338: Logprob(logprob=0.0, rank=-1, decoded_token=None)})], prompt_logprobs=None), CompletionSequenceGroupOutput(samples=[SequenceOutput(parent_seq_id=3, output_token=297, logprobs={297: Logprob(logprob=0.0, rank=-1, decoded_token=None)})], prompt_logprobs=None), CompletionSequenceGroupOutput(samples=[SequenceOutput(parent_seq_id=3, output_token=278, logprobs={278: Logprob(logprob=0.0, rank=-1, decoded_token=None)})], prompt_logprobs=None), CompletionSequenceGroupOutput(samples=[SequenceOutput(parent_seq_id=3, output_token=29214, logprobs={29214: Logprob(logprob=0.0, rank=-1, decoded_token=None)})], prompt_logprobs=None), CompletionSequenceGroupOutput(samples=[SequenceOutput(parent_seq_id=3, output_token=310, logprobs={310: Logprob(logprob=0.0, rank=-1, decoded_token=None)})], prompt_logprobs=None)]]
output has multiples True 6
SEQ GROUP DIO CANE SHOULD SAMPLE HERE True
SequenceGroup(request_id=1, sampling_params=SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=True, max_tokens=32, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), guided_decoding=None, num_seqs=1)
output has multiples True 6
SEQ GROUP DIO CANE SHOULD SAMPLE HERE True
SequenceGroup(request_id=3, sampling_params=SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=True, max_tokens=32, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), guided_decoding=None, num_seqs=1)
PROMPTS (is prompt, K, token_chunk, do_sample):  [(False, None, 1, True), (False, None, 1, True)]
SPECULATIING WITH REQ METADATA [SequenceGroupMetadata(request_id='1', is_prompt=False, seq_data={1: SequenceData(prompt_token_ids=array('l', [1, 450, 5434, 310, 319, 29902, 338, 29892, 297, 596, 1914, 3838, 29901, 29871]), output_token_ids=(29896, 29900, 29901, 450, 29877, 29889, 29884, 29889, 29879, 29889, 13, 29900, 29900, 29901, 29871, 29900, 29900, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0), cumulative_logprob=0.0, get_num_computed_tokens=44}, sampling_params=SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=True, max_tokens=32, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), guided_decoding=None, block_tables={1: [2, 4, 9, 1]}, do_sample=True, pooling_params=None, lora_request=None, computed_block_nums=[], state=SequenceGroupState(num_steps=1, current_step=0), multi_modal_data=None, encoder_seq_data=None, cross_block_table=None, prompt_adapter_request=None, token_chunk_size=1, num_speculative_tokens=None), SequenceGroupMetadata(request_id='3', is_prompt=False, seq_data={3: SequenceData(prompt_token_ids=array('l', [1, 450, 5434, 310, 319, 29902, 338, 29892, 297, 596, 1914, 3838, 29901, 29871]), output_token_ids=(29906, 29900, 29896, 29947, 29892, 29871, 29906, 29900, 29896, 29929, 29889, 13, 1576, 3186, 338, 297, 278, 29214, 310, 263, 19479, 29889, 450, 3186, 338, 297, 278, 29214, 310), cumulative_logprob=0.0, get_num_computed_tokens=43}, sampling_params=SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=True, max_tokens=32, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), guided_decoding=None, block_tables={3: [6, 5, 0]}, do_sample=True, pooling_params=None, lora_request=None, computed_block_nums=[], state=SequenceGroupState(num_steps=1, current_step=0), multi_modal_data=None, encoder_seq_data=None, cross_block_table=None, prompt_adapter_request=None, token_chunk_size=1, num_speculative_tokens=None)]
SCORING ON THE UNION OF REQUESTS (includes expanded batch), is prompt? [False, False, False, False, False, False, False, False, False, False, False, False]
SCORING ON THE UNION OF REQUESTS (includes expanded batch), do_sample? [True, True, True, True, True, True, True, True, True, True, True, True]
Proposal len X do_sample X is_prompt [(5, (True, False)), (5, (True, False))]
Target probs distr tensor(0., device='cuda:0') tensor(3.1250e-05, device='cuda:0') tensor(1., device='cuda:0') torch.Size([2, 6, 32000])
Draft probs distr tensor(0., device='cuda:0') tensor(3.1250e-05, device='cuda:0') tensor(1., device='cuda:0') torch.Size([2, 5, 32000])
VERIFY STEP ENDED tensor([[    0,    -1,    -1,    -1,    -1,    -1],
        [  263, 19479, 29889,   450,  3186,   338]], device='cuda:0')
SAMPLER OUTPUT LIST 6x2
LLM STep outputs [SamplerOutput(outputs=[CompletionSequenceGroupOutput(samples=[SequenceOutput(parent_seq_id=1, output_token=0, logprobs={0: Logprob(logprob=0.0, rank=-1, decoded_token=None)})], prompt_logprobs=None), CompletionSequenceGroupOutput(samples=[SequenceOutput(parent_seq_id=3, output_token=263, logprobs={263: Logprob(logprob=0.0, rank=-1, decoded_token=None)})], prompt_logprobs=None)], sampled_token_probs=None, sampled_token_ids=None, spec_decode_worker_metrics=None), SamplerOutput(outputs=[CompletionSequenceGroupOutput(samples=[SequenceOutput(parent_seq_id=1, output_token=-1, logprobs={-1: Logprob(logprob=0.0, rank=-1, decoded_token=None)})], prompt_logprobs=None), CompletionSequenceGroupOutput(samples=[SequenceOutput(parent_seq_id=3, output_token=19479, logprobs={19479: Logprob(logprob=0.0, rank=-1, decoded_token=None)})], prompt_logprobs=None)], sampled_token_probs=None, sampled_token_ids=None, spec_decode_worker_metrics=None), SamplerOutput(outputs=[CompletionSequenceGroupOutput(samples=[SequenceOutput(parent_seq_id=1, output_token=-1, logprobs={-1: Logprob(logprob=0.0, rank=-1, decoded_token=None)})], prompt_logprobs=None), CompletionSequenceGroupOutput(samples=[SequenceOutput(parent_seq_id=3, output_token=29889, logprobs={29889: Logprob(logprob=0.0, rank=-1, decoded_token=None)})], prompt_logprobs=None)], sampled_token_probs=None, sampled_token_ids=None, spec_decode_worker_metrics=None), SamplerOutput(outputs=[CompletionSequenceGroupOutput(samples=[SequenceOutput(parent_seq_id=1, output_token=-1, logprobs={-1: Logprob(logprob=0.0, rank=-1, decoded_token=None)})], prompt_logprobs=None), CompletionSequenceGroupOutput(samples=[SequenceOutput(parent_seq_id=3, output_token=450, logprobs={450: Logprob(logprob=0.0, rank=-1, decoded_token=None)})], prompt_logprobs=None)], sampled_token_probs=None, sampled_token_ids=None, spec_decode_worker_metrics=None), SamplerOutput(outputs=[CompletionSequenceGroupOutput(samples=[SequenceOutput(parent_seq_id=1, output_token=-1, logprobs={-1: Logprob(logprob=0.0, rank=-1, decoded_token=None)})], prompt_logprobs=None), CompletionSequenceGroupOutput(samples=[SequenceOutput(parent_seq_id=3, output_token=3186, logprobs={3186: Logprob(logprob=0.0, rank=-1, decoded_token=None)})], prompt_logprobs=None)], sampled_token_probs=None, sampled_token_ids=None, spec_decode_worker_metrics=None), SamplerOutput(outputs=[CompletionSequenceGroupOutput(samples=[SequenceOutput(parent_seq_id=1, output_token=-1, logprobs={-1: Logprob(logprob=0.0, rank=-1, decoded_token=None)})], prompt_logprobs=None), CompletionSequenceGroupOutput(samples=[SequenceOutput(parent_seq_id=3, output_token=338, logprobs={338: Logprob(logprob=0.0, rank=-1, decoded_token=None)})], prompt_logprobs=None)], sampled_token_probs=None, sampled_token_ids=None, spec_decode_worker_metrics=None)]
LLM process output indices range(0, 2)
OUTPUTS PER SEQ GROUP (ONE IS PROMPT THE OTHER NOT) [[CompletionSequenceGroupOutput(samples=[SequenceOutput(parent_seq_id=1, output_token=0, logprobs={0: Logprob(logprob=0.0, rank=-1, decoded_token=None)})], prompt_logprobs=None), CompletionSequenceGroupOutput(samples=[SequenceOutput(parent_seq_id=1, output_token=-1, logprobs={-1: Logprob(logprob=0.0, rank=-1, decoded_token=None)})], prompt_logprobs=None), CompletionSequenceGroupOutput(samples=[SequenceOutput(parent_seq_id=1, output_token=-1, logprobs={-1: Logprob(logprob=0.0, rank=-1, decoded_token=None)})], prompt_logprobs=None), CompletionSequenceGroupOutput(samples=[SequenceOutput(parent_seq_id=1, output_token=-1, logprobs={-1: Logprob(logprob=0.0, rank=-1, decoded_token=None)})], prompt_logprobs=None), CompletionSequenceGroupOutput(samples=[SequenceOutput(parent_seq_id=1, output_token=-1, logprobs={-1: Logprob(logprob=0.0, rank=-1, decoded_token=None)})], prompt_logprobs=None), CompletionSequenceGroupOutput(samples=[SequenceOutput(parent_seq_id=1, output_token=-1, logprobs={-1: Logprob(logprob=0.0, rank=-1, decoded_token=None)})], prompt_logprobs=None)], [CompletionSequenceGroupOutput(samples=[SequenceOutput(parent_seq_id=3, output_token=263, logprobs={263: Logprob(logprob=0.0, rank=-1, decoded_token=None)})], prompt_logprobs=None), CompletionSequenceGroupOutput(samples=[SequenceOutput(parent_seq_id=3, output_token=19479, logprobs={19479: Logprob(logprob=0.0, rank=-1, decoded_token=None)})], prompt_logprobs=None), CompletionSequenceGroupOutput(samples=[SequenceOutput(parent_seq_id=3, output_token=29889, logprobs={29889: Logprob(logprob=0.0, rank=-1, decoded_token=None)})], prompt_logprobs=None), CompletionSequenceGroupOutput(samples=[SequenceOutput(parent_seq_id=3, output_token=450, logprobs={450: Logprob(logprob=0.0, rank=-1, decoded_token=None)})], prompt_logprobs=None), CompletionSequenceGroupOutput(samples=[SequenceOutput(parent_seq_id=3, output_token=3186, logprobs={3186: Logprob(logprob=0.0, rank=-1, decoded_token=None)})], prompt_logprobs=None), CompletionSequenceGroupOutput(samples=[SequenceOutput(parent_seq_id=3, output_token=338, logprobs={338: Logprob(logprob=0.0, rank=-1, decoded_token=None)})], prompt_logprobs=None)]]
output has multiples True 6
SEQ GROUP DIO CANE SHOULD SAMPLE HERE True
SequenceGroup(request_id=1, sampling_params=SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=True, max_tokens=32, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), guided_decoding=None, num_seqs=1)
output has multiples True 6
SEQ GROUP DIO CANE SHOULD SAMPLE HERE True
SequenceGroup(request_id=3, sampling_params=SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=True, max_tokens=32, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), guided_decoding=None, num_seqs=1)
PROMPTS (is prompt, K, token_chunk, do_sample):  [(False, None, 1, True)]
SPECULATIING WITH REQ METADATA [SequenceGroupMetadata(request_id='1', is_prompt=False, seq_data={1: SequenceData(prompt_token_ids=array('l', [1, 450, 5434, 310, 319, 29902, 338, 29892, 297, 596, 1914, 3838, 29901, 29871]), output_token_ids=(29896, 29900, 29901, 450, 29877, 29889, 29884, 29889, 29879, 29889, 13, 29900, 29900, 29901, 29871, 29900, 29900, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0), cumulative_logprob=0.0, get_num_computed_tokens=45}, sampling_params=SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=True, max_tokens=32, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), guided_decoding=None, block_tables={1: [2, 4, 9, 1]}, do_sample=True, pooling_params=None, lora_request=None, computed_block_nums=[], state=SequenceGroupState(num_steps=1, current_step=0), multi_modal_data=None, encoder_seq_data=None, cross_block_table=None, prompt_adapter_request=None, token_chunk_size=1, num_speculative_tokens=None)]
SCORING ON THE UNION OF REQUESTS (includes expanded batch), is prompt? [False, False, False, False, False, False]
SCORING ON THE UNION OF REQUESTS (includes expanded batch), do_sample? [True, True, True, True, True, True]
Proposal len X do_sample X is_prompt [(5, (True, False))]
Target probs distr tensor(0., device='cuda:0') tensor(3.1250e-05, device='cuda:0') tensor(1., device='cuda:0') torch.Size([1, 6, 32000])
Draft probs distr tensor(0., device='cuda:0') tensor(3.1250e-05, device='cuda:0') tensor(1., device='cuda:0') Fork a new process to run a test 4980
F

=================================== FAILURES ===================================
_ test_spec_decode_e2e_with_detokenization[1-4-test_llm_kwargs0-per_test_common_llm_kwargs1-common_llm_kwargs0] _

args = ()
kwargs = {'batch_size': 4, 'test_llm_generator': <function test_llm_generator.<locals>.generate at 0x77e5b6909510>}
Skipped = <class 'Skipped'>, pid = 4980, pgid = 4947, _pid = 4980
_exitcode = 256, old_signal_handler = <Handlers.SIG_DFL: 0>

    @functools.wraps(f)
    def wrapper(*args: _P.args, **kwargs: _P.kwargs) -> None:
        # Make the process the leader of its own process group
        # to avoid sending SIGTERM to the parent process
        os.setpgrp()
        from _pytest.outcomes import Skipped
        pid = os.fork()
        print(f"Fork a new process to run a test {pid}")
        if pid == 0:
            try:
                f(*args, **kwargs)
            except Skipped as e:
                # convert Skipped to exit code 0
                print(str(e))
                os._exit(0)
            except Exception:
                import traceback
                traceback.print_exc()
                os._exit(1)
            else:
                os._exit(0)
        else:
            pgid = os.getpgid(pid)
            _pid, _exitcode = os.waitpid(pid, 0)
            # ignore SIGTERM signal itself
            old_signal_handler = signal.signal(signal.SIGTERM, signal.SIG_IGN)
            # kill all child processes
            os.killpg(pgid, signal.SIGTERM)
            # restore the signal handler
            signal.signal(signal.SIGTERM, old_signal_handler)
>           assert _exitcode == 0, (f"function {f} failed when called with"
                                    f" args {args} and kwargs {kwargs}")
E           AssertionError: function <function test_spec_decode_e2e_with_detokenization at 0x77e5b6a332e0> failed when called with args () and kwargs {'test_llm_generator': <function test_llm_generator.<locals>.generate at 0x77e5b6909510>, 'batch_size': 4}

tests/utils.py:452: AssertionError
=========================== short test summary info ============================
FAILED tests/spec_decode/e2e/test_multistep_correctness.py::test_spec_decode_e2e_with_detokenization[1-4-test_llm_kwargs0-per_test_common_llm_kwargs1-common_llm_kwargs0]
============================== 1 failed in 15.85s ==============================
